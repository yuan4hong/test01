{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5e59620",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .toggle-button-e5816863-9dbe-4450-8ce4-ebe6798d775d {\n",
       "        background-color: #808080;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        padding: 5px 10px;\n",
       "        text-align: center;\n",
       "        text-decoration: none;\n",
       "        display: inline-block;\n",
       "        font-size: 14px;\n",
       "        margin: 2px 1px;\n",
       "        transition-duration: 0.4s;\n",
       "        cursor: pointer;\n",
       "        border-radius: 2px;\n",
       "        box-shadow: none;\n",
       "        outline: none;\n",
       "    }\n",
       "\n",
       "    .toggle-button-e5816863-9dbe-4450-8ce4-ebe6798d775d:hover {\n",
       "        background-color: #666666;\n",
       "    }\n",
       "\n",
       "    .toggle-button-e5816863-9dbe-4450-8ce4-ebe6798d775d:active {\n",
       "        background-color: #4d4d4d;\n",
       "        box-shadow: none;\n",
       "        outline: none;\n",
       "    }\n",
       "\n",
       "    .links-container-7d83199b-9db0-4632-acb5-3105a710e057 {\n",
       "        display: none;\n",
       "    }\n",
       "\n",
       "    .link-item-7d83199b-9db0-4632-acb5-3105a710e057 {\n",
       "        margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    .link-item-7d83199b-9db0-4632-acb5-3105a710e057 a {\n",
       "        color: #808080;\n",
       "        text-decoration: none;\n",
       "        font-size: 14px;\n",
       "    }\n",
       "\n",
       "    .link-item-7d83199b-9db0-4632-acb5-3105a710e057 a:hover {\n",
       "        color: #666666;\n",
       "    }\n",
       "    </style>\n",
       "\n",
       "    <div>\n",
       "        <button class=\"toggle-button-e5816863-9dbe-4450-8ce4-ebe6798d775d\" onclick=\"toggleLinks('7d83199b-9db0-4632-acb5-3105a710e057')\">Toggle Links</button>\n",
       "        <div class=\"links-container-7d83199b-9db0-4632-acb5-3105a710e057\" id=\"links-7d83199b-9db0-4632-acb5-3105a710e057\">\n",
       "            <table><tr><td>Elastic</td><td><a href='https://es.data.nvidiagrid.net/app/discover#/?_g=(refreshInterval:(pause:!t,value:0),time:(from:&#x27;2026-02-04T02:08:07.938621Z&#x27;,mode:absolute,to:&#x27;2026-02-04T03:08:07.938621Z&#x27;))&amp;_a=(columns:!(message),filters:!((&#x27;$state&#x27;:(store:appState),meta:(alias:!n,disabled:!f,index:kratos-services.logs.xp-dcartm-team,key:kubernetes.namespace_name,negate:!f,params:(query:dcartm-team),type:phrase),query:(match_phrase:(kubernetes.namespace_name:(query:dcartm-team)))),(&#x27;$state&#x27;:(store:appState),meta:(alias:!n,disabled:!f,index:kratos-services.logs.xp-dcartm-team,key:kubernetes.pod_name,negate:!f,params:(query:cluster-20260203202803-yawkv5ak-driver),type:phrase),query:(match_phrase:(kubernetes.pod_name:(query:cluster-20260203202803-yawkv5ak-driver))))),index:kratos-services.logs.xp-dcartm-team,interval:auto,query:(language:lucene,query:&#x27;*&#x27;),sort:!(!(&#x27;@timestamp&#x27;,desc)))' target='_blank'>Click Here</a></td></tr><tr><td>Spark UI</td><td><a href='https://xp.kratos.nvidia.com/nvspark/ui/dcartm-team/hongy/' target='_blank'>Click Here</a></td></tr><tr><td>Cluster Utilization</td><td><a href='https://xp.kratos.nvidia.com/ops/d/nvspark_cluster/nvspark-performance-metrics?var-namespace=dcartm-team&amp;var-Pod=cluster-20260203202803-yawkv5ak-driver' target='_blank'>Click Here</a></td></tr><tr><td>Executor Count</td><td>1</td></tr><tr><td>Spark Driver Logs</td><td>\n",
       "        <a href=\"#\" onclick=\"downloadLogs_20260204_020851(); return false;\" \n",
       "           style=\"cursor: pointer;\">\n",
       "            Download\n",
       "        </a>\n",
       "        <script>\n",
       "        function downloadLogs_20260204_020851() {\n",
       "            var logs = `2026-02-03T23:40:37+0000 INFO sparksLauncher Writing configs to /opt/kyuubi/conf/kyuubi-defaults.conf\n",
       "2026-02-03T23:40:37+0000 INFO sparksLauncher Writing configs to /opt/kyuubi/conf/nvoauth.conf\n",
       "2026-02-03T23:40:37+0000 INFO sparksLauncher Driver memory: 17179869184B, provided kyuubi memory: 0B\n",
       "2026-02-03T23:40:37+0000 INFO sparksLauncher Driver memory: 17179869184, computed heap size: (2048MB)\n",
       "2026-02-03T23:40:37+0000 INFO sparksLauncher Setting kyuubi heap size to 2048MB\n",
       "2026-02-03T23:40:37+0000 INFO sparksLauncher Starting Kyuubi server...\n",
       "2026-02-03T23:40:37+0000 INFO sparksLauncher Kyuubi server started with pid 9\n",
       "Using kyuubi environment file /opt/kyuubi/conf/kyuubi-env.sh to initialize...\n",
       "2026-02-03T23:40:37+0000 INFO sparksLauncher Waiting for kyuubi at http://100.67.56.160:10099/api/v1 to start 0 seconds\n",
       "JAVA_HOME: /usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "KYUUBI_HOME: /opt/kyuubi\n",
       "KYUUBI_CONF_DIR: /opt/kyuubi/conf\n",
       "KYUUBI_LOG_DIR: /opt/kyuubi/logs\n",
       "KYUUBI_PID_DIR: /run/kyuubi\n",
       "KYUUBI_WORK_DIR_ROOT: /opt/kyuubi/work\n",
       "FLINK_HOME: \n",
       "FLINK_ENGINE_HOME: /opt/kyuubi/externals/engines/flink\n",
       "SPARK_HOME: /opt/spark\n",
       "SPARK_CONF_DIR: /opt/spark/conf\n",
       "SPARK_ENGINE_HOME: /opt/kyuubi/externals/engines/spark\n",
       "TRINO_ENGINE_HOME: /opt/kyuubi/externals/engines/trino\n",
       "HIVE_ENGINE_HOME: /opt/kyuubi/externals/engines/hive\n",
       "HADOOP_CONF_DIR: \n",
       "YARN_CONF_DIR: \n",
       "Starting org.apache.kyuubi.server.KyuubiServer\n",
       "SLF4J: Class path contains multiple SLF4J bindings.\n",
       "SLF4J: Found binding in [jar:file:/opt/kyuubi/jars/kyuubi-ha_2.12-1.8.0.6.1-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
       "SLF4J: Found binding in [jar:file:/opt/kyuubi/jars/log4j-slf4j-impl-2.20.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
       "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
       "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
       "2026-02-03T23:40:38+0000 INFO sparksLauncher Waiting for kyuubi at http://100.67.56.160:10099/api/v1 to start 1 seconds\n",
       "2026-02-03T23:40:38,763Z INFO KyuubiServer: \n",
       "                  Welcome to\n",
       "  __  __                           __\n",
       " /\\\\ \\\\/\\\\ \\\\                         /\\\\ \\\\      __\n",
       " \\\\ \\\\ \\\\/'/'  __  __  __  __  __  __\\\\ \\\\ \\\\____/\\\\_\\\\\n",
       "  \\\\ \\\\ , <  /\\\\ \\\\/\\\\ \\\\/\\\\ \\\\/\\\\ \\\\/\\\\ \\\\/\\\\ \\\\\\\\ \\\\ '__\\`\\\\/\\\\ \\\\\n",
       "   \\\\ \\\\ \\\\\\\\\\`\\\\\\\\ \\\\ \\\\_\\\\ \\\\ \\\\ \\\\_\\\\ \\\\ \\\\ \\\\_\\\\ \\\\\\\\ \\\\ \\\\L\\\\ \\\\ \\\\ \\\\\n",
       "    \\\\ \\\\_\\\\ \\\\_\\\\/\\`____ \\\\ \\\\____/\\\\ \\\\____/ \\\\ \\\\_,__/\\\\ \\\\_\\\\\n",
       "     \\\\/_/\\\\/_/\\`/___/> \\\\/___/  \\\\/___/   \\\\/___/  \\\\/_/\n",
       "                /\\\\___/\n",
       "                \\\\/__/\n",
       "       \n",
       "2026-02-03T23:40:38,771Z INFO KyuubiServer: Version: 1.8.0.5-SNAPSHOT, Revision: 6cfa56eff5f3d0ba921d50423d371e0d3a4ff140 (2025-11-24 08:21:30 -0800), Branch: HEAD, Java: 1.8, Scala: 2.12, Spark: 3.3.2, Hadoop: 3.3.5, Hive: 3.1.3, Flink: 1.17.0, Trino: 363\n",
       "2026-02-03T23:40:38,776Z INFO KyuubiServer: Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 1.8.0_472\n",
       "2026-02-03T23:40:38,783Z INFO SignalRegister: Registering signal handler for TERM\n",
       "2026-02-03T23:40:38,784Z INFO SignalRegister: Registering signal handler for HUP\n",
       "2026-02-03T23:40:38,784Z INFO SignalRegister: Registering signal handler for INT\n",
       "2026-02-03T23:40:39,159Z INFO Utils: Loading Kyuubi properties from /opt/kyuubi/conf/kyuubi-defaults.conf\n",
       "2026-02-03T23:40:39,622Z WARN KyuubiServer: REST frontend protocol is experimental, API may change in the future.\n",
       "2026-02-03T23:40:39,627Z INFO KinitAuxiliaryService: Service[KinitAuxiliaryService] is initialized.\n",
       "2026-02-03T23:40:39,627Z INFO PeriodicGCService: Service[PeriodicGCService] is initialized.\n",
       "2026-02-03T23:40:39,640Z INFO ThreadUtils: KyuubiSessionManager-exec-pool: pool size: 600, wait queue size: 1000, thread keepalive time: 60000 ms\n",
       "2026-02-03T23:40:39+0000 INFO sparksLauncher Waiting for kyuubi at http://100.67.56.160:10099/api/v1 to start 2 seconds\n",
       "2026-02-03T23:40:39,729Z INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at /0.0.0.0:8032\n",
       "2026-02-03T23:40:39,892Z INFO YarnApplicationOperation: Successfully initialized yarn client: STARTED\n",
       "2026-02-03T23:40:40,021Z INFO KubernetesApplicationOperation: Start initializing Kubernetes Client.\n",
       "2026-02-03T23:40:40,027Z INFO KubernetesUtils: Auto-configuring K8S client using current context from users K8S config file\n",
       "2026-02-03T23:40:40,518Z INFO KubernetesApplicationOperation: Initialized Kubernetes Client connect to: https://172.20.0.1:443/\n",
       "2026-02-03T23:40:40+0000 INFO sparksLauncher Waiting for kyuubi at http://100.67.56.160:10099/api/v1 to start 3 seconds\n",
       "2026-02-03T23:40:41,005Z WARN KyuubiApplicationManager: Error starting KubernetesApplicationOperation: Failure executing: GET at: https://172.20.0.1:443/api/v1/namespaces/default/pods?labelSelector=kyuubi-unique-tag&resourceVersion=0. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods is forbidden: User \"system:serviceaccount:dcartm-team:default-editor\" cannot list resource \"pods\" in API group \"\" in the namespace \"default\".\n",
       "2026-02-03T23:40:41,006Z INFO KyuubiApplicationManager: Service[KyuubiApplicationManager] is initialized.\n",
       "2026-02-03T23:40:41,142Z WARN HadoopCredentialsManager: Service hadoopfs does not require a token. Check your configuration to see if security is disabled or not. If security is enabled, some configurations of hadoopfs  might be missing, please check the configurations in  https://kyuubi.readthedocs.io/en/latest/security/hadoop_credentials_manager.html#required-security-configs\n",
       "2026-02-03T23:40:41,145Z INFO HiveConf: Found configuration file null\n",
       "2026-02-03T23:40:41,308Z WARN HadoopCredentialsManager: Service hive does not require a token. Check your configuration to see if security is disabled or not. If security is enabled, some configurations of hive  might be missing, please check the configurations in  https://kyuubi.readthedocs.io/en/latest/security/hadoop_credentials_manager.html#required-security-configs\n",
       "2026-02-03T23:40:41,310Z WARN HadoopCredentialsManager: No delegation token is required by services.\n",
       "2026-02-03T23:40:41,310Z INFO HadoopCredentialsManager: Service[HadoopCredentialsManager] is initialized.\n",
       "2026-02-03T23:40:41,336Z INFO HikariDataSource: jdbc-metadata-store-pool - Starting...\n",
       "2026-02-03T23:40:41+0000 INFO sparksLauncher Waiting for kyuubi at http://100.67.56.160:10099/api/v1 to start 4 seconds\n",
       "2026-02-03T23:40:41,842Z INFO PoolBase: jdbc-metadata-store-pool - Driver does not support get/set network timeout for connections. (Feature not implemented: No details.)\n",
       "2026-02-03T23:40:41,844Z INFO HikariDataSource: jdbc-metadata-store-pool - Start completed.\n",
       "2026-02-03T23:40:42,152Z INFO JDBCMetadataStore: Execute init schema ddl: -- Derby does not support \\`CREATE TABLE IF NOT EXISTS\\`\n",
       "\n",
       "-- the metadata table ddl\n",
       "\n",
       "CREATE TABLE metadata(\n",
       "    key_id bigint PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY, -- the auto increment key id\n",
       "    identifier varchar(36) NOT NULL, -- the identifier id, which is an UUID\n",
       "    session_type varchar(32) NOT NULL, -- the session type, SQL or BATCH\n",
       "    real_user varchar(255) NOT NULL, -- the real user\n",
       "    user_name varchar(255) NOT NULL, -- the user name, might be a proxy user\n",
       "    ip_address varchar(128), -- the client ip address\n",
       "    kyuubi_instance varchar(1024) NOT NULL, -- the kyuubi instance that creates this\n",
       "    state varchar(128) NOT NULL, -- the session state\n",
       "    resource varchar(1024), -- the main resource\n",
       "    class_name varchar(1024), -- the main class name\n",
       "    request_name varchar(1024), -- the request name\n",
       "    request_conf clob, -- the request config map\n",
       "    request_args clob, -- the request arguments\n",
       "    create_time BIGINT NOT NULL, -- the metadata create time\n",
       "    engine_type varchar(32) NOT NULL, -- the engine type\n",
       "    cluster_manager varchar(128), -- the engine cluster manager\n",
       "    engine_open_time bigint, -- the engine open time\n",
       "    engine_id varchar(128), -- the engine application id\n",
       "    engine_name clob, -- the engine application name\n",
       "    engine_url varchar(1024), -- the engine tracking url\n",
       "    engine_state varchar(32), -- the engine application state\n",
       "    engine_error clob, -- the engine application diagnose\n",
       "    end_time bigint,  -- the metadata end time\n",
       "    peer_instance_closed boolean default FALSE -- closed by peer kyuubi instance\n",
       ") successfully.\n",
       "2026-02-03T23:40:42,160Z INFO JDBCMetadataStore: Execute init schema ddl: \n",
       "\n",
       "CREATE UNIQUE INDEX metadata_unique_identifier_index ON metadata(identifier) successfully.\n",
       "2026-02-03T23:40:42,166Z INFO JDBCMetadataStore: Execute init schema ddl: \n",
       "\n",
       "CREATE INDEX metadata_user_name_index ON metadata(user_name) successfully.\n",
       "2026-02-03T23:40:42,202Z INFO JDBCMetadataStore: Execute init schema ddl: \n",
       "\n",
       "CREATE INDEX metadata_engine_type_index ON metadata(engine_type) successfully.\n",
       "2026-02-03T23:40:42,206Z INFO MetadataManager: Service[MetadataManager] is initialized.\n",
       "2026-02-03T23:40:42,215Z INFO KyuubiOperationManager: Service[KyuubiOperationManager] is initialized.\n",
       "2026-02-03T23:40:42,217Z INFO KyuubiSessionManager: Service[KyuubiSessionManager] is initialized.\n",
       "2026-02-03T23:40:42,217Z INFO KyuubiServer: Service[KyuubiBackendService] is initialized.\n",
       "2026-02-03T23:40:42,232Z INFO log: Logging initialized @4545ms to org.eclipse.jetty.util.log.Slf4jLog\n",
       "2026-02-03T23:40:42,293Z INFO AuthenticationFilter: Add authentication handler BasicAuthenticationHandler for scheme BASIC\n",
       "2026-02-03T23:40:42,350Z INFO KyuubiTHttpFrontendService: Started KyuubiTHttpFrontendService in http mode on port 10009 path=/cliservice with 9 ... 999 threads\n",
       "2026-02-03T23:40:42,350Z INFO KyuubiTHttpFrontendService: Service[KyuubiTHttpFrontendService] is initialized.\n",
       "2026-02-03T23:40:42,361Z INFO KyuubiRestFrontendService: Service[KyuubiRestFrontendService] is initialized.\n",
       "2026-02-03T23:40:42,361Z INFO KyuubiServer: Service[KyuubiServer] is initialized.\n",
       "2026-02-03T23:40:42,362Z INFO KinitAuxiliaryService: Service[KinitAuxiliaryService] is started.\n",
       "2026-02-03T23:40:42,363Z INFO PeriodicGCService: Service[PeriodicGCService] is started.\n",
       "2026-02-03T23:40:42,366Z INFO KyuubiApplicationManager: Service[KyuubiApplicationManager] is started.\n",
       "2026-02-03T23:40:42,366Z INFO HadoopCredentialsManager: Service[HadoopCredentialsManager] is started.\n",
       "2026-02-03T23:40:42,366Z INFO MetadataManager: Service[MetadataManager] is started.\n",
       "2026-02-03T23:40:42,368Z INFO KyuubiOperationManager: Service[KyuubiOperationManager] is started.\n",
       "2026-02-03T23:40:42,368Z INFO KyuubiSessionManager: Service[KyuubiSessionManager] is started.\n",
       "2026-02-03T23:40:42,369Z INFO KyuubiServer: Service[KyuubiBackendService] is started.\n",
       "2026-02-03T23:40:42,369Z INFO KyuubiTHttpFrontendService: Service[KyuubiTHttpFrontendService] is started.\n",
       "2026-02-03T23:40:42,370Z INFO KyuubiTHttpFrontendService: Starting and exposing JDBC connection at: jdbc:hive2://100.67.56.160:10009/\n",
       "2026-02-03T23:40:42,371Z INFO Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_472-8u472-ga-1~22.04-b08\n",
       "2026-02-03T23:40:42,371Z INFO Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 1.8.0_472-8u472-ga-1~22.04-b08\n",
       "2026-02-03T23:40:42,384Z INFO Server: Started @4697ms\n",
       "2026-02-03T23:40:42,390Z INFO session: DefaultSessionIdManager workerName=node0\n",
       "2026-02-03T23:40:42,390Z INFO session: No SessionScavenger set, using defaults\n",
       "2026-02-03T23:40:42,391Z INFO session: node0 Scavenging every 600000ms\n",
       "2026-02-03T23:40:42,395Z WARN SecurityHandler: ServletContext@o.e.j.s.ServletContextHandler@4da85630{/,null,STARTING} has uncovered http methods for path: /*\n",
       "2026-02-03T23:40:42,401Z INFO AbstractConnector: Started ServerConnector@63cd2cd2{HTTP/1.1, (http/1.1)}{100.67.56.160:10099}\n",
       "2026-02-03T23:40:42,439Z WARN AuthenticationFilter: Authentication handler has been defined for scheme BASIC\n",
       "2026-02-03T23:40:42,439Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4da85630{/,null,AVAILABLE}\n",
       "2026-02-03T23:40:42,443Z INFO AbstractConnector: Started ServerConnector@59696551{HTTP/1.1, (http/1.1)}{0.0.0.0:10009}\n",
       "2026-02-03T23:40:42,444Z INFO Server: Started @4757ms\n",
       "2026-02-03T23:40:42,651Z INFO AuthenticationFilter: Add authentication handler BasicAuthenticationHandler for scheme BASIC\n",
       "2026-02-03T23:40:42+0000 INFO sparksLauncher Waiting for kyuubi at http://100.67.56.160:10099/api/v1 to start 5 seconds\n",
       "2026-02-03T23:40:42+0000 INFO sparksLauncher response: <Response [404]>\n",
       "2026-02-03T23:40:43,510Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@717d7587{/api,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,513Z WARN ContextHandler: o.e.j.s.ServletContextHandler@4b9dbf07{/,null,STOPPED} contextPath ends with /\n",
       "2026-02-03T23:40:43,514Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b9dbf07{/static,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,514Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@16bd7ae1{/,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,515Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3312f4f4{/static,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,515Z WARN ContextHandler: o.e.j.s.ServletContextHandler@76cdafa3{/,null,STOPPED} contextPath ends with /\n",
       "2026-02-03T23:40:43,515Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@76cdafa3{/swagger-static,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,516Z WARN ContextHandler: o.e.j.s.ServletContextHandler@4fb64e14{/,null,STOPPED} contextPath ends with /\n",
       "2026-02-03T23:40:43,516Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4fb64e14{/swagger,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,516Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@a2b54e3{/docs,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,517Z WARN ContextHandler: o.e.j.s.ServletContextHandler@21f7e537{/,null,STOPPED} contextPath ends with /\n",
       "2026-02-03T23:40:43,517Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21f7e537{/docs,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,517Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@62b6c045{/swagger,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,518Z INFO ContextHandler: Started o.e.j.s.ServletContextHandler@46b2dcc5{/ui,null,AVAILABLE}\n",
       "2026-02-03T23:40:43,518Z INFO KyuubiRestFrontendService: Service[KyuubiRestFrontendService] is started.\n",
       "2026-02-03T23:40:43,519Z INFO KyuubiRestFrontendService: Exposing REST endpoint at: http://100.67.56.160:10099\n",
       "2026-02-03T23:40:43,519Z INFO KyuubiServer: Service[KyuubiServer] is started.\n",
       "2026-02-03T23:40:43,527Z INFO Utils: Loading Kyuubi properties from /opt/kyuubi/conf/kyuubi-defaults.conf\n",
       "2026-02-03T23:40:43+0000 INFO sparksLauncher Waiting for kyuubi at http://100.67.56.160:10099/api/v1 to start 6 seconds\n",
       "2026-02-03T23:40:44+0000 INFO sparksLauncher response: <Response [200]>\n",
       "2026-02-03T23:40:44+0000 INFO sparksLauncher Kyuubi started 200\n",
       "2026-02-03T23:40:44,043Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.56.160\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:44,043Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260203234024-mmysc0ll\n",
       "2026-02-03T23:40:44,093Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.56.160\n",
       "2026-02-03T23:40:44,100Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:44,100Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:44,113Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:44,134Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/cfc8b837-b0f7-40ee-b388-391d8316c4dc/56d8d163-f51b-4169-a367-b4eb66ed7e92\n",
       "2026-02-03T23:40:44,138Z INFO LaunchEngine: Processing anonymous's query[56d8d163-f51b-4169-a367-b4eb66ed7e92]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:40:44,138Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]/hongy-default-20260203202802-startup is opened, current opening sessions 1\n",
       "2026-02-03T23:40:44+0000 INFO sparksLauncher Session created with identifier cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "2026-02-03T23:40:44,155Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.56.160\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:44,256Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:44,263Z INFO ZooKeeper: Client environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT\n",
       "2026-02-03T23:40:44,263Z INFO ZooKeeper: Client environment:host.name=cluster-20260203202803-yawkv5ak-driver\n",
       "2026-02-03T23:40:44,264Z INFO ZooKeeper: Client environment:java.version=1.8.0_472\n",
       "2026-02-03T23:40:44,264Z INFO ZooKeeper: Client environment:java.vendor=Private Build\n",
       "2026-02-03T23:40:44,264Z INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n",
       "2026-02-03T23:40:44,264Z INFO ZooKeeper: Client environment:java.class.path=/opt/kyuubi/conf:/opt/kyuubi/web-ui:/opt/kyuubi/jars/HikariCP-4.0.3.jar:/opt/kyuubi/jars/ST4-4.3.4.jar:/opt/kyuubi/jars/animal-sniffer-annotations-1.21.jar:/opt/kyuubi/jars/annotations-13.0.jar:/opt/kyuubi/jars/annotations-4.1.1.4.jar:/opt/kyuubi/jars/antlr-runtime-3.5.3.jar:/opt/kyuubi/jars/antlr4-runtime-4.9.3.jar:/opt/kyuubi/jars/aopalliance-repackaged-2.6.1.jar:/opt/kyuubi/jars/arrow-format-12.0.0.jar:/opt/kyuubi/jars/arrow-memory-core-12.0.0.jar:/opt/kyuubi/jars/arrow-memory-netty-12.0.0.jar:/opt/kyuubi/jars/arrow-vector-12.0.0.jar:/opt/kyuubi/jars/classgraph-4.8.138.jar:/opt/kyuubi/jars/commons-codec-1.15.jar:/opt/kyuubi/jars/commons-collections-3.2.2.jar:/opt/kyuubi/jars/commons-lang-2.6.jar:/opt/kyuubi/jars/commons-lang3-3.12.0.jar:/opt/kyuubi/jars/commons-logging-1.1.3.jar:/opt/kyuubi/jars/credentials-provider-25.08.0.jar:/opt/kyuubi/jars/derby-10.14.2.0.jar:/opt/kyuubi/jars/dummy-group-provider-25.08.0.jar:/opt/kyuubi/jars/error_prone_annotations-2.14.0.jar:/opt/kyuubi/jars/failsafe-2.4.4.jar:/opt/kyuubi/jars/failureaccess-1.0.1.jar:/opt/kyuubi/jars/flatbuffers-java-1.12.0.jar:/opt/kyuubi/jars/fliptables-1.0.2.jar:/opt/kyuubi/jars/grpc-api-1.48.0.jar:/opt/kyuubi/jars/grpc-context-1.48.0.jar:/opt/kyuubi/jars/grpc-core-1.48.0.jar:/opt/kyuubi/jars/grpc-grpclb-1.48.0.jar:/opt/kyuubi/jars/grpc-netty-1.48.0.jar:/opt/kyuubi/jars/grpc-protobuf-1.48.0.jar:/opt/kyuubi/jars/grpc-protobuf-lite-1.48.0.jar:/opt/kyuubi/jars/grpc-stub-1.48.0.jar:/opt/kyuubi/jars/gson-2.9.0.jar:/opt/kyuubi/jars/guava-31.1-jre.jar:/opt/kyuubi/jars/hive-common-3.1.3.jar:/opt/kyuubi/jars/hive-metastore-3.1.3.jar:/opt/kyuubi/jars/hive-serde-3.1.3.jar:/opt/kyuubi/jars/hive-service-rpc-3.1.3.jar:/opt/kyuubi/jars/hive-shims-0.23-3.1.3.jar:/opt/kyuubi/jars/hive-shims-common-3.1.3.jar:/opt/kyuubi/jars/hive-standalone-metastore-3.1.3.jar:/opt/kyuubi/jars/hive-storage-api-2.7.0.jar:/opt/kyuubi/jars/hk2-api-2.6.1.jar:/opt/kyuubi/jars/hk2-locator-2.6.1.jar:/opt/kyuubi/jars/hk2-utils-2.6.1.jar:/opt/kyuubi/jars/httpclient-4.5.14.jar:/opt/kyuubi/jars/httpcore-4.4.16.jar:/opt/kyuubi/jars/httpmime-4.5.14.jar:/opt/kyuubi/jars/j2objc-annotations-1.3.jar:/opt/kyuubi/jars/jackson-annotations-2.15.0.jar:/opt/kyuubi/jars/jackson-core-2.15.0.jar:/opt/kyuubi/jars/jackson-databind-2.15.0.jar:/opt/kyuubi/jars/jackson-dataformat-yaml-2.15.0.jar:/opt/kyuubi/jars/jackson-datatype-jdk8-2.15.0.jar:/opt/kyuubi/jars/jackson-datatype-jsr310-2.15.0.jar:/opt/kyuubi/jars/jackson-jaxrs-base-2.15.0.jar:/opt/kyuubi/jars/jackson-jaxrs-json-provider-2.15.0.jar:/opt/kyuubi/jars/jackson-module-jaxb-annotations-2.15.0.jar:/opt/kyuubi/jars/jackson-module-scala_2.12-2.15.0.jar:/opt/kyuubi/jars/jakarta.annotation-api-1.3.5.jar:/opt/kyuubi/jars/jakarta.inject-2.6.1.jar:/opt/kyuubi/jars/jakarta.servlet-api-4.0.4.jar:/opt/kyuubi/jars/jakarta.validation-api-2.0.2.jar:/opt/kyuubi/jars/jakarta.ws.rs-api-2.1.6.jar:/opt/kyuubi/jars/jakarta.xml.bind-api-2.3.2.jar:/opt/kyuubi/jars/java-jwt-4.2.1.jar:/opt/kyuubi/jars/javassist-3.25.0-GA.jar:/opt/kyuubi/jars/jcl-over-slf4j-1.7.36.jar:/opt/kyuubi/jars/jersey-client-2.39.1.jar:/opt/kyuubi/jars/jersey-common-2.39.1.jar:/opt/kyuubi/jars/jersey-container-servlet-core-2.39.1.jar:/opt/kyuubi/jars/jersey-entity-filtering-2.39.1.jar:/opt/kyuubi/jars/jersey-hk2-2.39.1.jar:/opt/kyuubi/jars/jersey-media-json-jackson-2.39.1.jar:/opt/kyuubi/jars/jersey-media-multipart-2.39.1.jar:/opt/kyuubi/jars/jersey-server-2.39.1.jar:/opt/kyuubi/jars/jetcd-api-0.7.3.jar:/opt/kyuubi/jars/jetcd-common-0.7.3.jar:/opt/kyuubi/jars/jetcd-core-0.7.3.jar:/opt/kyuubi/jars/jetcd-grpc-0.7.3.jar:/opt/kyuubi/jars/jetty-http-9.4.51.v20230217.jar:/opt/kyuubi/jars/jetty-io-9.4.51.v20230217.jar:/opt/kyuubi/jars/jetty-security-9.4.51.v20230217.jar:/opt/kyuubi/jars/jetty-server-9.4.51.v20230217.jar:/opt/kyuubi/jars/jetty-servlet-9.4.51.v20230217.jar:/opt/kyuubi/jars/jetty-util-9.4.51.v20230217.jar:/opt/kyuubi/jars/jetty-util-ajax-9.4.51.v20230217.jar:/opt/kyuubi/jars/jline-0.9.94.jar:/opt/kyuubi/jars/json-20230227.jar:/opt/kyuubi/jars/json-file-credentials-provider-25.08.0.jar:/opt/kyuubi/jars/jul-to-slf4j-1.7.36.jar:/opt/kyuubi/jars/jwks-rsa-0.21.2.jar:/opt/kyuubi/jars/kafka-clients-3.4.0.jar:/opt/kyuubi/jars/kotlin-stdlib-1.8.21.jar:/opt/kyuubi/jars/kotlin-stdlib-common-1.9.10.jar:/opt/kyuubi/jars/kotlin-stdlib-jdk7-1.8.21.jar:/opt/kyuubi/jars/kotlin-stdlib-jdk8-1.8.21.jar:/opt/kyuubi/jars/kubernetes-client-6.4.1.jar:/opt/kyuubi/jars/kubernetes-client-api-6.4.1.jar:/opt/kyuubi/jars/kubernetes-httpclient-okhttp-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-admissionregistration-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-apiextensions-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-apps-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-autoscaling-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-batch-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-certificates-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-common-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-coordination-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-core-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-discovery-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-events-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-extensions-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-flowcontrol-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-gatewayapi-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-metrics-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-networking-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-node-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-policy-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-rbac-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-scheduling-6.4.1.jar:/opt/kyuubi/jars/kubernetes-model-storageclass-6.4.1.jar:/opt/kyuubi/jars/kyuubi-common_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-ctl_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-events_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-ha_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-hive-jdbc-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-metrics_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-rest-client-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-server-plugin-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-server_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-shaded-zookeeper-36-0.1.0.jar:/opt/kyuubi/jars/kyuubi-util-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-util-scala_2.12-1.8.0.6.1-SNAPSHOT-tests.jar:/opt/kyuubi/jars/kyuubi-util-scala_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/kyuubi-zookeeper_2.12-1.8.0.6.1-SNAPSHOT.jar:/opt/kyuubi/jars/libfb303-0.9.3.jar:/opt/kyuubi/jars/libthrift-0.9.3-1.jar:/opt/kyuubi/jars/log4j-1.2-api-2.20.0.jar:/opt/kyuubi/jars/log4j-api-2.20.0.jar:/opt/kyuubi/jars/log4j-core-2.20.0.jar:/opt/kyuubi/jars/log4j-slf4j-impl-2.20.0.jar:/opt/kyuubi/jars/logging-interceptor-3.12.12.jar:/opt/kyuubi/jars/lz4-java-1.8.0.jar:/opt/kyuubi/jars/metrics-core-4.2.8.jar:/opt/kyuubi/jars/metrics-jmx-4.2.8.jar:/opt/kyuubi/jars/metrics-json-4.2.8.jar:/opt/kyuubi/jars/metrics-jvm-4.2.8.jar:/opt/kyuubi/jars/mimepull-1.9.15.jar:/opt/kyuubi/jars/netty-all-4.1.89.Final.jar:/opt/kyuubi/jars/netty-buffer-4.1.89.Final.jar:/opt/kyuubi/jars/netty-codec-4.1.89.Final.jar:/opt/kyuubi/jars/netty-codec-dns-4.1.89.Final.jar:/opt/kyuubi/jars/netty-codec-http-4.1.89.Final.jar:/opt/kyuubi/jars/netty-codec-http2-4.1.89.Final.jar:/opt/kyuubi/jars/netty-codec-socks-4.1.89.Final.jar:/opt/kyuubi/jars/netty-common-4.1.89.Final.jar:/opt/kyuubi/jars/netty-handler-4.1.89.Final.jar:/opt/kyuubi/jars/netty-handler-proxy-4.1.89.Final.jar:/opt/kyuubi/jars/netty-resolver-4.1.89.Final.jar:/opt/kyuubi/jars/netty-resolver-dns-4.1.89.Final.jar:/opt/kyuubi/jars/netty-transport-4.1.89.Final.jar:/opt/kyuubi/jars/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/kyuubi/jars/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/kyuubi/jars/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/kyuubi/jars/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/kyuubi/jars/okhttp-4.12.0.jar:/opt/kyuubi/jars/okhttp-urlconnection-3.14.9.jar:/opt/kyuubi/jars/okio-3.6.0.jar:/opt/kyuubi/jars/okio-jvm-3.6.0.jar:/opt/kyuubi/jars/osgi-resource-locator-1.0.3.jar:/opt/kyuubi/jars/paranamer-2.8.jar:/opt/kyuubi/jars/perfmark-api-0.25.0.jar:/opt/kyuubi/jars/proto-google-common-protos-2.9.0.jar:/opt/kyuubi/jars/protobuf-java-3.21.7.jar:/opt/kyuubi/jars/protobuf-java-util-3.21.7.jar:/opt/kyuubi/jars/scala-xml_2.12-2.1.0.jar:/opt/kyuubi/jars/scopt_2.12-4.1.0.jar:/opt/kyuubi/jars/simpleclient-0.16.0.jar:/opt/kyuubi/jars/simpleclient_common-0.16.0.jar:/opt/kyuubi/jars/simpleclient_dropwizard-0.16.0.jar:/opt/kyuubi/jars/simpleclient_servlet-0.16.0.jar:/opt/kyuubi/jars/simpleclient_servlet_common-0.16.0.jar:/opt/kyuubi/jars/simpleclient_tracer_common-0.16.0.jar:/opt/kyuubi/jars/simpleclient_tracer_otel-0.16.0.jar:/opt/kyuubi/jars/simpleclient_tracer_otel_agent-0.16.0.jar:/opt/kyuubi/jars/slf4j-api-1.7.36.jar:/opt/kyuubi/jars/snakeyaml-2.2.jar:/opt/kyuubi/jars/snappy-java-1.1.8.4.jar:/opt/kyuubi/jars/starfleet-oauthn-25.08.0.jar:/opt/kyuubi/jars/static-config-group-provider-25.08.0-jar-with-dependencies.jar:/opt/kyuubi/jars/static-config-group-provider-25.08.0.jar:/opt/kyuubi/jars/swagger-annotations-2.2.1.jar:/opt/kyuubi/jars/swagger-core-2.2.1.jar:/opt/kyuubi/jars/swagger-integration-2.2.1.jar:/opt/kyuubi/jars/swagger-jaxrs2-2.2.1.jar:/opt/kyuubi/jars/swagger-models-2.2.1.jar:/opt/kyuubi/jars/swagger-ui-4.9.1.jar:/opt/kyuubi/jars/test-credentials-provider-25.08.0-jar-with-dependencies.jar:/opt/kyuubi/jars/trino-client-363.jar:/opt/kyuubi/jars/units-1.6.jar:/opt/kyuubi/jars/vertx-core-4.3.2.jar:/opt/kyuubi/jars/vertx-grpc-4.3.2.jar:/opt/kyuubi/jars/zjsonpatch-0.3.0.jar:/opt/kyuubi/jars/zstd-jni-1.5.2-1.jar:/opt/kyuubi/jars/delta-spark_2.12-3.3.2-25-12-10.jar:/opt/kyuubi/jars/kyuubi-spark-authz_2.12-1.7.0.jar:/opt/kyuubi/jars/nvsparkaas-k8s-plugin-3.5.3-25.08.5.jar:/opt/kyuubi/jars/rapids-4-spark_2.12-25.10.1-cuda12.jar:/opt/kyuubi/jars/spark-catalyst_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-common-utils_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-core_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-graphx_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-hadoop-cloud_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-hive-thriftserver_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-hive_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-kubernetes_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-kvstore_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-launcher_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-mesos_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-mllib-local_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-mllib_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-network-common_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-network-shuffle_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-repl_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-sketch_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-sql-api_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-sql_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-streaming_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-tags_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-unsafe_2.12-3.5.3.jar:/opt/kyuubi/jars/spark-yarn_2.12-3.5.3.jar:/opt/kyuubi/jars/scala-library-2.12.18.jar:/opt/kyuubi/jars/hadoop-client-runtime-3.4.1.jar:/opt/kyuubi/jars/hadoop-client-api-3.4.1.jar:\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:java.library.path=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:java.compiler=<NA>\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:os.name=Linux\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:os.arch=amd64\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:os.version=6.1.148-173.267.amzn2023.x86_64\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:user.name=root\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:user.home=/root\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:user.dir=/opt/kyuubi/bin\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:os.memory.free=115MB\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:os.memory.max=2048MB\n",
       "2026-02-03T23:40:44,265Z INFO ZooKeeper: Client environment:os.memory.total=282MB\n",
       "2026-02-03T23:40:44,268Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@33557ebe\n",
       "2026-02-03T23:40:44,273Z INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n",
       "2026-02-03T23:40:44,277Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:44,284Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:44,294Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:44,294Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:40:44,295Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:44,297Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:48764, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:40:44,305Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a49070041, negotiated timeout = 120000\n",
       "2026-02-03T23:40:44,313Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:44,321Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:44,331Z INFO Compatibility: Using org.apache.zookeeper.server.quorum.MultipleAddresses\n",
       "2026-02-03T23:40:44,331Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:44,511Z INFO ProcBuilder: Creating default's working directory at /opt/kyuubi/work/default\n",
       "2026-02-03T23:40:44,530Z INFO EngineRef: Launching engine:\n",
       "/opt/spark-3.5.3-bin-hadoop3/bin/spark-submit \\\\\n",
       "\t--class org.apache.kyuubi.engine.spark.SparkSQLEngine \\\\\n",
       "\t--conf spark.hadoop.hadoop.security.group.mapping=com.nvidia.sparkaas.hadoop.plugin.StaticConfigGroupProvider \\\\\n",
       "\t--conf spark.kyuubi.authentication=NONE \\\\\n",
       "\t--conf spark.kyuubi.backend.server.exec.pool.size=600 \\\\\n",
       "\t--conf spark.kyuubi.backend.server.exec.pool.wait.queue.size=1000 \\\\\n",
       "\t--conf spark.kyuubi.batch.session.idle.timeout=PT30S \\\\\n",
       "\t--conf spark.kyuubi.client.ipAddress=100.67.56.160 \\\\\n",
       "\t--conf spark.kyuubi.ctl.rest.connect.timeout=PT300S \\\\\n",
       "\t--conf spark.kyuubi.engine.connection.url.use.hostname=false \\\\\n",
       "\t--conf spark.kyuubi.engine.group.name=default \\\\\n",
       "\t--conf spark.kyuubi.engine.security.enabled=false \\\\\n",
       "\t--conf spark.kyuubi.engine.security.secret.provider=simple \\\\\n",
       "\t--conf spark.kyuubi.engine.share.level=GROUP \\\\\n",
       "\t--conf spark.kyuubi.engine.share.level.subdomain=cluster-20260203202803-yawkv5ak \\\\\n",
       "\t--conf spark.kyuubi.engine.submit.time=1770162044504 \\\\\n",
       "\t--conf spark.kyuubi.engine.type=SPARK_SQL \\\\\n",
       "\t--conf spark.kyuubi.engine.ui.stop.enabled=false \\\\\n",
       "\t--conf spark.kyuubi.frontend.bind.port=1009 \\\\\n",
       "\t--conf spark.kyuubi.frontend.connection.url.use.hostname=false \\\\\n",
       "\t--conf spark.kyuubi.frontend.protocols=THRIFT_HTTP,REST \\\\\n",
       "\t--conf spark.kyuubi.frontend.rest.bind.port=10099 \\\\\n",
       "\t--conf spark.kyuubi.frontend.thrift.http.bind.port=10009 \\\\\n",
       "\t--conf spark.kyuubi.frontend.thrift.login.timeout=PT300S \\\\\n",
       "\t--conf spark.kyuubi.ha.addresses=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 \\\\\n",
       "\t--conf spark.kyuubi.ha.enabled=false \\\\\n",
       "\t--conf spark.kyuubi.ha.engine.ref.id=cfc8b837-b0f7-40ee-b388-391d8316c4dc \\\\\n",
       "\t--conf spark.kyuubi.ha.namespace=/kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak \\\\\n",
       "\t--conf spark.kyuubi.ha.zookeeper.session.timeout=120000 \\\\\n",
       "\t--conf spark.kyuubi.metrics.enabled=false \\\\\n",
       "\t--conf spark.kyuubi.metrics.reporters=PROMETHEUS \\\\\n",
       "\t--conf spark.kyuubi.operation.idle.timeout=PT1M \\\\\n",
       "\t--conf spark.kyuubi.operation.language=PYTHON \\\\\n",
       "\t--conf spark.kyuubi.server.administrators=admin,anonymous \\\\\n",
       "\t--conf spark.kyuubi.server.credentials.provider=com.nvidia.sparkaas.credentials.JsonFileCredentialsProvider \\\\\n",
       "\t--conf spark.kyuubi.server.credentials.provider.params=/vault/secrets/config.json \\\\\n",
       "\t--conf spark.kyuubi.server.ipAddress=100.67.56.160 \\\\\n",
       "\t--conf spark.kyuubi.session.check.interval=PT30S \\\\\n",
       "\t--conf spark.kyuubi.session.conf.ignore.list=spark.sql.optimizer.excludedRules \\\\\n",
       "\t--conf spark.kyuubi.session.conf.restrict.list=spark.sql.optimizer.excludedRules,spark.yarn.security.credentials.hbase.enabled,spark.security.credentials.hbase.enabled,spark.driver.userClassPathFirst \\\\\n",
       "\t--conf spark.kyuubi.session.connection.url=100.67.56.160:10099 \\\\\n",
       "\t--conf spark.kyuubi.session.engine.alive.probe.enabled=true \\\\\n",
       "\t--conf spark.kyuubi.session.engine.alive.probe.interval=PT10S \\\\\n",
       "\t--conf spark.kyuubi.session.engine.alive.timeout=PT2M \\\\\n",
       "\t--conf spark.kyuubi.session.engine.check.interval=PT1M \\\\\n",
       "\t--conf spark.kyuubi.session.engine.idle.timeout=PT30M \\\\\n",
       "\t--conf spark.kyuubi.session.engine.initialize.timeout=PT60M \\\\\n",
       "\t--conf spark.kyuubi.session.engine.spark.main.resource=/opt/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar \\\\\n",
       "\t--conf spark.kyuubi.session.group.provider=com.nvidia.sparkaas.kyuubi.plugin.StaticConfigGroupProvider \\\\\n",
       "\t--conf spark.kyuubi.session.idle.timeout=PT60M \\\\\n",
       "\t--conf spark.kyuubi.session.name=hongy-default-20260203202802-startup \\\\\n",
       "\t--conf spark.kyuubi.session.real.user=anonymous \\\\\n",
       "\t--conf spark.kyuubi.sparkaas.groupprovider.static.group=default \\\\\n",
       "\t--conf spark.sessionType=INTERACTIVE \\\\\n",
       "\t--conf spark.acls.enable=false \\\\\n",
       "\t--conf spark.app.name=kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc \\\\\n",
       "\t--conf spark.app.submitTime=1770162029146 \\\\\n",
       "\t--conf spark.authenticate=true \\\\\n",
       "\t--conf spark.databricks.delta.autoCompact.enabled=true \\\\\n",
       "\t--conf spark.databricks.delta.autoCompact.minNumFiles=128 \\\\\n",
       "\t--conf spark.databricks.delta.deletedFileRetentionDuration=7 days \\\\\n",
       "\t--conf spark.databricks.delta.logRetentionDuration=7 days \\\\\n",
       "\t--conf spark.databricks.delta.optimize.maxThreads=128 \\\\\n",
       "\t--conf spark.databricks.delta.optimizeWrite.enabled=true \\\\\n",
       "\t--conf spark.databricks.delta.vacuum.parallelDelete.enabled=true \\\\\n",
       "\t--conf spark.databricks.delta.vacuum.parallelDelete.parallelism=1024 \\\\\n",
       "\t--conf spark.driver.blockManager.port=7079 \\\\\n",
       "\t--conf spark.driver.cores=2 \\\\\n",
       "\t--conf spark.driver.extraJavaOptions=-Djava.net.preferIPv4Stack=true -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:-UseContainerSupport -XX:ParallelGCThreads=4 -XX:ConcGCThreads=2 \\\\\n",
       "\t--conf spark.driver.host=100.67.56.160 \\\\\n",
       "\t--conf spark.driver.memory=16g \\\\\n",
       "\t--conf spark.dynamicAllocation.cachedExecutorIdleTimeout=900s \\\\\n",
       "\t--conf spark.dynamicAllocation.enabled=true \\\\\n",
       "\t--conf spark.dynamicAllocation.executorIdleTimeout=900s \\\\\n",
       "\t--conf spark.dynamicAllocation.initialExecutors=1 \\\\\n",
       "\t--conf spark.dynamicAllocation.maxExecutors=4 \\\\\n",
       "\t--conf spark.dynamicAllocation.minExecutors=0 \\\\\n",
       "\t--conf spark.dynamicAllocation.schedulerBacklogTimeout=10s \\\\\n",
       "\t--conf spark.dynamicAllocation.shuffleTracking.enabled=true \\\\\n",
       "\t--conf spark.dynamicAllocation.shuffleTracking.timeout=900s \\\\\n",
       "\t--conf spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=30s \\\\\n",
       "\t--conf spark.eventLog.dir=s3://spark-k8-eks-xpca/event_logs \\\\\n",
       "\t--conf spark.eventLog.enabled=true \\\\\n",
       "\t--conf spark.eventLog.rolling.enabled=true \\\\\n",
       "\t--conf spark.eventLog.rolling.maxFileSize=32m \\\\\n",
       "\t--conf spark.executor.cores=4 \\\\\n",
       "\t--conf spark.executor.diskSize=200G \\\\\n",
       "\t--conf spark.executor.extraJavaOptions=-Djava.net.preferIPv4Stack=true -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:-UseContainerSupport -XX:ParallelGCThreads=4 -XX:ConcGCThreads=2 \\\\\n",
       "\t--conf spark.executor.memory=16g \\\\\n",
       "\t--conf spark.executor.memoryOverhead=8g \\\\\n",
       "\t--conf spark.executor.processTreeMetrics.enabled=true \\\\\n",
       "\t--conf spark.executor.resource.gpu.amount=1 \\\\\n",
       "\t--conf spark.executor.resource.gpu.discoveryScript=/opt/spark/examples/src/main/scripts/getGpusResources.sh \\\\\n",
       "\t--conf spark.executor.resource.gpu.vendor=nvidia.com \\\\\n",
       "\t--conf spark.hadoop.delta.enableFastS3AListFrom=true \\\\\n",
       "\t--conf spark.hadoop.fs.s3.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.access.key= \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.attempts.maximum=10 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.WebIdentityTokenCredentialsProvider \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.block.size=67108864 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.committer.name=magic \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.connection.maximum=256 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.connection.timeout=50000 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.directory.marker.retention=keep \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.experimental.input.fadvise=random \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.fast.upload=true \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.fast.upload.active.blocks=32 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.fast.upload.default=true \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.impl.disable.cache=false \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.max.total.tasks=1000 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.multipart.size=10485760 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.multipart.threshold=104857600 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.path.style.access=true \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.performance.flag=create,mkdir \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.retry.interval=250ms \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.retry.limit=6 \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.retry.throttle.interval=500ms \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.secret.key= \\\\\n",
       "\t--conf spark.hadoop.fs.s3a.threads.max=512 \\\\\n",
       "\t--conf spark.hive.metastore.uris=thrift://nvspark-metastore.nvspark.svc.cluster.local:9083 \\\\\n",
       "\t--conf spark.io.delta.storage.S3ZookeeperLogStore.fs.read.consistency=true \\\\\n",
       "\t--conf spark.io.delta.storage.S3ZookeeperLogStore.fs.read.retries=3 \\\\\n",
       "\t--conf spark.io.delta.storage.S3ZookeeperLogStore.ttl.seconds=3600 \\\\\n",
       "\t--conf spark.io.delta.storage.S3ZookeeperLogStore.zk.auth.scheme=digest \\\\\n",
       "\t--conf spark.io.delta.storage.S3ZookeeperLogStore.zk.auth.token=username:password \\\\\n",
       "\t--conf spark.io.delta.storage.S3ZookeeperLogStore.zk.basePath=/delta_log \\\\\n",
       "\t--conf spark.io.delta.storage.S3ZookeeperLogStore.zk.connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 \\\\\n",
       "\t--conf spark.jars= \\\\\n",
       "\t--conf spark.kubernetes.authenticate.driver.serviceAccountName=default-editor \\\\\n",
       "\t--conf spark.kubernetes.container.image=900732750576.dkr.ecr.us-west-1.amazonaws.com/kratosxp-platform:sparkaas-spark-kyuubi-3.5.3-1.8.0.6.1-SNAPSHOT-25.08.8 \\\\\n",
       "\t--conf spark.kubernetes.container.image.pullPolicy=Always \\\\\n",
       "\t--conf spark.kubernetes.driver.annotation.sidecar.istio.io/inject=false \\\\\n",
       "\t--conf spark.kubernetes.driver.label.app-name=nvspark \\\\\n",
       "\t--conf spark.kubernetes.driver.label.cluster-type=interactivecluster \\\\\n",
       "\t--conf spark.kubernetes.driver.label.kratos.nvidia.com/kratosxp-userid= \\\\\n",
       "\t--conf spark.kubernetes.driver.label.kyuubi-unique-tag=cfc8b837-b0f7-40ee-b388-391d8316c4dc \\\\\n",
       "\t--conf spark.kubernetes.driver.label.nvsparkaas-cluster-id=cluster-20260203202803-yawkv5ak \\\\\n",
       "\t--conf spark.kubernetes.driver.label.owner=hongy \\\\\n",
       "\t--conf spark.kubernetes.driver.limit.cores=2 \\\\\n",
       "\t--conf spark.kubernetes.driver.node.selector.role=nvspark-driver \\\\\n",
       "\t--conf spark.kubernetes.driver.node.selector.topology.kubernetes.io/zone=us-west-1a \\\\\n",
       "\t--conf spark.kubernetes.driver.ownPersistentVolumeClaim=false \\\\\n",
       "\t--conf spark.kubernetes.driver.pod.name=cluster-20260203202803-yawkv5ak-driver \\\\\n",
       "\t--conf spark.kubernetes.driver.podTemplateFile=s3://spark-k8-eks-xpva/templates/driver-pod-template.yaml \\\\\n",
       "\t--conf spark.kubernetes.driver.request.cores=1 \\\\\n",
       "\t--conf spark.kubernetes.driver.reusePersistentVolumeClaim=false \\\\\n",
       "\t--conf spark.kubernetes.driver.service.label.nvsparkaas-cluster-id=cluster-20260203202803-yawkv5ak \\\\\n",
       "\t--conf spark.kubernetes.executor.annotation.sidecar.istio.io/inject=false \\\\\n",
       "\t--conf spark.kubernetes.executor.label.app-name=nvspark \\\\\n",
       "\t--conf spark.kubernetes.executor.label.cluster-type=interactivecluster \\\\\n",
       "\t--conf spark.kubernetes.executor.label.kratos.nvidia.com/kratosxp-userid= \\\\\n",
       "\t--conf spark.kubernetes.executor.label.nvsparkaas-cluster-id=cluster-20260203202803-yawkv5ak \\\\\n",
       "\t--conf spark.kubernetes.executor.label.owner=hongy \\\\\n",
       "\t--conf spark.kubernetes.executor.limit.cores=4 \\\\\n",
       "\t--conf spark.kubernetes.executor.node.selector.role=nvspark-executor \\\\\n",
       "\t--conf spark.kubernetes.executor.node.selector.topology.kubernetes.io/zone=us-west-1a \\\\\n",
       "\t--conf spark.kubernetes.executor.pod.featureSteps=org.apache.spark.rapids.k8s.SparkExecutorFsGroupFeatureStep,org.apache.spark.rapids.k8s.SparkHostAliasesFeatureStep \\\\\n",
       "\t--conf spark.kubernetes.executor.podNamePrefix=cluster-20260203202803-yawkv5ak \\\\\n",
       "\t--conf spark.kubernetes.executor.podTemplateFile=/opt/spark/pod-template/pod-spec-template.yml \\\\\n",
       "\t--conf spark.kubernetes.executor.request.cores=1 \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-fast-volume.mount.path=/local_disk0/spark-local-dir \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-fast-volume.mount.readOnly=false \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-fast-volume.options.sizeLimit=200G \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly=false \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName=OnDemand \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit=200Gi \\\\\n",
       "\t--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass=gp3 \\\\\n",
       "\t--conf spark.kubernetes.file.upload.path=s3://spark-k8-eks-xpca/spark_upload \\\\\n",
       "\t--conf spark.kubernetes.memoryOverheadFactor=0.1 \\\\\n",
       "\t--conf spark.kubernetes.namespace=dcartm-team \\\\\n",
       "\t--conf spark.kubernetes.resource.type=java \\\\\n",
       "\t--conf spark.kubernetes.submission.waitAppCompletion=false \\\\\n",
       "\t--conf spark.kubernetes.submitInDriver=true \\\\\n",
       "\t--conf spark.kyuubi.ha.addresses=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 \\\\\n",
       "\t--conf spark.locality.wait=0 \\\\\n",
       "\t--conf spark.master=k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com \\\\\n",
       "\t--conf spark.plugins=com.nvidia.spark.SQLPlugin,org.apache.spark.rapids.k8s.WheelInstallPlugin \\\\\n",
       "\t--conf spark.rapids.memory.pinnedPool.size=2G \\\\\n",
       "\t--conf spark.rapids.shuffle.mode=MULTITHREADED \\\\\n",
       "\t--conf spark.rapids.sql.adaptive.skewJoin.broadcast.enabled=true \\\\\n",
       "\t--conf spark.rapids.sql.batchSizeBytes=2147483646 \\\\\n",
       "\t--conf spark.rapids.sql.coalescing.reader.numFilterParallel=2 \\\\\n",
       "\t--conf spark.rapids.sql.concurrentGpuTasks=2 \\\\\n",
       "\t--conf spark.rapids.sql.multiThreadedRead.numThreads=40 \\\\\n",
       "\t--conf spark.redaction.regex=(?i)secret|password|token|driverEnv|executorEnv|access[.]key \\\\\n",
       "\t--conf spark.scheduler.mode=FAIR \\\\\n",
       "\t--conf spark.shuffle.manager=com.nvidia.spark.rapids.spark353.RapidsShuffleManager \\\\\n",
       "\t--conf spark.sparkaas.groupprovider.static.group=default \\\\\n",
       "\t--conf spark.sparkaas.prometheus.servicemonitor.enable=true \\\\\n",
       "\t--conf spark.sparkaas.sparkui.service.type=ClusterIP \\\\\n",
       "\t--conf spark.sql.cache.serializer=com.nvidia.spark.ParquetCachedBatchSerializer \\\\\n",
       "\t--conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \\\\\n",
       "\t--conf spark.sql.execution.arrow.pyspark.enabled=true \\\\\n",
       "\t--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \\\\\n",
       "\t--conf spark.sql.files.maxPartitionBytes=2048m \\\\\n",
       "\t--conf spark.sql.parquet.datetimeRebaseModeInRead=CORRECTED \\\\\n",
       "\t--conf spark.sql.parquet.datetimeRebaseModeInWrite=EXCEPTION \\\\\n",
       "\t--conf spark.sql.parquet.int96RebaseModeInRead=CORRECTED \\\\\n",
       "\t--conf spark.sql.parquet.int96RebaseModeInWrite=EXCEPTION \\\\\n",
       "\t--conf spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter \\\\\n",
       "\t--conf spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol \\\\\n",
       "\t--conf spark.sql.streaming.metricsEnabled=true \\\\\n",
       "\t--conf spark.sql.streaming.streamingQueryListeners=org.apache.spark.sql.streaming.kratos.AppAnalyticsListener,org.apache.spark.sql.kratos.KratosQueryListener \\\\\n",
       "\t--conf spark.sql.warehouse.dir=s3a://kratos-spark-hive/datalake \\\\\n",
       "\t--conf spark.submit.deployMode=client \\\\\n",
       "\t--conf spark.submit.pyFiles= \\\\\n",
       "\t--conf spark.task.cpus=1 \\\\\n",
       "\t--conf spark.task.resource.gpu.amount=0.001 \\\\\n",
       "\t--conf spark.ui.prometheus.enabled=true \\\\\n",
       "\t--conf spark.ui.proxyBase=/nvspark/ui/dcartm-team/hongy \\\\\n",
       "\t--conf spark.ui.proxyRedirectUri=/ \\\\\n",
       "\t--conf spark.ui.view.acls.groups=default \\\\\n",
       "\t--conf spark.sparkaas.transaction.id= \\\\\n",
       "\t--conf spark.kubernetes.driverEnv.SPARK_USER_NAME=default \\\\\n",
       "\t--conf spark.executorEnv.SPARK_USER_NAME=default \\\\\n",
       "\t--proxy-user default /opt/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar\n",
       "2026-02-03T23:40:44,540Z INFO ProcBuilder: Logging to /opt/kyuubi/work/default/kyuubi-spark-sql-engine.log.0\n",
       "2026-02-03T23:40:45,137Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260203234024-mmysc0ll\n",
       "2026-02-03T23:40:45,738Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:45,739Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:45,744Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:45,744Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@30dbb3b5\n",
       "2026-02-03T23:40:45,745Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:45,745Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:45,746Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:45,746Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:45,747Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:40:45,747Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:45,749Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:38486, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:40:45,753Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f5cc, negotiated timeout = 120000\n",
       "2026-02-03T23:40:45,804Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:45,808Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:45,808Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:45,809Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:45,810Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:45,924Z INFO ZooKeeper: Session: 0x30263a585b4f5cc closed\n",
       "2026-02-03T23:40:45,924Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f5cc\n",
       "2026-02-03T23:40:45,939Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:46,243Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:47,404Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:47,404Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:47,406Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:47,407Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@1087358\n",
       "2026-02-03T23:40:47,408Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:47,409Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:47,410Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:40:47,410Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:47,410Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:47,410Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:47,412Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:45418, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:40:47,416Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f5ea, negotiated timeout = 120000\n",
       "2026-02-03T23:40:47,416Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:47,419Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:47,419Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:47,420Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:47,421Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:47,525Z INFO ZooKeeper: Session: 0x30263a585b4f5ea closed\n",
       "2026-02-03T23:40:47,525Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f5ea\n",
       "2026-02-03T23:40:47,537Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:47,825Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:48,970Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:48,970Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:48,972Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:48,972Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@21f11677\n",
       "2026-02-03T23:40:48,972Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:48,973Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:48,973Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:48,973Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:48,974Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-03T23:40:48,974Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:48,975Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:44632, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-03T23:40:48,979Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f3f128, negotiated timeout = 120000\n",
       "2026-02-03T23:40:48,979Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:48,981Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:48,981Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:48,981Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:48,981Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:49,086Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f3f128\n",
       "2026-02-03T23:40:49,086Z INFO ZooKeeper: Session: 0x200d89556f3f128 closed\n",
       "2026-02-03T23:40:49,100Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:49+0000 INFO sparksLauncher Spark driver running with pid: 103\n",
       "2026-02-03T23:40:49+0000 INFO sparksLauncher Found Spark driver with PID: 103\n",
       "2026-02-03T23:40:49+0000 INFO sparksLauncher Starting log rotation thread for /opt/kyuubi/work/default/kyuubi-spark-sql-engine.log.0\n",
       "2026-02-03T23:40:49+0000 INFO sparksLauncher tailing log file: /opt/kyuubi/work/default/kyuubi-spark-sql-engine.log.0\n",
       "2.123: [GC (Metadata GC Threshold) [PSYoungGen: 203998K->14289K(594944K)] 203998K->14305K(1954816K), 0.0157699 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \n",
       "2.139: [Full GC (Metadata GC Threshold) [PSYoungGen: 14289K->0K(594944K)] [ParOldGen: 16K->13631K(855552K)] 14305K->13631K(1450496K), [Metaspace: 20367K->20352K(1067008K)], 0.0297589 secs] [Times: user=0.06 sys=0.01, real=0.03 secs] \n",
       "Running driver with proxy user. Cluster manager: Kubernetes\n",
       "26/02/03 23:40:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
       "26/02/03 23:40:47 INFO SignalRegister: Registering signal handler for TERM\n",
       "26/02/03 23:40:47 INFO SignalRegister: Registering signal handler for HUP\n",
       "26/02/03 23:40:47 INFO SignalRegister: Registering signal handler for INT\n",
       "26/02/03 23:40:48 INFO HiveConf: Found configuration file null\n",
       "26/02/03 23:40:48 WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "26/02/03 23:40:48 WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "26/02/03 23:40:48 INFO SparkContext: Running Spark version 3.5.3\n",
       "26/02/03 23:40:48 INFO SparkContext: OS info Linux, 6.1.148-173.267.amzn2023.x86_64, amd64\n",
       "26/02/03 23:40:48 INFO SparkContext: Java version 1.8.0_472\n",
       "26/02/03 23:40:48 INFO ResourceUtils: ==============================================================\n",
       "26/02/03 23:40:48 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
       "26/02/03 23:40:48 INFO ResourceUtils: ==============================================================\n",
       "26/02/03 23:40:48 INFO SparkContext: Submitted application: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "26/02/03 23:40:48 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(gpu -> name: gpu, amount: 1, script: /opt/spark/examples/src/main/scripts/getGpusResources.sh, vendor: nvidia.com, cores -> name: cores, amount: 4, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: , memoryOverhead -> name: memoryOverhead, amount: 8192, script: , vendor: , memory -> name: memory, amount: 16384, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0, gpu -> name: gpu, amount: 0.001)\n",
       "26/02/03 23:40:48 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor\n",
       "26/02/03 23:40:48 WARN ResourceUtils: The configuration of resource: gpu (exec = 1, task = 0.001/1000, runnable tasks = 1000) will result in wasted resources due to resource cpus limiting the number of runnable tasks per executor to: 4. Please adjust your configuration.\n",
       "26/02/03 23:40:48 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
       "26/02/03 23:40:48 INFO SecurityManager: Changing view acls to: root,spring\n",
       "26/02/03 23:40:48 INFO SecurityManager: Changing modify acls to: root,spring\n",
       "26/02/03 23:40:48 INFO SecurityManager: Changing view acls groups to: default\n",
       "26/02/03 23:40:48 INFO SecurityManager: Changing modify acls groups to: \n",
       "26/02/03 23:40:48 INFO SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users with view permissions: root, spring; groups with view permissions: default; users with modify permissions: root, spring; groups with modify permissions: EMPTY\n",
       "3.670: [GC (Metadata GC Threshold) [PSYoungGen: 228069K->12521K(594944K)] 241700K->26160K(1450496K), 0.0123445 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \n",
       "3.682: [Full GC (Metadata GC Threshold) [PSYoungGen: 12521K->0K(594944K)] [ParOldGen: 13639K->17006K(1443328K)] 26160K->17006K(2038272K), [Metaspace: 33143K->33143K(1079296K)], 0.0337772 secs] [Times: user=0.04 sys=0.03, real=0.04 secs] \n",
       "26/02/03 23:40:48 INFO Utils: Successfully started service 'sparkDriver' on port 46445.\n",
       "26/02/03 23:40:48 INFO SparkEnv: Registering MapOutputTracker\n",
       "26/02/03 23:40:48 INFO SparkEnv: Registering BlockManagerMaster\n",
       "26/02/03 23:40:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
       "26/02/03 23:40:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
       "26/02/03 23:40:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
       "26/02/03 23:40:48 INFO DiskBlockManager: Created local directory at /var/data/spark-dbc00d20-334d-441c-a507-991867431d91/blockmgr-8d6c021f-c1f3-4f1e-b866-57361203c52c\n",
       "26/02/03 23:40:48 INFO MemoryStore: MemoryStore started with capacity 8.4 GiB\n",
       "26/02/03 23:40:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
       "2026-02-03T23:40:49,405Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:40:49 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
       "26/02/03 23:40:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
       "26/02/03 23:40:49 INFO SparkContext: Added JAR file:/opt/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar at spark://100.67.56.160:46445/jars/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar with timestamp 1770162048311\n",
       "26/02/03 23:40:49 INFO ShimLoader: Loading shim for Spark version: 3.5.3\n",
       "26/02/03 23:40:49 INFO ShimLoader: Complete Spark build info: 3.5.3, https://github.com/apache/spark, HEAD, 32232e9ed33bb16b93ad58cfde8b82e0f07c0970, 2024-09-09T05:20:05Z\n",
       "26/02/03 23:40:49 INFO ShimLoader: Scala version: version 2.12.18\n",
       "26/02/03 23:40:49 INFO ShimLoader: findURLClassLoader found a URLClassLoader org.apache.spark.util.MutableURLClassLoader@1e1d3956\n",
       "26/02/03 23:40:49 INFO ShimLoader: Updating spark classloader org.apache.spark.util.MutableURLClassLoader@1e1d3956 with the URLs: jar:file:/opt/spark-3.5.3-bin-hadoop3/jars/rapids-4-spark_2.12-25.10.1-cuda12.jar!/spark-shared/, jar:file:/opt/spark-3.5.3-bin-hadoop3/jars/rapids-4-spark_2.12-25.10.1-cuda12.jar!/spark353/\n",
       "26/02/03 23:40:49 INFO ShimLoader: Spark classLoader org.apache.spark.util.MutableURLClassLoader@1e1d3956 updated successfully\n",
       "26/02/03 23:40:49 INFO ShimLoader: Updating spark classloader org.apache.spark.util.MutableURLClassLoader@1e1d3956 with the URLs: jar:file:/opt/spark-3.5.3-bin-hadoop3/jars/rapids-4-spark_2.12-25.10.1-cuda12.jar!/spark-shared/, jar:file:/opt/spark-3.5.3-bin-hadoop3/jars/rapids-4-spark_2.12-25.10.1-cuda12.jar!/spark353/\n",
       "26/02/03 23:40:49 INFO ShimLoader: Spark classLoader org.apache.spark.util.MutableURLClassLoader@1e1d3956 updated successfully\n",
       "26/02/03 23:40:49 INFO RapidsPluginUtils: RAPIDS Accelerator build: Map(url -> https://gitlab-master.nvidia.com/nvspark/spark-rapids.git, branch -> HEAD, revision -> da53f95521101bfca6eb83b57c8bfb1d725390de, version -> 25.10.1, date -> 2025-11-20T11:51:40Z, cudf_version -> 25.10.1, user -> root)\n",
       "26/02/03 23:40:49 INFO RapidsPluginUtils: RAPIDS Accelerator JNI build: Map(url -> https://gitlab-master.nvidia.com/nvspark/spark-rapids-jni.git, branch -> HEAD, gpu_architectures -> 100;120;70;75;80;86;90, revision -> 2f73417c7045374182c6c3cf3b97e024677d9449, version -> 25.10.1, date -> 2025-11-11T13:47:07Z, user -> root)\n",
       "26/02/03 23:40:49 INFO RapidsPluginUtils: cudf build: Map(url -> https://gitlab-master.nvidia.com/RAPIDS/cudf, branch -> HEAD, gpu_architectures -> 100;120;70;75;80;86;90, revision -> 46537ea1ad0aac34664cccce818172b03ff6b064, version -> 25.10.1, date -> 2025-11-11T13:47:04Z, user -> root)\n",
       "26/02/03 23:40:49 INFO RapidsPluginUtils: RAPIDS Accelerator Private Map(url -> https://gitlab-master.nvidia.com/nvspark/spark-rapids-private.git, branch -> HEAD, revision -> 28e7c65b3c0526baba384d9b97ee12e4d6766d9a, version -> 25.10.0, date -> 2025-10-10T05:45:36Z, user -> root)\n",
       "26/02/03 23:40:49 WARN RapidsPluginUtils: RAPIDS Accelerator 25.10.1 using cudf 25.10.1, private revision 28e7c65b3c0526baba384d9b97ee12e4d6766d9a\n",
       "26/02/03 23:40:50 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set \\`spark.rapids.sql.enabled\\` to false.\n",
       "26/02/03 23:40:50 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to \\`NOT_ON_GPU\\`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n",
       "26/02/03 23:40:50 WARN RapidsShuffleInternalManagerBase: Rapids Shuffle Plugin enabled. Multi-threaded shuffle mode (write threads=20, read threads=20). To disable the RAPIDS Shuffle Manager set \\`spark.rapids.shuffle.enabled\\` to false\n",
       "26/02/03 23:40:50 INFO DriverPluginContainer: Initialized driver component for plugin com.nvidia.spark.SQLPlugin.\n",
       "26/02/03 23:40:50 INFO DriverPluginContainer: Initialized driver component for plugin org.apache.spark.rapids.k8s.WheelInstallPlugin.\n",
       "2026-02-03T23:40:50,534Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:50,534Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:50,536Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:50,536Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@7c8458ca\n",
       "2026-02-03T23:40:50,536Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:50,537Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:50,537Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:50,537Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:40:50,538Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:50,538Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:50,538Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:55098, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:40:50,543Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a490700a3, negotiated timeout = 120000\n",
       "2026-02-03T23:40:50,543Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:50,545Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:50,545Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:50,545Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:50,545Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:50,650Z INFO ZooKeeper: Session: 0x100f75a490700a3 closed\n",
       "2026-02-03T23:40:50,650Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a490700a3\n",
       "2026-02-03T23:40:50,654Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:50,921Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:40:50 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
       "2026-02-03T23:40:52,064Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:52,064Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:52,065Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:52,065Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@3bbae00e\n",
       "2026-02-03T23:40:52,066Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:52,066Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:52,066Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:40:52,067Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:52,067Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:52,067Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:52,068Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:45420, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:40:52,072Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f631, negotiated timeout = 120000\n",
       "2026-02-03T23:40:52,073Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:52,102Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:52,103Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:52,103Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:52,103Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "6.772: [GC (Metadata GC Threshold) [PSYoungGen: 336573K->25299K(594944K)] 353580K->42321K(2038272K), 0.0172507 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] \n",
       "6.790: [Full GC (Metadata GC Threshold) [PSYoungGen: 25299K->0K(594944K)] [ParOldGen: 17022K->32651K(2131456K)] 42321K->32651K(2726400K), [Metaspace: 55060K->54920K(1099776K)], 0.1361095 secs] [Times: user=0.22 sys=0.02, real=0.14 secs] \n",
       "2026-02-03T23:40:52,208Z INFO ZooKeeper: Session: 0x30263a585b4f631 closed\n",
       "2026-02-03T23:40:52,213Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:52,214Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f631\n",
       "2026-02-03T23:40:52,408Z INFO KyuubiSessionImpl: [anonymous:100.67.56.160] SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc] - Starting to wait the launch engine operation finished\n",
       "2026-02-03T23:40:52,497Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:40:52 INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
       "26/02/03 23:40:52 INFO FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1\n",
       "26/02/03 23:40:52 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
       "26/02/03 23:40:52 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : log4j2.properties,nvoauth.conf,metrics.properties\n",
       "26/02/03 23:40:52 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : log4j2.properties,nvoauth.conf,metrics.properties\n",
       "26/02/03 23:40:52 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
       "26/02/03 23:40:53 INFO SparkExecutorFsGroupFeatureStep: SparkExecutorFsGroupFeatureStep configure security context fsGroup\n",
       "2026-02-03T23:40:53,624Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:53,624Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:53,625Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:53,626Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@6787d1e3\n",
       "2026-02-03T23:40:53,626Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:53,626Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:53,627Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:40:53,627Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:53,627Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:53,627Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:53,628Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:55110, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:40:53,700Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a490700c7, negotiated timeout = 120000\n",
       "2026-02-03T23:40:53,701Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:53,702Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:53,703Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:53,703Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:53,703Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:53,808Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a490700c7\n",
       "2026-02-03T23:40:53,808Z INFO ZooKeeper: Session: 0x100f75a490700c7 closed\n",
       "2026-02-03T23:40:53,813Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:54,098Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:40:53 INFO ExecutorPodsAllocator: Trying to create PersistentVolumeClaim cluster-20260203202803-yawkv5ak-exec-1-pvc-0 with StorageClass gp3\n",
       "26/02/03 23:40:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
       "26/02/03 23:40:53 INFO NettyBlockTransferService: Server created on 100.67.56.160:7079\n",
       "26/02/03 23:40:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
       "26/02/03 23:40:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 100.67.56.160, 7079, None)\n",
       "26/02/03 23:40:53 INFO BlockManagerMasterEndpoint: Registering block manager 100.67.56.160:7079 with 8.4 GiB RAM, BlockManagerId(driver, 100.67.56.160, 7079, None)\n",
       "26/02/03 23:40:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 100.67.56.160, 7079, None)\n",
       "26/02/03 23:40:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 100.67.56.160, 7079, None)\n",
       "26/02/03 23:40:53 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
       "26/02/03 23:40:53 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
       "26/02/03 23:40:53 INFO MetricsSystemImpl: s3a-file-system metrics system started\n",
       "26/02/03 23:40:53 WARN CredentialProviderListFactory: Credentials option fs.s3a.aws.credentials.provider contains AWS v1 SDK entry com.amazonaws.auth.WebIdentityTokenCredentialsProvider\n",
       "9.978: [GC (Allocation Failure) [PSYoungGen: 509952K->32001K(678912K)] 542603K->64660K(2810368K), 0.0285534 secs] [Times: user=0.05 sys=0.02, real=0.03 secs] \n",
       "2026-02-03T23:40:55,240Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:55,240Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:55,241Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:55,242Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@106ed2d7\n",
       "2026-02-03T23:40:55,242Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:55,242Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:55,243Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:40:55,243Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:55,243Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:55,243Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:55,244Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:45426, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:40:55,249Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f653, negotiated timeout = 120000\n",
       "2026-02-03T23:40:55,249Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:55,251Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:55,251Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:55,251Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:55,251Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:55,355Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f653\n",
       "2026-02-03T23:40:55,355Z INFO ZooKeeper: Session: 0x30263a585b4f653 closed\n",
       "2026-02-03T23:40:55,359Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:55,703Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:40:55 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
       "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
       "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
       "The AWS SDK for Java 1.x is being used here:\n",
       "at java.lang.Thread.getStackTrace(Thread.java:1564)\n",
       "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
       "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
       "at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)\n",
       "at com.amazonaws.auth.STSAssumeRoleWithWebIdentitySessionCredentialsProvider.buildStsClient(STSAssumeRoleWithWebIdentitySessionCredentialsProvider.java:133)\n",
       "at com.amazonaws.auth.STSAssumeRoleWithWebIdentitySessionCredentialsProvider.<init>(STSAssumeRoleWithWebIdentitySessionCredentialsProvider.java:111)\n",
       "at com.amazonaws.auth.STSAssumeRoleWithWebIdentitySessionCredentialsProvider.<init>(STSAssumeRoleWithWebIdentitySessionCredentialsProvider.java:52)\n",
       "at com.amazonaws.auth.STSAssumeRoleWithWebIdentitySessionCredentialsProvider\\$Builder.build(STSAssumeRoleWithWebIdentitySessionCredentialsProvider.java:241)\n",
       "at com.amazonaws.services.securitytoken.internal.STSProfileCredentialsService.getAssumeRoleCredentialsProvider(STSProfileCredentialsService.java:40)\n",
       "at com.amazonaws.auth.profile.internal.securitytoken.STSProfileCredentialsServiceProvider.getProfileCredentialsProvider(STSProfileCredentialsServiceProvider.java:39)\n",
       "at com.amazonaws.auth.profile.internal.securitytoken.STSProfileCredentialsServiceProvider.getCredentials(STSProfileCredentialsServiceProvider.java:71)\n",
       "at com.amazonaws.auth.WebIdentityTokenCredentialsProvider.getCredentials(WebIdentityTokenCredentialsProvider.java:87)\n",
       "at org.apache.hadoop.fs.s3a.adapter.V1ToV2AwsCredentialProviderAdapter.resolveCredentials(V1ToV2AwsCredentialProviderAdapter.java:76)\n",
       "at org.apache.hadoop.fs.s3a.AWSCredentialProviderList.resolveCredentials(AWSCredentialProviderList.java:175)\n",
       "at software.amazon.awssdk.auth.credentials.AwsCredentialsProvider.resolveIdentity(AwsCredentialsProvider.java:54)\n",
       "at software.amazon.awssdk.services.s3.auth.scheme.internal.S3AuthSchemeInterceptor.lambda\\$trySelectAuthScheme\\$3(S3AuthSchemeInterceptor.java:152)\n",
       "at software.amazon.awssdk.core.internal.util.MetricUtils.reportDuration(MetricUtils.java:77)\n",
       "at software.amazon.awssdk.services.s3.auth.scheme.internal.S3AuthSchemeInterceptor.trySelectAuthScheme(S3AuthSchemeInterceptor.java:152)\n",
       "at software.amazon.awssdk.services.s3.auth.scheme.internal.S3AuthSchemeInterceptor.selectAuthScheme(S3AuthSchemeInterceptor.java:83)\n",
       "at software.amazon.awssdk.services.s3.auth.scheme.internal.S3AuthSchemeInterceptor.beforeExecution(S3AuthSchemeInterceptor.java:63)\n",
       "at software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.lambda\\$beforeExecution\\$1(ExecutionInterceptorChain.java:59)\n",
       "at java.util.ArrayList.forEach(ArrayList.java:1259)\n",
       "at software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.beforeExecution(ExecutionInterceptorChain.java:59)\n",
       "at software.amazon.awssdk.awscore.internal.AwsExecutionContextBuilder.runInitialInterceptors(AwsExecutionContextBuilder.java:239)\n",
       "at software.amazon.awssdk.awscore.internal.AwsExecutionContextBuilder.invokeInterceptorsAndCreateExecutionContext(AwsExecutionContextBuilder.java:130)\n",
       "at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.invokeInterceptorsAndCreateExecutionContext(AwsSyncClientHandler.java:67)\n",
       "at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda\\$execute\\$1(BaseSyncClientHandler.java:76)\n",
       "at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n",
       "at software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\n",
       "at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\n",
       "at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\n",
       "at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:6319)\n",
       "at software.amazon.awssdk.services.s3.DelegatingS3Client.lambda\\$headObject\\$53(DelegatingS3Client.java:5053)\n",
       "at software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\n",
       "at software.amazon.awssdk.services.s3.DelegatingS3Client.headObject(DelegatingS3Client.java:5053)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda\\$getObjectMetadata\\$10(S3AFileSystem.java:3049)\n",
       "at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n",
       "at org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:3036)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:3016)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4079)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4007)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda\\$getFileStatus\\$22(S3AFileSystem.java:3984)\n",
       "at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n",
       "at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda\\$trackDurationOfOperation\\$5(IOStatisticsBinding.java:528)\n",
       "at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2865)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2884)\n",
       "at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3982)\n",
       "at org.apache.spark.deploy.history.EventLogFileWriter.requireLogBaseDirAsDirectory(EventLogFileWriters.scala:77)\n",
       "at org.apache.spark.deploy.history.RollingEventLogFilesWriter.start(EventLogFileWriters.scala:311)\n",
       "at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:81)\n",
       "at org.apache.spark.SparkContext.<init>(SparkContext.scala:632)\n",
       "at org.apache.spark.SparkContext\\$.getOrCreate(SparkContext.scala:2883)\n",
       "at org.apache.spark.sql.SparkSession\\$Builder.\\$anonfun\\$getOrCreate\\$2(SparkSession.scala:1099)\n",
       "at scala.Option.getOrElse(Option.scala:189)\n",
       "at org.apache.spark.sql.SparkSession\\$Builder.getOrCreate(SparkSession.scala:1093)\n",
       "at org.apache.kyuubi.engine.spark.SparkSQLEngine\\$.createSpark(SparkSQLEngine.scala:253)\n",
       "at org.apache.kyuubi.engine.spark.SparkSQLEngine\\$.main(SparkSQLEngine.scala:326)\n",
       "at org.apache.kyuubi.engine.spark.SparkSQLEngine.main(SparkSQLEngine.scala)\n",
       "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "at java.lang.reflect.Method.invoke(Method.java:498)\n",
       "at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
       "at org.apache.spark.deploy.SparkSubmit.org\\$apache\\$spark\\$deploy\\$SparkSubmit\\$\\$runMain(SparkSubmit.scala:1029)\n",
       "at org.apache.spark.deploy.SparkSubmit.\\$anonfun\\$submit\\$2(SparkSubmit.scala:169)\n",
       "at org.apache.spark.deploy.SparkHadoopUtil\\$\\$anon\\$1.run(SparkHadoopUtil.scala:62)\n",
       "at org.apache.spark.deploy.SparkHadoopUtil\\$\\$anon\\$1.run(SparkHadoopUtil.scala:61)\n",
       "at java.security.AccessController.doPrivileged(Native Method)\n",
       "at javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)\n",
       "at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)\n",
       "at org.apache.spark.deploy.SparkSubmit.doRunMain\\$1(SparkSubmit.scala:169)\n",
       "at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
       "at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
       "at org.apache.spark.deploy.SparkSubmit\\$\\$anon\\$2.doSubmit(SparkSubmit.scala:1120)\n",
       "at org.apache.spark.deploy.SparkSubmit\\$.main(SparkSubmit.scala:1129)\n",
       "at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
       "2026-02-03T23:40:56,846Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:56,847Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:56,847Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:56,848Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@486a6ccc\n",
       "2026-02-03T23:40:56,848Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:56,848Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:56,849Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-03T23:40:56,849Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:56,849Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:56,849Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:56,850Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:44634, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-03T23:40:56,854Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f3f181, negotiated timeout = 120000\n",
       "2026-02-03T23:40:56,854Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:56,856Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:56,856Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:56,856Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:56,856Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:56,961Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f3f181\n",
       "2026-02-03T23:40:56,961Z INFO ZooKeeper: Session: 0x200d89556f3f181 closed\n",
       "2026-02-03T23:40:56,964Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:57,245Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:40:57 INFO RollingEventLogFilesWriter: Logging events to s3://spark-k8-eks-xpca/event_logs/eventlog_v2_spark-cdd9e5d0115345edb431647f51f82517/events_1_spark-cdd9e5d0115345edb431647f51f82517\n",
       "12.350: [GC (Metadata GC Threshold) [PSYoungGen: 369934K->26626K(710144K)] 402593K->59294K(2841600K), 0.0236452 secs] [Times: user=0.07 sys=0.01, real=0.03 secs] \n",
       "12.374: [Full GC (Metadata GC Threshold) [PSYoungGen: 26626K->0K(710144K)] [ParOldGen: 32667K->44315K(3088384K)] 59294K->44315K(3798528K), [Metaspace: 91652K->91652K(1134592K)], 0.1148764 secs] [Times: user=0.27 sys=0.00, real=0.11 secs] \n",
       "26/02/03 23:40:57 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to event_logs/eventlog_v2_spark-cdd9e5d0115345edb431647f51f82517/events_1_spark-cdd9e5d0115345edb431647f51f82517. This is Unsupported\n",
       "26/02/03 23:40:57 INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
       "26/02/03 23:40:57 INFO ExecutorAllocationManager: Dynamic allocation is enabled without a shuffle service.\n",
       "2026-02-03T23:40:58,373Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:58,374Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:58,375Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:58,375Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@ef1c389\n",
       "2026-02-03T23:40:58,375Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:58,375Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:58,376Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:58,376Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:40:58,376Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:58,376Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:58,378Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54658, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:40:58,381Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f672, negotiated timeout = 120000\n",
       "2026-02-03T23:40:58,382Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:58,384Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:58,384Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:58,384Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:58,384Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:58,488Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f672\n",
       "2026-02-03T23:40:58,488Z INFO ZooKeeper: Session: 0x30263a585b4f672 closed\n",
       "2026-02-03T23:40:58,492Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:58,774Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:40:59,917Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:40:59,917Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:40:59,918Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:40:59,918Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@509b4f1f\n",
       "2026-02-03T23:40:59,919Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:40:59,919Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:40:59,919Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:40:59,919Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:40:59,919Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:40:59,919Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:59,921Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54672, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:40:59,925Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f680, negotiated timeout = 120000\n",
       "2026-02-03T23:40:59,925Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:40:59,928Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:40:59,928Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:40:59,928Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:40:59,928Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:00,033Z INFO ZooKeeper: Session: 0x30263a585b4f680 closed\n",
       "2026-02-03T23:41:00,033Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f680\n",
       "2026-02-03T23:41:00,036Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:00,302Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:01,445Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:01,446Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:01,447Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:01,447Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@43bdaf88\n",
       "2026-02-03T23:41:01,448Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:01,448Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:01,448Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:01,448Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:01,449Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:01,449Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:01,450Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54678, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:01,454Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f694, negotiated timeout = 120000\n",
       "2026-02-03T23:41:01,454Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:01,456Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:01,456Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:01,457Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:01,457Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:01,561Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f694\n",
       "2026-02-03T23:41:01,561Z INFO ZooKeeper: Session: 0x30263a585b4f694 closed\n",
       "2026-02-03T23:41:01,566Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:01,846Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:02,972Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:02,972Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:02,973Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:02,974Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@26dced82\n",
       "2026-02-03T23:41:02,974Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:02,974Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:02,975Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:02,975Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:02,975Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:02,975Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:02,976Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54694, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:02,980Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f6a1, negotiated timeout = 120000\n",
       "2026-02-03T23:41:02,981Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:02,982Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:02,982Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:02,983Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:02,983Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:03,087Z INFO ZooKeeper: Session: 0x30263a585b4f6a1 closed\n",
       "2026-02-03T23:41:03,087Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f6a1\n",
       "2026-02-03T23:41:03,091Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:03,358Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:04,500Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:04,500Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:04,502Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:04,502Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@5e6d59e0\n",
       "2026-02-03T23:41:04,503Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:04,503Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:04,504Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:04,504Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:04,504Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:04,504Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:04,505Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54698, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:04,509Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f6b6, negotiated timeout = 120000\n",
       "2026-02-03T23:41:04,509Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:04,511Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:04,512Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:04,512Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:04,512Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:04,616Z INFO ZooKeeper: Session: 0x30263a585b4f6b6 closed\n",
       "2026-02-03T23:41:04,616Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f6b6\n",
       "2026-02-03T23:41:04,620Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:04,891Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:06,035Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:06,035Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:06,036Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:06,036Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@5f291d7a\n",
       "2026-02-03T23:41:06,037Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:06,037Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:06,038Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:06,038Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:06,038Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:06,038Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:06,040Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54710, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:06,044Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f6c6, negotiated timeout = 120000\n",
       "2026-02-03T23:41:06,044Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:06,046Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:06,047Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:06,047Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:06,047Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:06,152Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f6c6\n",
       "2026-02-03T23:41:06,152Z INFO ZooKeeper: Session: 0x30263a585b4f6c6 closed\n",
       "2026-02-03T23:41:06,158Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:06,444Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:07,572Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:07,572Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:07,573Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:07,574Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4113766b\n",
       "2026-02-03T23:41:07,574Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:07,574Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:07,575Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-03T23:41:07,575Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:07,575Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:07,575Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:07,576Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:52420, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-03T23:41:07,580Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f3f1f4, negotiated timeout = 120000\n",
       "2026-02-03T23:41:07,580Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:07,582Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:07,582Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:07,582Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:07,582Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:07,687Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f3f1f4\n",
       "2026-02-03T23:41:07,687Z INFO ZooKeeper: Session: 0x200d89556f3f1f4 closed\n",
       "2026-02-03T23:41:07,691Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:07,975Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:09,121Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:09,121Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:09,122Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:09,122Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@1ccd40cd\n",
       "2026-02-03T23:41:09,123Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:09,123Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:09,124Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:09,124Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:41:09,124Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:09,124Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:09,125Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:38066, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:41:09,130Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a4907016e, negotiated timeout = 120000\n",
       "2026-02-03T23:41:09,130Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:09,131Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:09,131Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:09,131Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:09,132Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:09,237Z INFO ZooKeeper: Session: 0x100f75a4907016e closed\n",
       "2026-02-03T23:41:09,237Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a4907016e\n",
       "2026-02-03T23:41:09,242Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:09,533Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:10,676Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:10,676Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:10,677Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:10,677Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@1eedcae9\n",
       "2026-02-03T23:41:10,677Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:10,678Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:10,678Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:10,678Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-03T23:41:10,678Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:10,678Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:10,679Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:52428, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-03T23:41:10,684Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f3f213, negotiated timeout = 120000\n",
       "2026-02-03T23:41:10,684Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:10,685Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:10,685Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:10,685Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:10,685Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:10,790Z INFO ZooKeeper: Session: 0x200d89556f3f213 closed\n",
       "2026-02-03T23:41:10,790Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f3f213\n",
       "2026-02-03T23:41:10,795Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:11,079Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:12,227Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:12,227Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:12,228Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:12,228Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@72fee90\n",
       "2026-02-03T23:41:12,229Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:12,229Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:12,230Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:12,230Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-03T23:41:12,230Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:12,230Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:12,231Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:52444, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-03T23:41:12,235Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f3f22c, negotiated timeout = 120000\n",
       "2026-02-03T23:41:12,235Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:12,237Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:12,237Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:12,237Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:12,237Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:12,342Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f3f22c\n",
       "2026-02-03T23:41:12,342Z INFO ZooKeeper: Session: 0x200d89556f3f22c closed\n",
       "2026-02-03T23:41:12,346Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:12,367Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 1\n",
       "2026-02-03T23:41:12,632Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:13,781Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:13,781Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:13,782Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:13,782Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@78ea84db\n",
       "2026-02-03T23:41:13,783Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:13,783Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:13,784Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:13,784Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:13,784Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:13,784Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:13,786Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:53926, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:13,790Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f71e, negotiated timeout = 120000\n",
       "2026-02-03T23:41:13,790Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:13,792Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:13,792Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:13,792Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:13,793Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:13,897Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f71e\n",
       "2026-02-03T23:41:13,897Z INFO ZooKeeper: Session: 0x30263a585b4f71e closed\n",
       "2026-02-03T23:41:13,902Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:14,177Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:15,331Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:15,331Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:15,332Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:15,333Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4a170022\n",
       "2026-02-03T23:41:15,333Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:15,333Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:15,334Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:15,334Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:15,338Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:15,338Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:15,340Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:53940, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:15,344Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f736, negotiated timeout = 120000\n",
       "2026-02-03T23:41:15,344Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:15,346Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:15,347Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:15,347Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:15,350Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:15,451Z INFO ZooKeeper: Session: 0x30263a585b4f736 closed\n",
       "2026-02-03T23:41:15,451Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f736\n",
       "2026-02-03T23:41:15,455Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:15,748Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:16,891Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:16,891Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:16,892Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:16,892Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@79f992cc\n",
       "2026-02-03T23:41:16,892Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:16,892Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:16,893Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:16,893Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:16,893Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:41:16,894Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:16,894Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:38068, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:41:16,898Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a490701e7, negotiated timeout = 120000\n",
       "2026-02-03T23:41:16,899Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:16,900Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:16,900Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:16,900Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:16,900Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:17,005Z INFO ZooKeeper: Session: 0x100f75a490701e7 closed\n",
       "2026-02-03T23:41:17,005Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a490701e7\n",
       "2026-02-03T23:41:17,008Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:17,294Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:18 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.56.40:41534\n",
       "2026-02-03T23:41:18,438Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:18,439Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:18,440Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:18,440Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@61b00a10\n",
       "2026-02-03T23:41:18,440Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:18,441Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:18,443Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:18,443Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:18,443Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:18,443Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:18,444Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54788, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:18,448Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f75c, negotiated timeout = 120000\n",
       "2026-02-03T23:41:18,448Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:18,450Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:18,450Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:18,450Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:18,451Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:18,555Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f75c\n",
       "2026-02-03T23:41:18,555Z INFO ZooKeeper: Session: 0x30263a585b4f75c closed\n",
       "2026-02-03T23:41:18,559Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:18,903Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:18 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (100.67.56.40:41540) with ID 1,  ResourceProfileId 0\n",
       "26/02/03 23:41:18 INFO ExecutorMonitor: New executor 1 has registered (new total is 1)\n",
       "26/02/03 23:41:18 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
       "26/02/03 23:41:18 INFO AsyncEventQueue: Process of event SparkListenerEnvironmentUpdate(Map(Spark Properties -> ArrayBuffer((spark.acls.enable,false), (spark.app.id,spark-cdd9e5d0115345edb431647f51f82517), (spark.app.initial.jar.urls,spark://100.67.56.160:46445/jars/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar), (spark.app.name,kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc), (spark.app.startTime,1770162048311), (spark.app.submitTime,1770162047934), (spark.authenticate,true), (spark.databricks.delta.autoCompact.enabled,true), (spark.databricks.delta.autoCompact.minNumFiles,128), (spark.databricks.delta.deletedFileRetentionDuration,7 days), (spark.databricks.delta.logRetentionDuration,7 days), (spark.databricks.delta.optimize.maxThreads,128), (spark.databricks.delta.optimizeWrite.enabled,true), (spark.databricks.delta.vacuum.parallelDelete.enabled,true), (spark.databricks.delta.vacuum.parallelDelete.parallelism,1024), (spark.driver.blockManager.port,7079), (spark.driver.cores,2), (spark.driver.extraJavaOptions,-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.net.preferIPv4Stack=true -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:-UseContainerSupport -XX:ParallelGCThreads=4 -XX:ConcGCThreads=2), (spark.driver.host,100.67.56.160), (spark.driver.memory,16g), (spark.driver.port,46445), (spark.dynamicAllocation.cachedExecutorIdleTimeout,900s), (spark.dynamicAllocation.enabled,true), (spark.dynamicAllocation.executorIdleTimeout,900s), (spark.dynamicAllocation.initialExecutors,1), (spark.dynamicAllocation.maxExecutors,4), (spark.dynamicAllocation.minExecutors,0), (spark.dynamicAllocation.schedulerBacklogTimeout,10s), (spark.dynamicAllocation.shuffleTracking.enabled,true), (spark.dynamicAllocation.shuffleTracking.timeout,900s), (spark.dynamicAllocation.sustainedSchedulerBacklogTimeout,30s), (spark.eventLog.dir,s3://spark-k8-eks-xpca/event_logs), (spark.eventLog.enabled,true), (spark.eventLog.rolling.enabled,true), (spark.eventLog.rolling.maxFileSize,32m), (spark.executor.cores,4), (spark.executor.diskSize,200G), (spark.executor.extraJavaOptions,-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.net.preferIPv4Stack=true -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:-UseContainerSupport -XX:ParallelGCThreads=4 -XX:ConcGCThreads=2), (spark.executor.id,driver), (spark.executor.memory,16g), (spark.executor.memoryOverhead,8g), (spark.executor.processTreeMetrics.enabled,true), (spark.executor.resource.gpu.amount,1), (spark.executor.resource.gpu.discoveryScript,/opt/spark/examples/src/main/scripts/getGpusResources.sh), (spark.executor.resource.gpu.vendor,nvidia.com), (spark.executorEnv.SPARK_USER_NAME,*********(redacted)), (spark.hadoop.delta.enableFastS3AListFrom,true), (spark.hadoop.fs.s3.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (spark.hadoop.fs.s3a.access.key,*********(redacted)), (spark.hadoop.fs.s3a.attempts.maximum,10), (spark.hadoop.fs.s3a.aws.credentials.provider,*********(redacted)), (spark.hadoop.fs.s3a.block.size,67108864), (spark.hadoop.fs.s3a.committer.name,magic), (spark.hadoop.fs.s3a.connection.maximum,256), (spark.hadoop.fs.s3a.connection.timeout,50000), (spark.hadoop.fs.s3a.directory.marker.retention,keep), (spark.hadoop.fs.s3a.experimental.input.fadvise,random), (spark.hadoop.fs.s3a.fast.upload,true), (spark.hadoop.fs.s3a.fast.upload.active.blocks,32), (spark.hadoop.fs.s3a.fast.upload.default,true), (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (spark.hadoop.fs.s3a.impl.disable.cache,false), (spark.hadoop.fs.s3a.max.total.tasks,1000), (spark.hadoop.fs.s3a.multipart.size,10485760), (spark.hadoop.fs.s3a.multipart.threshold,104857600), (spark.hadoop.fs.s3a.path.style.access,true), (spark.hadoop.fs.s3a.performance.flag,create,mkdir), (spark.hadoop.fs.s3a.retry.interval,250ms), (spark.hadoop.fs.s3a.retry.limit,6), (spark.hadoop.fs.s3a.retry.throttle.interval,500ms), (spark.hadoop.fs.s3a.secret.key,*********(redacted)), (spark.hadoop.fs.s3a.threads.max,512), (spark.hadoop.hadoop.security.group.mapping,com.nvidia.sparkaas.hadoop.plugin.StaticConfigGroupProvider), (spark.hadoop.mapreduce.input.fileinputformat.list-status.num-threads,20), (spark.hive.metastore.uris,thrift://nvspark-metastore.nvspark.svc.cluster.local:9083), (spark.io.delta.storage.S3ZookeeperLogStore.fs.read.consistency,true), (spark.io.delta.storage.S3ZookeeperLogStore.fs.read.retries,3), (spark.io.delta.storage.S3ZookeeperLogStore.ttl.seconds,3600), (spark.io.delta.storage.S3ZookeeperLogStore.zk.auth.scheme,digest), (spark.io.delta.storage.S3ZookeeperLogStore.zk.auth.token,*********(redacted)), (spark.io.delta.storage.S3ZookeeperLogStore.zk.basePath,/delta_log), (spark.io.delta.storage.S3ZookeeperLogStore.zk.connectString,nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181), (spark.jars,file:/opt/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar), (spark.kubernetes.authenticate.driver.serviceAccountName,default-editor), (spark.kubernetes.container.image,900732750576.dkr.ecr.us-west-1.amazonaws.com/kratosxp-platform:sparkaas-spark-kyuubi-3.5.3-1.8.0.6.1-SNAPSHOT-25.08.8), (spark.kubernetes.container.image.pullPolicy,Always), (spark.kubernetes.driver.annotation.sidecar.istio.io/inject,false), (spark.kubernetes.driver.label.app-name,nvspark), (spark.kubernetes.driver.label.cluster-type,interactivecluster), (spark.kubernetes.driver.label.kratos.nvidia.com/kratosxp-userid,), (spark.kubernetes.driver.label.kyuubi-unique-tag,cfc8b837-b0f7-40ee-b388-391d8316c4dc), (spark.kubernetes.driver.label.nvsparkaas-cluster-id,cluster-20260203202803-yawkv5ak), (spark.kubernetes.driver.label.owner,hongy), (spark.kubernetes.driver.limit.cores,2), (spark.kubernetes.driver.node.selector.role,nvspark-driver), (spark.kubernetes.driver.node.selector.topology.kubernetes.io/zone,us-west-1a), (spark.kubernetes.driver.ownPersistentVolumeClaim,false), (spark.kubernetes.driver.pod.name,cluster-20260203202803-yawkv5ak-driver), (spark.kubernetes.driver.podTemplateFile,s3://spark-k8-eks-xpva/templates/driver-pod-template.yaml), (spark.kubernetes.driver.request.cores,1), (spark.kubernetes.driver.reusePersistentVolumeClaim,false), (spark.kubernetes.driver.service.label.nvsparkaas-cluster-id,cluster-20260203202803-yawkv5ak), (spark.kubernetes.driverEnv.SPARK_USER_NAME,*********(redacted)), (spark.kubernetes.executor.annotation.sidecar.istio.io/inject,false), (spark.kubernetes.executor.label.app-name,nvspark), (spark.kubernetes.executor.label.cluster-type,interactivecluster), (spark.kubernetes.executor.label.kratos.nvidia.com/kratosxp-userid,), (spark.kubernetes.executor.label.nvsparkaas-cluster-id,cluster-20260203202803-yawkv5ak), (spark.kubernetes.executor.label.owner,hongy), (spark.kubernetes.executor.limit.cores,4), (spark.kubernetes.executor.node.selector.role,nvspark-executor), (spark.kubernetes.executor.node.selector.topology.kubernetes.io/zone,us-west-1a), (spark.kubernetes.executor.pod.featureSteps,org.apache.spark.rapids.k8s.SparkExecutorFsGroupFeatureStep,org.apache.spark.rapids.k8s.SparkHostAliasesFeatureStep), (spark.kubernetes.executor.podNamePrefix,cluster-20260203202803-yawkv5ak), (spark.kubernetes.executor.podTemplateFile,/opt/spark/pod-template/pod-spec-template.yml), (spark.kubernetes.executor.request.cores,1), (spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-fast-volume.mount.path,/local_disk0/spark-local-dir), (spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-fast-volume.mount.readOnly,false), (spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-fast-volume.options.sizeLimit,200G), (spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path,/data), (spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly,false), (spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName,OnDemand), (spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit,200Gi), (spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass,gp3), (spark.kubernetes.file.upload.path,s3://spark-k8-eks-xpca/spark_upload), (spark.kubernetes.memoryOverheadFactor,0.1), (spark.kubernetes.namespace,dcartm-team), (spark.kubernetes.resource.type,java), (spark.kubernetes.submission.waitAppCompletion,false), (spark.kubernetes.submitInDriver,true), (spark.kyuubi.authentication,NONE), (spark.kyuubi.backend.server.exec.pool.size,600), (spark.kyuubi.backend.server.exec.pool.wait.queue.size,1000), (spark.kyuubi.batch.session.idle.timeout,PT30S), (spark.kyuubi.client.ipAddress,100.67.56.160), (spark.kyuubi.ctl.rest.connect.timeout,PT300S), (spark.kyuubi.engine.connection.url.use.hostname,false), (spark.kyuubi.engine.credentials,), (spark.kyuubi.engine.group.name,default), (spark.kyuubi.engine.security.enabled,false), (spark.kyuubi.engine.security.secret.provider,*********(redacted)), (spark.kyuubi.engine.share.level,GROUP), (spark.kyuubi.engine.share.level.subdomain,cluster-20260203202803-yawkv5ak), (spark.kyuubi.engine.submit.time,1770162044504), (spark.kyuubi.engine.type,SPARK_SQL), (spark.kyuubi.engine.ui.stop.enabled,false), (spark.kyuubi.frontend.bind.port,1009), (spark.kyuubi.frontend.connection.url.use.hostname,false), (spark.kyuubi.frontend.protocols,THRIFT_HTTP,REST), (spark.kyuubi.frontend.rest.bind.port,10099), (spark.kyuubi.frontend.thrift.http.bind.port,10009), (spark.kyuubi.frontend.thrift.login.timeout,PT300S), (spark.kyuubi.ha.addresses,nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181), (spark.kyuubi.ha.enabled,false), (spark.kyuubi.ha.engine.ref.id,cfc8b837-b0f7-40ee-b388-391d8316c4dc), (spark.kyuubi.ha.namespace,/kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak), (spark.kyuubi.ha.zookeeper.session.timeout,120000), (spark.kyuubi.metrics.enabled,false), (spark.kyuubi.metrics.reporters,PROMETHEUS), (spark.kyuubi.operation.idle.timeout,PT1M), (spark.kyuubi.operation.language,PYTHON), (spark.kyuubi.server.administrators,admin,anonymous), (spark.kyuubi.server.credentials.provider,com.nvidia.sparkaas.credentials.JsonFileCredentialsProvider), (spark.kyuubi.server.credentials.provider.params,*********(redacted)), (spark.kyuubi.server.ipAddress,100.67.56.160), (spark.kyuubi.session.check.interval,PT30S), (spark.kyuubi.session.conf.ignore.list,spark.sql.optimizer.excludedRules), (spark.kyuubi.session.conf.restrict.list,spark.sql.optimizer.excludedRules,spark.yarn.security.credentials.hbase.enabled,spark.security.credentials.hbase.enabled,spark.driver.userClassPathFirst), (spark.kyuubi.session.connection.url,100.67.56.160:10099), (spark.kyuubi.session.engine.alive.probe.enabled,true), (spark.kyuubi.session.engine.alive.probe.interval,PT10S), (spark.kyuubi.session.engine.alive.timeout,PT2M), (spark.kyuubi.session.engine.check.interval,PT1M), (spark.kyuubi.session.engine.idle.timeout,PT30M), (spark.kyuubi.session.engine.initialize.timeout,PT60M), (spark.kyuubi.session.engine.spark.main.resource,/opt/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar), (spark.kyuubi.session.group.provider,com.nvidia.sparkaas.kyuubi.plugin.StaticConfigGroupProvider), (spark.kyuubi.session.idle.timeout,PT60M), (spark.kyuubi.session.name,hongy-default-20260203202802-startup), (spark.kyuubi.session.real.user,anonymous), (spark.kyuubi.sparkaas.groupprovider.static.group,default), (spark.locality.wait,0), (spark.master,k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com), (spark.plugins,com.nvidia.spark.SQLPlugin,org.apache.spark.rapids.k8s.WheelInstallPlugin), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.driver.user.timezone,Z), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.memory.pinnedPool.size,2G), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.shuffle.mode,MULTITHREADED), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.adaptive.skewJoin.broadcast.enabled,true), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.batchSizeBytes,2147483646), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.coalescing.reader.numFilterParallel,2), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.concurrentGpuTasks,2), (spark.plugins.internal.conf.com.nvidia.spark.SQLPlugin.spark.rapids.sql.multiThreadedRead.numThreads,40), (spark.rapids.driver.user.timezone,Z), (spark.rapids.memory.pinnedPool.size,2G), (spark.rapids.shuffle.mode,MULTITHREADED), (spark.rapids.sql.adaptive.skewJoin.broadcast.enabled,true), (spark.rapids.sql.batchSizeBytes,2147483646), (spark.rapids.sql.coalescing.reader.numFilterParallel,2), (spark.rapids.sql.concurrentGpuTasks,2), (spark.rapids.sql.multiThreadedRead.numThreads,40), (spark.redaction.regex,*********(redacted)), (spark.repl.class.outputDir,/var/data/spark-dbc00d20-334d-441c-a507-991867431d91/spark-1bd6c670-49e7-4520-8553-bae19179e324/repl-d454d64b-fe4a-4fff-9a11-435be6112e76), (spark.repl.class.uri,spark://100.67.56.160:46445/classes), (spark.repl.local.jars,), (spark.scheduler.mode,FAIR), (spark.sessionType,INTERACTIVE), (spark.shuffle.manager,com.nvidia.spark.rapids.spark353.RapidsShuffleManager), (spark.sparkaas.groupprovider.static.group,default), (spark.sparkaas.prometheus.servicemonitor.enable,true), (spark.sparkaas.sparkui.service.type,ClusterIP), (spark.sparkaas.transaction.id,), (spark.sql.cache.serializer,com.nvidia.spark.ParquetCachedBatchSerializer), (spark.sql.catalog.spark_catalog,org.apache.spark.sql.delta.catalog.DeltaCatalog), (spark.sql.catalogImplementation,hive), (spark.sql.execution.arrow.pyspark.enabled,true), (spark.sql.execution.topKSortFallbackThreshold,10000), (spark.sql.extensions,io.delta.sql.DeltaSparkSessionExtension,com.nvidia.spark.rapids.SQLExecPlugin,com.nvidia.spark.udf.Plugin,com.nvidia.spark.DFUDFPlugin,com.nvidia.spark.rapids.optimizer.SQLOptimizerPlugin), (spark.sql.files.maxPartitionBytes,2048m), (spark.sql.legacy.castComplexTypesToString.enabled,true), (spark.sql.parquet.datetimeRebaseModeInRead,CORRECTED), (spark.sql.parquet.datetimeRebaseModeInWrite,EXCEPTION), (spark.sql.parquet.int96RebaseModeInRead,CORRECTED), (spark.sql.parquet.int96RebaseModeInWrite,EXCEPTION), (spark.sql.parquet.output.committer.class,org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter), (spark.sql.sources.commitProtocolClass,org.apache.spark.internal.io.cloud.PathOutputCommitProtocol), (spark.sql.streaming.metricsEnabled,true), (spark.sql.streaming.streamingQueryListeners,org.apache.spark.sql.streaming.kratos.AppAnalyticsListener,org.apache.spark.sql.kratos.KratosQueryListener), (spark.sql.warehouse.dir,s3a://kratos-spark-hive/datalake), (spark.submit.deployMode,client), (spark.submit.pyFiles,), (spark.task.cpus,1), (spark.task.resource.gpu.amount,0.001), (spark.ui.prometheus.enabled,true), (spark.ui.proxyBase,/nvspark/ui/dcartm-team/hongy), (spark.ui.proxyRedirectUri,/), (spark.ui.view.acls.groups,default)), Classpath Entries -> Vector((/opt/spark/conf/,System Classpath), (/opt/spark/jars/HikariCP-2.5.1.jar,System Classpath), (/opt/spark/jars/JLargeArrays-1.5.jar,System Classpath), (/opt/spark/jars/JTransforms-3.1.jar,System Classpath), (/opt/spark/jars/RoaringBitmap-0.9.45.jar,System Classpath), (/opt/spark/jars/ST4-4.0.4.jar,System Classpath), (/opt/spark/jars/activation-1.1.1.jar,System Classpath), (/opt/spark/jars/aircompressor-0.27.jar,System Classpath), (/opt/spark/jars/algebra_2.12-2.0.1.jar,System Classpath), (/opt/spark/jars/annotations-17.0.0.jar,System Classpath), (/opt/spark/jars/antlr-runtime-3.5.2.jar,System Classpath), (/opt/spark/jars/antlr4-runtime-4.9.3.jar,System Classpath), (/opt/spark/jars/aopalliance-repackaged-2.6.1.jar,System Classpath), (/opt/spark/jars/app-analytics-listener_3.5.3-0.1.4.jar,System Classpath), (/opt/spark/jars/arpack-3.0.3.jar,System Classpath), (/opt/spark/jars/arpack_combined_all-0.1.jar,System Classpath), (/opt/spark/jars/arrow-format-12.0.1.jar,System Classpath), (/opt/spark/jars/arrow-memory-core-12.0.1.jar,System Classpath), (/opt/spark/jars/arrow-memory-netty-12.0.1.jar,System Classpath), (/opt/spark/jars/arrow-vector-12.0.1.jar,System Classpath), (/opt/spark/jars/audience-annotations-0.5.0.jar,System Classpath), (/opt/spark/jars/avro-1.11.2.jar,System Classpath), (/opt/spark/jars/avro-ipc-1.11.2.jar,System Classpath), (/opt/spark/jars/avro-mapred-1.11.2.jar,System Classpath), (/opt/spark/jars/aws-java-sdk-bundle-1.12.780.jar,System Classpath), (/opt/spark/jars/blas-3.0.3.jar,System Classpath), (/opt/spark/jars/bonecp-0.8.0.RELEASE.jar,System Classpath), (/opt/spark/jars/breeze-macros_2.12-2.1.0.jar,System Classpath), (/opt/spark/jars/breeze_2.12-2.1.0.jar,System Classpath), (/opt/spark/jars/bundle-2.24.6.jar,System Classpath), (/opt/spark/jars/cats-kernel_2.12-2.1.1.jar,System Classpath), (/opt/spark/jars/chill-java-0.10.0.jar,System Classpath), (/opt/spark/jars/chill_2.12-0.10.0.jar,System Classpath), (/opt/spark/jars/commons-cli-1.5.0.jar,System Classpath), (/opt/spark/jars/commons-codec-1.16.1.jar,System Classpath), (/opt/spark/jars/commons-collections-3.2.2.jar,System Classpath), (/opt/spark/jars/commons-collections4-4.4.jar,System Classpath), (/opt/spark/jars/commons-compiler-3.1.9.jar,System Classpath), (/opt/spark/jars/commons-compress-1.23.0.jar,System Classpath), (/opt/spark/jars/commons-crypto-1.1.0.jar,System Classpath), (/opt/spark/jars/commons-dbcp-1.4.jar,System Classpath), (/opt/spark/jars/commons-io-2.16.1.jar,System Classpath), (/opt/spark/jars/commons-lang-2.6.jar,System Classpath), (/opt/spark/jars/commons-lang3-3.12.0.jar,System Classpath), (/opt/spark/jars/commons-logging-1.1.3.jar,System Classpath), (/opt/spark/jars/commons-math3-3.6.1.jar,System Classpath), (/opt/spark/jars/commons-pool-1.5.4.jar,System Classpath), (/opt/spark/jars/commons-pool2-2.11.1.jar,System Classpath), (/opt/spark/jars/commons-text-1.10.0.jar,System Classpath), (/opt/spark/jars/compress-lzf-1.1.2.jar,System Classpath), (/opt/spark/jars/curator-client-5.7.1.jar,System Classpath), (/opt/spark/jars/curator-framework-5.7.1.jar,System Classpath), (/opt/spark/jars/curator-recipes-5.7.1.jar,System Classpath), (/opt/spark/jars/datanucleus-api-jdo-4.2.4.jar,System Classpath), (/opt/spark/jars/datanucleus-core-4.1.17.jar,System Classpath), (/opt/spark/jars/datanucleus-rdbms-4.1.19.jar,System Classpath), (/opt/spark/jars/datasketches-java-3.3.0.jar,System Classpath), (/opt/spark/jars/datasketches-memory-2.1.0.jar,System Classpath), (/opt/spark/jars/delta-spark_2.12-3.3.2-25-12-10.jar,System Classpath), (/opt/spark/jars/delta-storage-3.3.2-25-12-10.jar,System Classpath), (/opt/spark/jars/delta-storage-s3-dynamodb-3.3.2-25-12-10.jar,System Classpath), (/opt/spark/jars/derby-10.14.2.0.jar,System Classpath), (/opt/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar,System Classpath), (/opt/spark/jars/dummy-shs-filter-25.08.0.jar,System Classpath), (/opt/spark/jars/flatbuffers-java-1.12.0.jar,System Classpath), (/opt/spark/jars/gethostname4j-1.0.0.jar,System Classpath), (/opt/spark/jars/gson-2.2.4.jar,System Classpath), (/opt/spark/jars/guava-14.0.1.jar,System Classpath), (/opt/spark/jars/hadoop-aws-3.4.1.jar,System Classpath), (/opt/spark/jars/hadoop-client-api-3.4.1.jar,System Classpath), (/opt/spark/jars/hadoop-client-runtime-3.4.1.jar,System Classpath), (/opt/spark/jars/hadoop-shaded-guava-1.3.0.jar,System Classpath), (/opt/spark/jars/hive-beeline-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-cli-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-common-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-exec-2.3.9-core.jar,System Classpath), (/opt/spark/jars/hive-jdbc-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-llap-common-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-metastore-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-serde-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-service-rpc-3.1.3.jar,System Classpath), (/opt/spark/jars/hive-shims-0.23-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-shims-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-shims-common-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-shims-scheduler-2.3.9.jar,System Classpath), (/opt/spark/jars/hive-storage-api-2.8.1.jar,System Classpath), (/opt/spark/jars/hk2-api-2.6.1.jar,System Classpath), (/opt/spark/jars/hk2-locator-2.6.1.jar,System Classpath), (/opt/spark/jars/hk2-utils-2.6.1.jar,System Classpath), (/opt/spark/jars/httpclient-4.5.14.jar,System Classpath), (/opt/spark/jars/httpcore-4.4.16.jar,System Classpath), (/opt/spark/jars/istack-commons-runtime-3.0.8.jar,System Classpath), (/opt/spark/jars/ivy-2.5.1.jar,System Classpath), (/opt/spark/jars/jackson-annotations-2.15.2.jar,System Classpath), (/opt/spark/jars/jackson-core-2.15.2.jar,System Classpath), (/opt/spark/jars/jackson-core-asl-1.9.13.jar,System Classpath), (/opt/spark/jars/jackson-databind-2.15.2.jar,System Classpath), (/opt/spark/jars/jackson-dataformat-yaml-2.15.2.jar,System Classpath), (/opt/spark/jars/jackson-datatype-jsr310-2.15.2.jar,System Classpath), (/opt/spark/jars/jackson-jaxrs-1.9.9.jar,System Classpath), (/opt/spark/jars/jackson-mapper-asl-1.9.13.jar,System Classpath), (/opt/spark/jars/jackson-module-scala_2.12-2.15.2.jar,System Classpath), (/opt/spark/jars/jakarta.annotation-api-1.3.5.jar,System Classpath), (/opt/spark/jars/jakarta.inject-2.6.1.jar,System Classpath), (/opt/spark/jars/jakarta.servlet-api-4.0.3.jar,System Classpath), (/opt/spark/jars/jakarta.validation-api-2.0.2.jar,System Classpath), (/opt/spark/jars/jakarta.ws.rs-api-2.1.6.jar,System Classpath), (/opt/spark/jars/jakarta.xml.bind-api-2.3.2.jar,System Classpath), (/opt/spark/jars/janino-3.1.9.jar,System Classpath), (/opt/spark/jars/javassist-3.29.2-GA.jar,System Classpath), (/opt/spark/jars/javax.jdo-3.2.0-m3.jar,System Classpath), (/opt/spark/jars/javolution-5.5.1.jar,System Classpath), (/opt/spark/jars/jaxb-runtime-2.3.2.jar,System Classpath), (/opt/spark/jars/jcl-over-slf4j-2.0.7.jar,System Classpath), (/opt/spark/jars/jdo-api-3.0.1.jar,System Classpath), (/opt/spark/jars/jersey-client-1.19.4.jar,System Classpath), (/opt/spark/jars/jersey-client-2.40.jar,System Classpath), (/opt/spark/jars/jersey-common-2.40.jar,System Classpath), (/opt/spark/jars/jersey-container-servlet-2.40.jar,System Classpath), (/opt/spark/jars/jersey-container-servlet-core-2.40.jar,System Classpath), (/opt/spark/jars/jersey-core-1.9.1.jar,System Classpath), (/opt/spark/jars/jersey-hk2-2.40.jar,System Classpath), (/opt/spark/jars/jersey-server-2.40.jar,System Classpath), (/opt/spark/jars/jline-2.14.6.jar,System Classpath), (/opt/spark/jars/jna-5.13.0.jar,System Classpath), (/opt/spark/jars/jna-platform-5.13.0.jar,System Classpath), (/opt/spark/jars/joda-time-2.12.5.jar,System Classpath), (/opt/spark/jars/jodd-core-3.5.2.jar,System Classpath), (/opt/spark/jars/jpam-1.1.jar,System Classpath), (/opt/spark/jars/json-1.8.jar,System Classpath), (/opt/spark/jars/json-file-credentials-provider-25.08.0.jar,System Classpath), (/opt/spark/jars/json4s-ast_2.12-3.7.0-M11.jar,System Classpath), (/opt/spark/jars/json4s-core_2.12-3.7.0-M11.jar,System Classpath), (/opt/spark/jars/json4s-jackson_2.12-3.7.0-M11.jar,System Classpath), (/opt/spark/jars/json4s-scalap_2.12-3.7.0-M11.jar,System Classpath), (/opt/spark/jars/jsr305-3.0.0.jar,System Classpath), (/opt/spark/jars/jta-1.1.jar,System Classpath), (/opt/spark/jars/jul-to-slf4j-2.0.7.jar,System Classpath), (/opt/spark/jars/kafka-clients-3.4.1.jar,System Classpath), (/opt/spark/jars/kratos-spark-sql-functions_2.12-0.0.4-3.5.3-java17-SNAPSHOT.jar,System Classpath), (/opt/spark/jars/kryo-shaded-4.0.2.jar,System Classpath), (/opt/spark/jars/kubernetes-client-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-client-api-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-httpclient-okhttp-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-admissionregistration-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-apiextensions-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-apps-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-autoscaling-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-batch-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-certificates-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-common-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-coordination-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-core-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-discovery-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-events-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-extensions-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-flowcontrol-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-gatewayapi-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-metrics-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-networking-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-node-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-policy-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-rbac-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-resource-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-scheduling-6.7.2.jar,System Classpath), (/opt/spark/jars/kubernetes-model-storageclass-6.7.2.jar,System Classpath), (/opt/spark/jars/kyuubi-spark-authz_2.12-1.7.0.jar,System Classpath), (/opt/spark/jars/lapack-3.0.3.jar,System Classpath), (/opt/spark/jars/leveldbjni-all-1.8.jar,System Classpath), (/opt/spark/jars/libfb303-0.9.3.jar,System Classpath), (/opt/spark/jars/libthrift-0.12.0.jar,System Classpath), (/opt/spark/jars/log4j-1.2-api-2.20.0.jar,System Classpath), (/opt/spark/jars/log4j-api-2.20.0.jar,System Classpath), (/opt/spark/jars/log4j-core-2.20.0.jar,System Classpath), (/opt/spark/jars/log4j-slf4j2-impl-2.20.0.jar,System Classpath), (/opt/spark/jars/logging-interceptor-3.12.12.jar,System Classpath), (/opt/spark/jars/lz4-java-1.8.0.jar,System Classpath), (/opt/spark/jars/mesos-1.4.3-shaded-protobuf.jar,System Classpath), (/opt/spark/jars/metrics-core-4.2.19.jar,System Classpath), (/opt/spark/jars/metrics-graphite-4.2.19.jar,System Classpath), (/opt/spark/jars/metrics-jmx-4.2.19.jar,System Classpath), (/opt/spark/jars/metrics-json-4.2.19.jar,System Classpath), (/opt/spark/jars/metrics-jvm-4.2.19.jar,System Classpath), (/opt/spark/jars/minlog-1.3.0.jar,System Classpath), (/opt/spark/jars/mysql-connector-j-8.0.33.jar,System Classpath), (/opt/spark/jars/netty-all-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-buffer-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-codec-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-codec-http-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-codec-http2-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-codec-socks-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-common-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-handler-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-handler-proxy-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-resolver-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-transport-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-transport-classes-epoll-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-transport-classes-kqueue-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar,System Classpath), (/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar,System Classpath), (/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar,System Classpath), (/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar,System Classpath), (/opt/spark/jars/netty-transport-native-unix-common-4.1.96.Final.jar,System Classpath), (/opt/spark/jars/nvsparkaas-k8s-plugin-3.5.3-25.08.5.jar,System Classpath), (/opt/spark/jars/objenesis-3.3.jar,System Classpath), (/opt/spark/jars/okhttp-3.12.12.jar,System Classpath), (/opt/spark/jars/okio-1.17.6.jar,System Classpath), (/opt/spark/jars/opencsv-2.3.jar,System Classpath), (/opt/spark/jars/orc-core-1.9.4-shaded-protobuf.jar,System Classpath), (/opt/spark/jars/orc-mapreduce-1.9.4-shaded-protobuf.jar,System Classpath), (/opt/spark/jars/orc-shims-1.9.4.jar,System Classpath), (/opt/spark/jars/oro-2.0.8.jar,System Classpath), (/opt/spark/jars/osgi-resource-locator-1.0.3.jar,System Classpath), (/opt/spark/jars/paranamer-2.8.jar,System Classpath), (/opt/spark/jars/parquet-column-1.13.1.jar,System Classpath), (/opt/spark/jars/parquet-common-1.13.1.jar,System Classpath), (/opt/spark/jars/parquet-encoding-1.13.1.jar,System Classpath), (/opt/spark/jars/parquet-format-structures-1.13.1.jar,System Classpath), (/opt/spark/jars/parquet-hadoop-1.13.1.jar,System Classpath), (/opt/spark/jars/parquet-jackson-1.13.1.jar,System Classpath), (/opt/spark/jars/pickle-1.3.jar,System Classpath), (/opt/spark/jars/postgresql-42.6.0.jar,System Classpath), (/opt/spark/jars/py4j-0.10.9.7.jar,System Classpath), (/opt/spark/jars/ranger-plugins-audit-2.3.0.jar,System Classpath), (/opt/spark/jars/ranger-plugins-common-2.3.0.jar,System Classpath), (/opt/spark/jars/rapids-4-spark_2.12-25.10.1-cuda12.jar,System Classpath), (/opt/spark/jars/rocksdbjni-8.3.2.jar,System Classpath), (/opt/spark/jars/scala-collection-compat_2.12-2.7.0.jar,System Classpath), (/opt/spark/jars/scala-compiler-2.12.18.jar,System Classpath), (/opt/spark/jars/scala-library-2.12.18.jar,System Classpath), (/opt/spark/jars/scala-parser-combinators_2.12-2.3.0.jar,System Classpath), (/opt/spark/jars/scala-reflect-2.12.18.jar,System Classpath), (/opt/spark/jars/scala-xml_2.12-2.1.0.jar,System Classpath), (/opt/spark/jars/shims-0.9.45.jar,System Classpath), (/opt/spark/jars/shs-filter-25.08.0.jar,System Classpath), (/opt/spark/jars/slf4j-api-1.7.36.jar,System Classpath), (/opt/spark/jars/slf4j-api-2.0.7.jar,System Classpath), (/opt/spark/jars/slf4j-log4j12-1.7.16.jar,System Classpath), (/opt/spark/jars/slf4j-reload4j-1.7.36.jar,System Classpath), (/opt/spark/jars/snakeyaml-2.0.jar,System Classpath), (/opt/spark/jars/snakeyaml-engine-2.6.jar,System Classpath), (/opt/spark/jars/snappy-java-1.1.10.5.jar,System Classpath), (/opt/spark/jars/spark-avro_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-catalyst_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-common-utils_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-core_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-graphx_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-hadoop-cloud_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-hive-thriftserver_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-hive_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-kubernetes_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-kvstore_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-launcher_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-mesos_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-mllib-local_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-mllib_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-network-common_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-network-shuffle_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-repl_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-sketch_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-sql-api_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-sql_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-streaming_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-tags_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-unsafe_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spark-yarn_2.12-3.5.3.jar,System Classpath), (/opt/spark/jars/spire-macros_2.12-0.17.0.jar,System Classpath), (/opt/spark/jars/spire-platform_2.12-0.17.0.jar,System Classpath), (/opt/spark/jars/spire-util_2.12-0.17.0.jar,System Classpath), (/opt/spark/jars/spire_2.12-0.17.0.jar,System Classpath), (/opt/spark/jars/static-config-group-provider-25.08.0-jar-with-dependencies.jar,System Classpath), (/opt/spark/jars/static-config-group-provider-25.08.0.jar,System Classpath), (/opt/spark/jars/stax-api-1.0.1.jar,System Classpath), (/opt/spark/jars/stream-2.9.6.jar,System Classpath), (/opt/spark/jars/super-csv-2.2.0.jar,System Classpath), (/opt/spark/jars/test-credentials-provider-25.08.0-jar-with-dependencies.jar,System Classpath), (/opt/spark/jars/threeten-extra-1.7.1.jar,System Classpath), (/opt/spark/jars/tink-1.9.0.jar,System Classpath), (/opt/spark/jars/transaction-api-1.1.jar,System Classpath), (/opt/spark/jars/univocity-parsers-2.9.1.jar,System Classpath), (/opt/spark/jars/xbean-asm9-shaded-4.23.jar,System Classpath), (/opt/spark/jars/xz-1.9.jar,System Classpath), (/opt/spark/jars/zjsonpatch-0.3.0.jar,System Classpath), (/opt/spark/jars/zookeeper-3.6.3.jar,System Classpath), (/opt/spark/jars/zookeeper-jute-3.6.3.jar,System Classpath), (/opt/spark/jars/zstd-jni-1.5.5-4.jar,System Classpath), (spark://100.67.56.160:46445/jars/kyuubi-spark-sql-engine_2.12-1.8.0.6.1-SNAPSHOT.jar,Added By User)), Hadoop Properties -> List((adl.feature.ownerandgroup.enableupn,false), (adl.http.timeout,-1), (delta.enableFastS3AListFrom,true), (dfs.client.ignore.namenode.default.kms.uri,false), (dfs.ha.fencing.ssh.connect-timeout,30000), (file.blocksize,67108864), (file.bytes-per-checksum,512), (file.client-write-packet-size,65536), (file.replication,1), (file.stream-buffer-size,4096), (fs.AbstractFileSystem.abfs.impl,org.apache.hadoop.fs.azurebfs.Abfs), (fs.AbstractFileSystem.abfss.impl,org.apache.hadoop.fs.azurebfs.Abfss), (fs.AbstractFileSystem.adl.impl,org.apache.hadoop.fs.adl.Adl), (fs.AbstractFileSystem.file.impl,org.apache.hadoop.fs.local.LocalFs), (fs.AbstractFileSystem.ftp.impl,org.apache.hadoop.fs.ftp.FtpFs), (fs.AbstractFileSystem.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS), (fs.AbstractFileSystem.har.impl,org.apache.hadoop.fs.HarFs), (fs.AbstractFileSystem.hdfs.impl,org.apache.hadoop.fs.Hdfs), (fs.AbstractFileSystem.o3fs.impl,org.apache.hadoop.fs.ozone.OzFs), (fs.AbstractFileSystem.ofs.impl,org.apache.hadoop.fs.ozone.RootedOzFs), (fs.AbstractFileSystem.s3a.impl,org.apache.hadoop.fs.s3a.S3A), (fs.AbstractFileSystem.swebhdfs.impl,org.apache.hadoop.fs.SWebHdfs), (fs.AbstractFileSystem.viewfs.impl,org.apache.hadoop.fs.viewfs.ViewFs), (fs.AbstractFileSystem.wasb.impl,org.apache.hadoop.fs.azure.Wasb), (fs.AbstractFileSystem.wasbs.impl,org.apache.hadoop.fs.azure.Wasbs), (fs.AbstractFileSystem.webhdfs.impl,org.apache.hadoop.fs.WebHdfs), (fs.abfs.impl,org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem), (fs.abfss.impl,org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem), (fs.adl.impl,org.apache.hadoop.fs.adl.AdlFileSystem), (fs.adl.oauth2.access.token.provider.type,*********(redacted)), (fs.automatic.close,true), (fs.azure.authorization,false), (fs.azure.authorization.caching.enable,true), (fs.azure.buffer.dir,\\${env.LOCAL_DIRS:-\\${hadoop.tmp.dir}}/abfs), (fs.azure.enable.readahead,true), (fs.azure.local.sas.key.mode,false), (fs.azure.sas.expiry.period,90d), (fs.azure.saskey.usecontainersaskeyforallaccess,true), (fs.azure.secure.mode,false), (fs.azure.user.agent.prefix,unknown), (fs.client.resolve.remote.symlinks,true), (fs.client.resolve.topology.enabled,false), (fs.creation.parallel.count,64), (fs.defaultFS,file:///), (fs.df.interval,60000), (fs.du.interval,600000), (fs.ftp.data.connection.mode,ACTIVE_LOCAL_DATA_CONNECTION_MODE), (fs.ftp.host,0.0.0.0), (fs.ftp.host.port,21), (fs.ftp.impl,org.apache.hadoop.fs.ftp.FTPFileSystem), (fs.ftp.timeout,0), (fs.ftp.transfer.mode,BLOCK_TRANSFER_MODE), (fs.getspaceused.jitterMillis,60000), (fs.har.impl.disable.cache,true), (fs.iostatistics.logging.level,debug), (fs.iostatistics.thread.level.enabled,true), (fs.permissions.umask-mode,022), (fs.s3.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (fs.s3a.access.key,*********(redacted)), (fs.s3a.accesspoint.required,false), (fs.s3a.assumed.role.credentials.provider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider), (fs.s3a.assumed.role.session.duration,30m), (fs.s3a.attempts.maximum,10), (fs.s3a.audit.enabled,true), (fs.s3a.aws.credentials.provider,*********(redacted)), (fs.s3a.block.size,67108864), (fs.s3a.buffer.dir,\\${env.LOCAL_DIRS:-\\${hadoop.tmp.dir}}/s3a), (fs.s3a.change.detection.mode,server), (fs.s3a.change.detection.source,etag), (fs.s3a.change.detection.version.required,true), (fs.s3a.committer.abort.pending.uploads,true), (fs.s3a.committer.magic.enabled,true), (fs.s3a.committer.name,magic), (fs.s3a.committer.staging.conflict-mode,append), (fs.s3a.committer.staging.tmp.path,tmp/staging), (fs.s3a.committer.staging.unique-filenames,true), (fs.s3a.committer.threads,8), (fs.s3a.connection.establish.timeout,30s), (fs.s3a.connection.maximum,256), (fs.s3a.connection.ssl.enabled,true), (fs.s3a.connection.timeout,50000), (fs.s3a.connection.ttl,5m), (fs.s3a.directory.marker.retention,keep), (fs.s3a.downgrade.syncable.exceptions,true), (fs.s3a.endpoint,s3.amazonaws.com), (fs.s3a.etag.checksum.enabled,false), (fs.s3a.executor.capacity,16), (fs.s3a.experimental.input.fadvise,random), (fs.s3a.fast.upload,true), (fs.s3a.fast.upload.active.blocks,32), (fs.s3a.fast.upload.buffer,disk), (fs.s3a.fast.upload.default,true), (fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (fs.s3a.impl.disable.cache,false), (fs.s3a.list.version,2), (fs.s3a.max.total.tasks,1000), (fs.s3a.multiobjectdelete.enable,true), (fs.s3a.multipart.purge,false), (fs.s3a.multipart.purge.age,24h), (fs.s3a.multipart.size,10485760), (fs.s3a.multipart.threshold,104857600), (fs.s3a.paging.maximum,5000), (fs.s3a.path.style.access,true), (fs.s3a.performance.flag,create,mkdir), (fs.s3a.readahead.range,64K), (fs.s3a.retry.interval,250ms), (fs.s3a.retry.limit,6), (fs.s3a.retry.throttle.interval,500ms), (fs.s3a.retry.throttle.limit,20), (fs.s3a.secret.key,*********(redacted)), (fs.s3a.select.enabled,true), (fs.s3a.select.errors.include.sql,false), (fs.s3a.select.input.compression,none), (fs.s3a.select.input.csv.comment.marker,#), (fs.s3a.select.input.csv.field.delimiter,,), (fs.s3a.select.input.csv.header,none), (fs.s3a.select.input.csv.quote.character,\"), (fs.s3a.select.input.csv.quote.escape.character,\\\\\\\\), (fs.s3a.select.input.csv.record.delimiter,\\\\n), (fs.s3a.select.output.csv.field.delimiter,,), (fs.s3a.select.output.csv.quote.character,\"), (fs.s3a.select.output.csv.quote.escape.character,\\\\\\\\), (fs.s3a.select.output.csv.quote.fields,always), (fs.s3a.select.output.csv.record.delimiter,\\\\n), (fs.s3a.socket.recv.buffer,8192), (fs.s3a.socket.send.buffer,8192), (fs.s3a.ssl.channel.mode,default_jsse), (fs.s3a.threads.keepalivetime,60s), (fs.s3a.threads.max,512), (fs.trash.checkpoint.interval,0), (fs.trash.clean.trashroot.enable,false), (fs.trash.interval,0), (fs.viewfs.overload.scheme.target.abfs.impl,org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem), (fs.viewfs.overload.scheme.target.abfss.impl,org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem), (fs.viewfs.overload.scheme.target.file.impl,org.apache.hadoop.fs.LocalFileSystem), (fs.viewfs.overload.scheme.target.ftp.impl,org.apache.hadoop.fs.ftp.FTPFileSystem), (fs.viewfs.overload.scheme.target.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS), (fs.viewfs.overload.scheme.target.hdfs.impl,org.apache.hadoop.hdfs.DistributedFileSystem), (fs.viewfs.overload.scheme.target.http.impl,org.apache.hadoop.fs.http.HttpFileSystem), (fs.viewfs.overload.scheme.target.https.impl,org.apache.hadoop.fs.http.HttpsFileSystem), (fs.viewfs.overload.scheme.target.o3fs.impl,org.apache.hadoop.fs.ozone.OzoneFileSystem), (fs.viewfs.overload.scheme.target.ofs.impl,org.apache.hadoop.fs.ozone.RootedOzoneFileSystem), (fs.viewfs.overload.scheme.target.oss.impl,org.apache.hadoop.fs.aliyun.oss.AliyunOSSFileSystem), (fs.viewfs.overload.scheme.target.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem), (fs.viewfs.overload.scheme.target.swebhdfs.impl,org.apache.hadoop.hdfs.web.SWebHdfsFileSystem), (fs.viewfs.overload.scheme.target.wasb.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem), (fs.viewfs.overload.scheme.target.webhdfs.impl,org.apache.hadoop.hdfs.web.WebHdfsFileSystem), (fs.viewfs.rename.strategy,SAME_MOUNTPOINT), (fs.wasb.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem), (fs.wasbs.impl,org.apache.hadoop.fs.azure.NativeAzureFileSystem\\$Secure), (ftp.blocksize,67108864), (ftp.bytes-per-checksum,512), (ftp.client-write-packet-size,65536), (ftp.replication,3), (ftp.stream-buffer-size,4096), (ha.failover-controller.active-standby-elector.zk.op.retries,3), (ha.failover-controller.cli-check.rpc-timeout.ms,20000), (ha.failover-controller.graceful-fence.connection.retries,1), (ha.failover-controller.graceful-fence.rpc-timeout.ms,5000), (ha.failover-controller.new-active.rpc-timeout.ms,60000), (ha.health-monitor.check-interval.ms,1000), (ha.health-monitor.connect-retry-interval.ms,1000), (ha.health-monitor.rpc-timeout.ms,45000), (ha.health-monitor.rpc.connect.max.retries,1), (ha.health-monitor.sleep-after-disconnect.ms,1000), (ha.zookeeper.acl,world:anyone:rwcda), (ha.zookeeper.parent-znode,/hadoop-ha), (ha.zookeeper.session-timeout.ms,10000), (hadoop.caller.context.enabled,false), (hadoop.caller.context.max.size,128), (hadoop.caller.context.separator,,), (hadoop.caller.context.signature.max.size,40), (hadoop.common.configuration.version,3.0.0), (hadoop.domainname.resolver.impl,org.apache.hadoop.net.DNSDomainNameResolver), (hadoop.http.authentication.kerberos.keytab,\\${user.home}/hadoop.keytab), (hadoop.http.authentication.kerberos.principal,HTTP/_HOST@LOCALHOST), (hadoop.http.authentication.signature.secret.file,*********(redacted)), (hadoop.http.authentication.simple.anonymous.allowed,true), (hadoop.http.authentication.token.validity,*********(redacted)), (hadoop.http.authentication.type,simple), (hadoop.http.cross-origin.allowed-headers,X-Requested-With,Content-Type,Accept,Origin), (hadoop.http.cross-origin.allowed-methods,GET,POST,HEAD), (hadoop.http.cross-origin.allowed-origins,*), (hadoop.http.cross-origin.enabled,false), (hadoop.http.cross-origin.max-age,1800), (hadoop.http.filter.initializers,org.apache.hadoop.http.lib.StaticUserWebFilter), (hadoop.http.idle_timeout.ms,60000), (hadoop.http.jmx.nan-filter.enabled,false), (hadoop.http.logs.enabled,true), (hadoop.http.metrics.enabled,true), (hadoop.http.sni.host.check.enabled,false), (hadoop.http.staticuser.user,dr.who), (hadoop.jetty.logs.serve.aliases,true), (hadoop.kerberos.keytab.login.autorenewal.enabled,false), (hadoop.kerberos.kinit.command,kinit), (hadoop.kerberos.min.seconds.before.relogin,60), (hadoop.metrics.jvm.use-thread-mxbean,false), (hadoop.prometheus.endpoint.enabled,false), (hadoop.registry.jaas.context,Client), (hadoop.registry.secure,false), (hadoop.registry.system.acls,sasl:yarn@, sasl:mapred@, sasl:hdfs@), (hadoop.registry.zk.connection.timeout.ms,15000), (hadoop.registry.zk.quorum,localhost:2181), (hadoop.registry.zk.retry.ceiling.ms,60000), (hadoop.registry.zk.retry.interval.ms,1000), (hadoop.registry.zk.retry.times,5), (hadoop.registry.zk.root,/registry), (hadoop.registry.zk.session.timeout.ms,60000), (hadoop.rpc.protection,authentication), (hadoop.rpc.socket.factory.class.default,org.apache.hadoop.net.StandardSocketFactory), (hadoop.security.auth_to_local.mechanism,hadoop), (hadoop.security.authentication,simple), (hadoop.security.authorization,false), (hadoop.security.credential.clear-text-fallback,true), (hadoop.security.crypto.buffer.size,8192), (hadoop.security.crypto.cipher.suite,AES/CTR/NoPadding), (hadoop.security.crypto.codec.classes.aes.ctr.nopadding,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec), (hadoop.security.crypto.codec.classes.sm4.ctr.nopadding,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec, org.apache.hadoop.crypto.JceSm4CtrCryptoCodec), (hadoop.security.dns.log-slow-lookups.enabled,false), (hadoop.security.dns.log-slow-lookups.threshold.ms,1000), (hadoop.security.group.mapping,com.nvidia.sparkaas.hadoop.plugin.StaticConfigGroupProvider), (hadoop.security.group.mapping.ldap.connection.timeout.ms,60000), (hadoop.security.group.mapping.ldap.conversion.rule,none), (hadoop.security.group.mapping.ldap.directory.search.timeout,10000), (hadoop.security.group.mapping.ldap.num.attempts,3), (hadoop.security.group.mapping.ldap.num.attempts.before.failover,3), (hadoop.security.group.mapping.ldap.posix.attr.gid.name,gidNumber), (hadoop.security.group.mapping.ldap.posix.attr.uid.name,uidNumber), (hadoop.security.group.mapping.ldap.read.timeout.ms,60000), (hadoop.security.group.mapping.ldap.search.attr.group.name,cn), (hadoop.security.group.mapping.ldap.search.attr.member,member), (hadoop.security.group.mapping.ldap.search.filter.group,(objectClass=group)), (hadoop.security.group.mapping.ldap.search.filter.user,(&(objectClass=user)(sAMAccountName={0}))), (hadoop.security.group.mapping.ldap.search.group.hierarchy.levels,0), (hadoop.security.group.mapping.ldap.ssl,false), (hadoop.security.group.mapping.providers.combined,true), (hadoop.security.groups.cache.background.reload,false), (hadoop.security.groups.cache.background.reload.threads,3), (hadoop.security.groups.cache.secs,300), (hadoop.security.groups.cache.warn.after.ms,5000), (hadoop.security.groups.negative-cache.secs,30), (hadoop.security.groups.shell.command.timeout,0s), (hadoop.security.instrumentation.requires.admin,false), (hadoop.security.java.secure.random.algorithm,SHA1PRNG), (hadoop.security.key.default.bitlength,128), (hadoop.security.key.default.cipher,AES/CTR/NoPadding), (hadoop.security.kms.client.authentication.retry-count,1), (hadoop.security.kms.client.encrypted.key.cache.expiry,43200000), (hadoop.security.kms.client.encrypted.key.cache.low-watermark,0.3f), (hadoop.security.kms.client.encrypted.key.cache.num.refill.threads,2), (hadoop.security.kms.client.encrypted.key.cache.size,500), (hadoop.security.kms.client.failover.sleep.base.millis,100), (hadoop.security.kms.client.failover.sleep.max.millis,2000), (hadoop.security.kms.client.timeout,60), (hadoop.security.random.device.file.path,/dev/urandom), (hadoop.security.resolver.impl,org.apache.hadoop.net.DNSDomainNameResolver), (hadoop.security.secure.random.impl,org.apache.hadoop.crypto.random.OpensslSecureRandom), (hadoop.security.sensitive-config-keys,*********(redacted)), (hadoop.security.token.service.use_ip,*********(redacted)), (hadoop.security.uid.cache.secs,14400), (hadoop.service.shutdown.timeout,30s), (hadoop.shell.missing.defaultFs.warning,false), (hadoop.shell.safely.delete.limit.num.files,100), (hadoop.ssl.client.conf,ssl-client.xml), (hadoop.ssl.enabled.protocols,TLSv1.2), (hadoop.ssl.hostname.verifier,DEFAULT), (hadoop.ssl.keystores.factory.class,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory), (hadoop.ssl.require.client.cert,false), (hadoop.ssl.server.conf,ssl-server.xml), (hadoop.system.tags,YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n",
       "      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL), (hadoop.tags.system,YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT\n",
       "      ,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL), (hadoop.tmp.dir,/tmp/hadoop-\\${user.name}), (hadoop.user.group.static.mapping.overrides,dr.who=;), (hadoop.util.hash.type,murmur), (hadoop.workaround.non.threadsafe.getpwuid,true), (hadoop.zk.acl,world:anyone:rwcda), (hadoop.zk.num-retries,1000), (hadoop.zk.retry-interval-ms,1000), (hadoop.zk.timeout-ms,10000), (hive.metastore.uris,thrift://nvspark-metastore.nvspark.svc.cluster.local:9083), (io.bytes.per.checksum,512), (io.compression.codec.bzip2.library,system-native), (io.compression.codec.lz4.buffersize,262144), (io.compression.codec.lz4.use.lz4hc,false), (io.compression.codec.lzo.buffersize,65536), (io.compression.codec.lzo.class,org.apache.hadoop.io.compress.LzoCodec), (io.compression.codec.snappy.buffersize,262144), (io.compression.codec.zstd.buffersize,0), (io.compression.codec.zstd.level,3), (io.erasurecode.codec.native.enabled,true), (io.erasurecode.codec.rs-legacy.rawcoders,rs-legacy_java), (io.erasurecode.codec.rs.rawcoders,rs_native,rs_java), (io.erasurecode.codec.xor.rawcoders,xor_native,xor_java), (io.file.buffer.size,65536), (io.map.index.interval,128), (io.map.index.skip,0), (io.mapfile.bloom.error.rate,0.005), (io.mapfile.bloom.size,1048576), (io.seqfile.compress.blocksize,1000000), (io.seqfile.local.dir,\\${hadoop.tmp.dir}/io/local), (io.serializations,org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization), (io.skip.checksum.errors,false), (ipc.[port_number].backoff.enable,false), (ipc.[port_number].callqueue.impl,java.util.concurrent.LinkedBlockingQueue), (ipc.[port_number].callqueue.overflow.trigger.failover,false), (ipc.[port_number].cost-provider.impl,org.apache.hadoop.ipc.DefaultCostProvider), (ipc.[port_number].decay-scheduler.backoff.responsetime.enable,false), (ipc.[port_number].decay-scheduler.backoff.responsetime.thresholds,10s,20s,30s,40s), (ipc.[port_number].decay-scheduler.decay-factor,0.5), (ipc.[port_number].decay-scheduler.metrics.top.user.count,10), (ipc.[port_number].decay-scheduler.period-ms,5000), (ipc.[port_number].decay-scheduler.thresholds,13,25,50), (ipc.[port_number].faircallqueue.multiplexer.weights,8,4,2,1), (ipc.[port_number].identity-provider.impl,org.apache.hadoop.ipc.UserIdentityProvider), (ipc.[port_number].scheduler.impl,org.apache.hadoop.ipc.DefaultRpcScheduler), (ipc.[port_number].scheduler.priority.levels,4), (ipc.[port_number].weighted-cost.handler,1), (ipc.[port_number].weighted-cost.lockexclusive,100), (ipc.[port_number].weighted-cost.lockfree,1), (ipc.[port_number].weighted-cost.lockshared,10), (ipc.[port_number].weighted-cost.response,1), (ipc.backoff.enable,false), (ipc.callqueue.impl,java.util.concurrent.LinkedBlockingQueue), (ipc.callqueue.overflow.trigger.failover,false), (ipc.client.async.calls.max,100), (ipc.client.bind.wildcard.addr,false), (ipc.client.connect.max.retries,10), (ipc.client.connect.max.retries.on.sasl,5), (ipc.client.connect.max.retries.on.timeouts,45), (ipc.client.connect.retry.interval,1000), (ipc.client.connect.timeout,20000), (ipc.client.connection.idle-scan-interval.ms,10000), (ipc.client.connection.maxidletime,10000), (ipc.client.fallback-to-simple-auth-allowed,false), (ipc.client.idlethreshold,4000), (ipc.client.kill.max,10), (ipc.client.low-latency,false), (ipc.client.ping,true), (ipc.client.rpc-timeout.ms,120000), (ipc.client.tcpnodelay,true), (ipc.cost-provider.impl,org.apache.hadoop.ipc.DefaultCostProvider), (ipc.identity-provider.impl,org.apache.hadoop.ipc.UserIdentityProvider), (ipc.maximum.data.length,134217728), (ipc.maximum.response.length,134217728), (ipc.ping.interval,60000), (ipc.scheduler.impl,org.apache.hadoop.ipc.DefaultRpcScheduler), (ipc.server.handler.queue.size,100), (ipc.server.listen.queue.size,256), (ipc.server.log.slow.rpc,false), (ipc.server.log.slow.rpc.threshold.ms,0), (ipc.server.max.connections,0), (ipc.server.max.response.size,1048576), (ipc.server.metrics.update.runner.interval,5000), (ipc.server.purge.interval,15), (ipc.server.read.connection-queue.size,100), (ipc.server.read.threadpool.size,1), (ipc.server.reuseaddr,true), (ipc.server.tcpnodelay,true), (map.sort.class,org.apache.hadoop.util.QuickSort), (mapreduce.am.max-attempts,2), (mapreduce.app-submission.cross-platform,false), (mapreduce.client.completion.pollinterval,5000), (mapreduce.client.libjars.wildcard,true), (mapreduce.client.output.filter,FAILED), (mapreduce.client.progressmonitor.pollinterval,1000), (mapreduce.client.submit.file.replication,10), (mapreduce.cluster.acls.enabled,false), (mapreduce.cluster.local.dir,\\${hadoop.tmp.dir}/mapred/local), (mapreduce.fileoutputcommitter.algorithm.version,1), (mapreduce.fileoutputcommitter.task.cleanup.enabled,false), (mapreduce.framework.name,local), (mapreduce.ifile.readahead,true), (mapreduce.ifile.readahead.bytes,4194304), (mapreduce.input.fileinputformat.list-status.num-threads,20), (mapreduce.input.fileinputformat.split.minsize,0), (mapreduce.input.lineinputformat.linespermap,1), (mapreduce.job.acl-modify-job, ), (mapreduce.job.acl-view-job, ), (mapreduce.job.cache.limit.max-resources,0), (mapreduce.job.cache.limit.max-resources-mb,0), (mapreduce.job.cache.limit.max-single-resource-mb,0), (mapreduce.job.classloader,false), (mapreduce.job.committer.setup.cleanup.needed,true), (mapreduce.job.complete.cancel.delegation.tokens,*********(redacted)), (mapreduce.job.counters.max,120), (mapreduce.job.dfs.storage.capacity.kill-limit-exceed,false), (mapreduce.job.emit-timeline-data,false), (mapreduce.job.encrypted-intermediate-data,false), (mapreduce.job.encrypted-intermediate-data-key-size-bits,128), (mapreduce.job.encrypted-intermediate-data.buffer.kb,128), (mapreduce.job.end-notification.max.attempts,5), (mapreduce.job.end-notification.max.retry.interval,5000), (mapreduce.job.end-notification.retry.attempts,0), (mapreduce.job.end-notification.retry.interval,1000), (mapreduce.job.finish-when-all-reducers-done,true), (mapreduce.job.hdfs-servers,\\${fs.defaultFS}), (mapreduce.job.heap.memory-mb.ratio,0.8), (mapreduce.job.local-fs.single-disk-limit.bytes,-1), (mapreduce.job.local-fs.single-disk-limit.check.interval-ms,5000), (mapreduce.job.local-fs.single-disk-limit.check.kill-limit-exceed,true), (mapreduce.job.map.output.collector.class,org.apache.hadoop.mapred.MapTask\\$MapOutputBuffer), (mapreduce.job.maps,2), (mapreduce.job.max.map,-1), (mapreduce.job.max.split.locations,15), (mapreduce.job.maxtaskfailures.per.tracker,3), (mapreduce.job.queuename,default), (mapreduce.job.reduce.shuffle.consumer.plugin.class,org.apache.hadoop.mapreduce.task.reduce.Shuffle), (mapreduce.job.reduce.slowstart.completedmaps,0.05), (mapreduce.job.reducer.preempt.delay.sec,0), (mapreduce.job.reducer.unconditional-preempt.delay.sec,300), (mapreduce.job.reduces,1), (mapreduce.job.running.map.limit,0), (mapreduce.job.running.reduce.limit,0), (mapreduce.job.sharedcache.mode,disabled), (mapreduce.job.speculative.minimum-allowed-tasks,10), (mapreduce.job.speculative.retry-after-no-speculate,1000), (mapreduce.job.speculative.retry-after-speculate,15000), (mapreduce.job.speculative.slowtaskthreshold,1.0), (mapreduce.job.speculative.speculative-cap-running-tasks,0.1), (mapreduce.job.speculative.speculative-cap-total-tasks,0.01), (mapreduce.job.split.metainfo.maxsize,10000000), (mapreduce.job.token.tracking.ids.enabled,*********(redacted)), (mapreduce.job.ubertask.enable,false), (mapreduce.job.ubertask.maxmaps,9), (mapreduce.job.ubertask.maxreduces,1), (mapreduce.jobhistory.address,0.0.0.0:10020), (mapreduce.jobhistory.admin.acl,*), (mapreduce.jobhistory.admin.address,0.0.0.0:10033), (mapreduce.jobhistory.always-scan-user-dir,false), (mapreduce.jobhistory.cleaner.enable,true), (mapreduce.jobhistory.cleaner.interval-ms,86400000), (mapreduce.jobhistory.client.thread-count,10), (mapreduce.jobhistory.datestring.cache.size,200000), (mapreduce.jobhistory.done-dir,\\${yarn.app.mapreduce.am.staging-dir}/history/done), (mapreduce.jobhistory.http.policy,HTTP_ONLY), (mapreduce.jobhistory.intermediate-done-dir,\\${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate), (mapreduce.jobhistory.intermediate-user-done-dir.permissions,770), (mapreduce.jobhistory.jhist.format,binary), (mapreduce.jobhistory.joblist.cache.size,20000), (mapreduce.jobhistory.jobname.limit,50), (mapreduce.jobhistory.keytab,/etc/security/keytab/jhs.service.keytab), (mapreduce.jobhistory.loadedjob.tasks.max,-1), (mapreduce.jobhistory.loadedjobs.cache.size,5), (mapreduce.jobhistory.max-age-ms,604800000), (mapreduce.jobhistory.minicluster.fixed.ports,false), (mapreduce.jobhistory.move.interval-ms,180000), (mapreduce.jobhistory.move.thread-count,3), (mapreduce.jobhistory.principal,jhs/_HOST@REALM.TLD), (mapreduce.jobhistory.recovery.enable,false), (mapreduce.jobhistory.recovery.store.class,org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService), (mapreduce.jobhistory.recovery.store.fs.uri,\\${hadoop.tmp.dir}/mapred/history/recoverystore), (mapreduce.jobhistory.recovery.store.leveldb.path,\\${hadoop.tmp.dir}/mapred/history/recoverystore), (mapreduce.jobhistory.webapp.address,0.0.0.0:19888), (mapreduce.jobhistory.webapp.https.address,0.0.0.0:19890), (mapreduce.jobhistory.webapp.rest-csrf.custom-header,X-XSRF-Header), (mapreduce.jobhistory.webapp.rest-csrf.enabled,false), (mapreduce.jobhistory.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (mapreduce.jobhistory.webapp.xfs-filter.xframe-options,SAMEORIGIN), (mapreduce.jvm.add-opens-as-default,true), (mapreduce.jvm.system-properties-to-log,os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name), (mapreduce.map.cpu.vcores,1), (mapreduce.map.log.level,INFO), (mapreduce.map.maxattempts,4), (mapreduce.map.memory.mb,-1), (mapreduce.map.output.compress,false), (mapreduce.map.output.compress.codec,org.apache.hadoop.io.compress.DefaultCodec), (mapreduce.map.skip.maxrecords,0), (mapreduce.map.skip.proc-count.auto-incr,true), (mapreduce.map.sort.spill.percent,0.80), (mapreduce.map.speculative,true), (mapreduce.output.fileoutputformat.compress,false), (mapreduce.output.fileoutputformat.compress.codec,org.apache.hadoop.io.compress.DefaultCodec), (mapreduce.output.fileoutputformat.compress.type,RECORD), (mapreduce.outputcommitter.factory.scheme.abfs,org.apache.hadoop.fs.azurebfs.commit.AzureManifestCommitterFactory), (mapreduce.outputcommitter.factory.scheme.gs,org.apache.hadoop.mapreduce.lib.output.committer.manifest.ManifestCommitterFactory), (mapreduce.outputcommitter.factory.scheme.s3a,org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory), (mapreduce.reduce.cpu.vcores,1), (mapreduce.reduce.input.buffer.percent,0.0), (mapreduce.reduce.log.level,INFO), (mapreduce.reduce.markreset.buffer.percent,0.0), (mapreduce.reduce.maxattempts,4), (mapreduce.reduce.memory.mb,-1), (mapreduce.reduce.merge.inmem.threshold,1000), (mapreduce.reduce.shuffle.connect.timeout,180000), (mapreduce.reduce.shuffle.fetch.retry.enabled,\\${yarn.nodemanager.recovery.enabled}), (mapreduce.reduce.shuffle.fetch.retry.interval-ms,1000), (mapreduce.reduce.shuffle.fetch.retry.timeout-ms,30000), (mapreduce.reduce.shuffle.input.buffer.percent,0.70), (mapreduce.reduce.shuffle.memory.limit.percent,0.25), (mapreduce.reduce.shuffle.merge.percent,0.66), (mapreduce.reduce.shuffle.parallelcopies,5), (mapreduce.reduce.shuffle.read.timeout,180000), (mapreduce.reduce.shuffle.retry-delay.max.ms,60000), (mapreduce.reduce.skip.maxgroups,0), (mapreduce.reduce.skip.proc-count.auto-incr,true), (mapreduce.reduce.speculative,true), (mapreduce.shuffle.connection-keep-alive.enable,false), (mapreduce.shuffle.connection-keep-alive.timeout,5), (mapreduce.shuffle.listen.queue.size,128), (mapreduce.shuffle.max.connections,0), (mapreduce.shuffle.max.threads,0), (mapreduce.shuffle.pathcache.concurrency-level,16), (mapreduce.shuffle.pathcache.expire-after-access-minutes,5), (mapreduce.shuffle.pathcache.max-weight,10485760), (mapreduce.shuffle.port,13562), (mapreduce.shuffle.ssl.enabled,false), (mapreduce.shuffle.ssl.file.buffer.size,65536), (mapreduce.shuffle.transfer.buffer.size,131072), (mapreduce.task.combine.progress.records,10000), (mapreduce.task.exit.timeout,60000), (mapreduce.task.exit.timeout.check-interval-ms,20000), (mapreduce.task.files.preserve.failedtasks,false), (mapreduce.task.io.sort.factor,10), (mapreduce.task.io.sort.mb,100), (mapreduce.task.local-fs.write-limit.bytes,-1), (mapreduce.task.merge.progress.records,10000), (mapreduce.task.ping-for-liveliness-check.enabled,false), (mapreduce.task.profile,false), (mapreduce.task.profile.map.params,\\${mapreduce.task.profile.params}), (mapreduce.task.profile.maps,0-2), (mapreduce.task.profile.params,-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s), (mapreduce.task.profile.reduce.params,\\${mapreduce.task.profile.params}), (mapreduce.task.profile.reduces,0-2), (mapreduce.task.skip.start.attempts,2), (mapreduce.task.spill.files.count.limit,-1), (mapreduce.task.stuck.timeout-ms,600000), (mapreduce.task.timeout,600000), (mapreduce.task.userlog.limit.kb,0), (net.topology.impl,org.apache.hadoop.net.NetworkTopology), (net.topology.node.switch.mapping.impl,org.apache.hadoop.net.ScriptBasedMapping), (net.topology.script.number.args,100), (nfs.exports.allowed.hosts,* rw), (rpc.metrics.quantile.enable,false), (rpc.metrics.timeunit,MILLISECONDS), (seq.io.sort.factor,100), (seq.io.sort.mb,100), (tfile.fs.input.buffer.size,262144), (tfile.fs.output.buffer.size,262144), (tfile.io.chunk.size,1048576), (yarn.acl.enable,false), (yarn.acl.reservation-enable,false), (yarn.admin.acl,*), (yarn.am.liveness-monitor.expiry-interval-ms,900000), (yarn.app.attempt.diagnostics.limit.kc,64), (yarn.app.mapreduce.am.command-opts,-Xmx1024m), (yarn.app.mapreduce.am.container.log.backups,0), (yarn.app.mapreduce.am.container.log.limit.kb,0), (yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size,10), (yarn.app.mapreduce.am.hard-kill-timeout-ms,10000), (yarn.app.mapreduce.am.job.committer.cancel-timeout,60000), (yarn.app.mapreduce.am.job.committer.commit-window,10000), (yarn.app.mapreduce.am.job.task.listener.thread-count,30), (yarn.app.mapreduce.am.log.level,INFO), (yarn.app.mapreduce.am.resource.cpu-vcores,1), (yarn.app.mapreduce.am.resource.mb,1536), (yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms,1000), (yarn.app.mapreduce.am.staging-dir,/tmp/hadoop-yarn/staging), (yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled,false), (yarn.app.mapreduce.am.webapp.https.client.auth,false), (yarn.app.mapreduce.am.webapp.https.enabled,false), (yarn.app.mapreduce.client-am.ipc.max-retries,3), (yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts,3), (yarn.app.mapreduce.client.job.max-retries,3), (yarn.app.mapreduce.client.job.retry-interval,2000), (yarn.app.mapreduce.client.max-retries,3), (yarn.app.mapreduce.shuffle.log.backups,0), (yarn.app.mapreduce.shuffle.log.limit.kb,0), (yarn.app.mapreduce.shuffle.log.separate,true), (yarn.app.mapreduce.task.container.log.backups,0), (yarn.apps.cache.enable,false), (yarn.apps.cache.expire,30s), (yarn.apps.cache.size,1000), (yarn.client.application-client-protocol.poll-interval-ms,200), (yarn.client.application-client-protocol.poll-timeout-ms,-1), (yarn.client.failover-no-ha-proxy-provider,org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider), (yarn.client.failover-proxy-provider,org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider), (yarn.client.failover-retries,0), (yarn.client.failover-retries-on-socket-timeouts,0), (yarn.client.load.resource-types.from-server,false), (yarn.client.max-cached-nodemanagers-proxies,0), (yarn.client.nodemanager-client-async.thread-pool-max-size,500), (yarn.client.nodemanager-connect.max-wait-ms,180000), (yarn.client.nodemanager-connect.retry-interval-ms,10000), (yarn.cluster.max-application-priority,0), (yarn.dispatcher.cpu-monitor.samples-per-min,60), (yarn.dispatcher.drain-events.timeout,300000), (yarn.dispatcher.print-events-info.threshold,5000), (yarn.dispatcher.print-thread-pool.core-pool-size,1), (yarn.dispatcher.print-thread-pool.keep-alive-time,10s), (yarn.dispatcher.print-thread-pool.maximum-pool-size,5), (yarn.fail-fast,false), (yarn.federation.amrmproxy.allocation.history.max.entry,100), (yarn.federation.amrmproxy.register.uam.interval,100ms), (yarn.federation.amrmproxy.register.uam.retry-count,3), (yarn.federation.cache-entity.nums,1000), (yarn.federation.cache-ttl.secs,300), (yarn.federation.cache.class,org.apache.hadoop.yarn.server.federation.cache.FederationJCache), (yarn.federation.enabled,false), (yarn.federation.failover.random.order,false), (yarn.federation.gpg.application.cleaner.class,org.apache.hadoop.yarn.server.globalpolicygenerator.applicationcleaner.DefaultApplicationCleaner), (yarn.federation.gpg.application.cleaner.contact.router.spec,3,10,600000), (yarn.federation.gpg.application.cleaner.interval-ms,-1s), (yarn.federation.gpg.policy.generator.class,org.apache.hadoop.yarn.server.globalpolicygenerator.policygenerator.NoOpGlobalPolicy), (yarn.federation.gpg.policy.generator.interval,1h), (yarn.federation.gpg.policy.generator.interval-ms,3600000), (yarn.federation.gpg.policy.generator.load-based.edit.maximum,3), (yarn.federation.gpg.policy.generator.load-based.pending.maximum,1000), (yarn.federation.gpg.policy.generator.load-based.pending.minimum,100), (yarn.federation.gpg.policy.generator.load-based.scaling,LINEAR), (yarn.federation.gpg.policy.generator.load-based.weight.minimum,0), (yarn.federation.gpg.policy.generator.readonly,false), (yarn.federation.gpg.scheduled.executor.threads,10), (yarn.federation.gpg.subcluster.cleaner.interval-ms,-1ms), (yarn.federation.gpg.subcluster.heartbeat.expiration-ms,30m), (yarn.federation.gpg.webapp.address,0.0.0.0:8069), (yarn.federation.gpg.webapp.connect-timeout,30s), (yarn.federation.gpg.webapp.cross-origin.enabled,false), (yarn.federation.gpg.webapp.https.address,0.0.0.0:8070), (yarn.federation.gpg.webapp.read-timeout,30s), (yarn.federation.non-ha.enabled,false), (yarn.federation.registry.base-dir,yarnfederation/), (yarn.federation.state-store.class,org.apache.hadoop.yarn.server.federation.store.impl.MemoryFederationStateStore), (yarn.federation.state-store.clean-up-retry-count,1), (yarn.federation.state-store.clean-up-retry-sleep-time,1s), (yarn.federation.state-store.heartbeat.initial-delay,30s), (yarn.federation.state-store.max-applications,1000), (yarn.federation.state-store.sql.conn-time-out,10s), (yarn.federation.state-store.sql.idle-time-out,10m), (yarn.federation.state-store.sql.max-life-time,30m), (yarn.federation.state-store.sql.minimum-idle,1), (yarn.federation.state-store.sql.pool-name,YARN-Federation-DataBasePool), (yarn.federation.subcluster-resolver.class,org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl), (yarn.fs-store.file.replication,0), (yarn.http.policy,HTTP_ONLY), (yarn.intermediate-data-encryption.enable,false), (yarn.ipc.rpc.class,org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC), (yarn.is.minicluster,false), (yarn.log-aggregation-enable,false), (yarn.log-aggregation-status.time-out.ms,600000), (yarn.log-aggregation.debug.filesize,104857600), (yarn.log-aggregation.enable-local-cleanup,true), (yarn.log-aggregation.file-controller.TFile.class,org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController), (yarn.log-aggregation.file-formats,TFile), (yarn.log-aggregation.retain-check-interval-seconds,-1), (yarn.log-aggregation.retain-seconds,-1), (yarn.minicluster.control-resource-monitoring,false), (yarn.minicluster.fixed.ports,false), (yarn.minicluster.use-rpc,false), (yarn.minicluster.yarn.nodemanager.resource.memory-mb,4096), (yarn.nm.liveness-monitor.expiry-interval-ms,600000), (yarn.node-attribute.fs-store.impl.class,org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore), (yarn.node-labels.configuration-type,centralized), (yarn.node-labels.enabled,false), (yarn.node-labels.fs-store.impl.class,org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore), (yarn.nodemanager.address,\\${yarn.nodemanager.hostname}:0), (yarn.nodemanager.admin-env,MALLOC_ARENA_MAX=\\$MALLOC_ARENA_MAX), (yarn.nodemanager.amrmproxy.address,0.0.0.0:8049), (yarn.nodemanager.amrmproxy.client.thread-count,25), (yarn.nodemanager.amrmproxy.enabled,false), (yarn.nodemanager.amrmproxy.ha.enable,false), (yarn.nodemanager.amrmproxy.interceptor-class.pipeline,org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor), (yarn.nodemanager.amrmproxy.wait.uam-register.done,false), (yarn.nodemanager.aux-services.%s.classpath,NONE), (yarn.nodemanager.aux-services.manifest.enabled,false), (yarn.nodemanager.aux-services.manifest.reload-ms,0), (yarn.nodemanager.aux-services.mapreduce_shuffle.class,org.apache.hadoop.mapred.ShuffleHandler), (yarn.nodemanager.collector-service.address,\\${yarn.nodemanager.hostname}:8048), (yarn.nodemanager.collector-service.thread-count,5), (yarn.nodemanager.container-diagnostics-maximum-size,10000), (yarn.nodemanager.container-executor.class,org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor), (yarn.nodemanager.container-executor.exit-code-file.timeout-ms,2000), (yarn.nodemanager.container-localizer.java.opts,-Xmx256m), (yarn.nodemanager.container-localizer.java.opts.add-exports-as-default,true), (yarn.nodemanager.container-localizer.log.level,INFO), (yarn.nodemanager.container-log-monitor.dir-size-limit-bytes,1000000000), (yarn.nodemanager.container-log-monitor.enable,false), (yarn.nodemanager.container-log-monitor.interval-ms,60000), (yarn.nodemanager.container-log-monitor.total-size-limit-bytes,10000000000), (yarn.nodemanager.container-manager.thread-count,20), (yarn.nodemanager.container-metrics.enable,true), (yarn.nodemanager.container-metrics.period-ms,-1), (yarn.nodemanager.container-metrics.unregister-delay-ms,10000), (yarn.nodemanager.container-monitor.enabled,true), (yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled,false), (yarn.nodemanager.container-retry-minimum-interval-ms,1000), (yarn.nodemanager.container.stderr.pattern,{*stderr*,*STDERR*}), (yarn.nodemanager.container.stderr.tail.bytes,4096), (yarn.nodemanager.containers-launcher.class,org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher), (yarn.nodemanager.default-container-executor.log-dirs.permissions,710), (yarn.nodemanager.delete.debug-delay-sec,0), (yarn.nodemanager.delete.thread-count,4), (yarn.nodemanager.disk-health-checker.disk-free-space-threshold.enabled,true), (yarn.nodemanager.disk-health-checker.disk-utilization-threshold.enabled,true), (yarn.nodemanager.disk-health-checker.enable,true), (yarn.nodemanager.disk-health-checker.interval-ms,120000), (yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage,90.0), (yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb,0), (yarn.nodemanager.disk-health-checker.min-free-space-per-disk-watermark-high-mb,0), (yarn.nodemanager.disk-health-checker.min-healthy-disks,0.25), (yarn.nodemanager.disk-validator,basic), (yarn.nodemanager.dispatcher.metric.enable,false), (yarn.nodemanager.distributed-scheduling.enabled,false), (yarn.nodemanager.elastic-memory-control.enabled,false), (yarn.nodemanager.elastic-memory-control.oom-handler,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.DefaultOOMHandler), (yarn.nodemanager.elastic-memory-control.timeout-sec,5), (yarn.nodemanager.emit-container-events,true), (yarn.nodemanager.env-whitelist,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ), (yarn.nodemanager.health-checker.interval-ms,600000), (yarn.nodemanager.health-checker.run-before-startup,false), (yarn.nodemanager.health-checker.scripts,script), (yarn.nodemanager.health-checker.timeout-ms,1200000), (yarn.nodemanager.hostname,0.0.0.0), (yarn.nodemanager.keytab,/etc/krb5.keytab), (yarn.nodemanager.least-load-policy-selector.enabled,false), (yarn.nodemanager.least-load-policy-selector.fail-on-error,true), (yarn.nodemanager.least-load-policy-selector.multiplier,50000), (yarn.nodemanager.least-load-policy-selector.pending-container.threshold,10000), (yarn.nodemanager.least-load-policy-selector.use-active-core,false), (yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms,20), (yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms,1000), (yarn.nodemanager.linux-container-executor.cgroups.hierarchy,/hadoop-yarn), (yarn.nodemanager.linux-container-executor.cgroups.mount,false), (yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage,false), (yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users,true), (yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user,nobody), (yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern,^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[\\$]?\\$), (yarn.nodemanager.linux-container-executor.resources-handler.class,org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler), (yarn.nodemanager.local-cache.max-files-per-directory,8192), (yarn.nodemanager.local-dirs,\\${hadoop.tmp.dir}/nm-local-dir), (yarn.nodemanager.localizer.address,\\${yarn.nodemanager.hostname}:8040), (yarn.nodemanager.localizer.cache.cleanup.interval-ms,600000), (yarn.nodemanager.localizer.cache.target-size-mb,10240), (yarn.nodemanager.localizer.client.thread-count,5), (yarn.nodemanager.localizer.fetch.thread-count,4), (yarn.nodemanager.log-aggregation.compression-type,none), (yarn.nodemanager.log-aggregation.num-log-files-per-app,30), (yarn.nodemanager.log-aggregation.policy.class,org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy), (yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds,-1), (yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min,3600), (yarn.nodemanager.log-container-debug-info-on-error.enabled,false), (yarn.nodemanager.log-container-debug-info.enabled,true), (yarn.nodemanager.log-dirs,\\${yarn.log.dir}/userlogs), (yarn.nodemanager.log.delete.threshold,100g), (yarn.nodemanager.log.deletion-threads-count,4), (yarn.nodemanager.log.retain-seconds,10800), (yarn.nodemanager.log.trigger.delete.by-size.enabled,false), (yarn.nodemanager.logaggregation.threadpool-size-max,100), (yarn.nodemanager.node-attributes.provider.fetch-interval-ms,600000), (yarn.nodemanager.node-attributes.provider.fetch-timeout-ms,1200000), (yarn.nodemanager.node-attributes.resync-interval-ms,120000), (yarn.nodemanager.node-labels.provider.fetch-interval-ms,600000), (yarn.nodemanager.node-labels.provider.fetch-timeout-ms,1200000), (yarn.nodemanager.node-labels.resync-interval-ms,120000), (yarn.nodemanager.numa-awareness.enabled,false), (yarn.nodemanager.numa-awareness.numactl.cmd,/usr/bin/numactl), (yarn.nodemanager.numa-awareness.read-topology,false), (yarn.nodemanager.opportunistic-containers-max-queue-length,0), (yarn.nodemanager.opportunistic-containers-queue-policy,BY_QUEUE_LEN), (yarn.nodemanager.opportunistic-containers-use-pause-for-preemption,false), (yarn.nodemanager.pluggable-device-framework.enabled,false), (yarn.nodemanager.pmem-check-enabled,true), (yarn.nodemanager.process-kill-wait.ms,5000), (yarn.nodemanager.recovery.compaction-interval-secs,3600), (yarn.nodemanager.recovery.dir,\\${hadoop.tmp.dir}/yarn-nm-recovery), (yarn.nodemanager.recovery.enabled,false), (yarn.nodemanager.recovery.supervised,false), (yarn.nodemanager.remote-app-log-dir,/tmp/logs), (yarn.nodemanager.remote-app-log-dir-include-older,true), (yarn.nodemanager.remote-app-log-dir-suffix,logs), (yarn.nodemanager.resource-monitor.interval-ms,3000), (yarn.nodemanager.resource-plugins.fpga.allowed-fpga-devices,auto), (yarn.nodemanager.resource-plugins.fpga.vendor-plugin.class,org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.fpga.IntelFpgaOpenclPlugin), (yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices,auto), (yarn.nodemanager.resource-plugins.gpu.docker-plugin,nvidia-docker-v1), (yarn.nodemanager.resource-plugins.gpu.docker-plugin.nvidia-docker-v1.endpoint,http://localhost:3476/v1.0/docker/cli), (yarn.nodemanager.resource.count-logical-processors-as-cores,false), (yarn.nodemanager.resource.cpu-vcores,-1), (yarn.nodemanager.resource.detect-hardware-capabilities,false), (yarn.nodemanager.resource.memory-mb,-1), (yarn.nodemanager.resource.memory.cgroups.soft-limit-percentage,90.0), (yarn.nodemanager.resource.memory.cgroups.swappiness,0), (yarn.nodemanager.resource.memory.enabled,false), (yarn.nodemanager.resource.memory.enforced,true), (yarn.nodemanager.resource.pcores-vcores-multiplier,1.0), (yarn.nodemanager.resource.percentage-physical-cpu-limit,100), (yarn.nodemanager.resource.system-reserved-memory-mb,-1), (yarn.nodemanager.resourcemanager.minimum.version,NONE), (yarn.nodemanager.runtime.linux.allowed-runtimes,default), (yarn.nodemanager.runtime.linux.docker.allowed-container-networks,host,none,bridge), (yarn.nodemanager.runtime.linux.docker.allowed-container-runtimes,runc), (yarn.nodemanager.runtime.linux.docker.capabilities,CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE), (yarn.nodemanager.runtime.linux.docker.default-container-network,host), (yarn.nodemanager.runtime.linux.docker.delayed-removal.allowed,false), (yarn.nodemanager.runtime.linux.docker.enable-userremapping.allowed,true), (yarn.nodemanager.runtime.linux.docker.host-pid-namespace.allowed,false), (yarn.nodemanager.runtime.linux.docker.image-update,false), (yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed,false), (yarn.nodemanager.runtime.linux.docker.stop.grace-period,10), (yarn.nodemanager.runtime.linux.docker.userremapping-gid-threshold,1), (yarn.nodemanager.runtime.linux.docker.userremapping-uid-threshold,1), (yarn.nodemanager.runtime.linux.runc.allowed-container-networks,host,none,bridge), (yarn.nodemanager.runtime.linux.runc.allowed-container-runtimes,runc), (yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-size,500), (yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs,360), (yarn.nodemanager.runtime.linux.runc.host-pid-namespace.allowed,false), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.cache-refresh-interval-secs,60), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.hdfs-hash-file,/runc-root/image-tag-to-hash), (yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin.num-manifests-to-cache,10), (yarn.nodemanager.runtime.linux.runc.image-toplevel-dir,/runc-root), (yarn.nodemanager.runtime.linux.runc.layer-mounts-interval-secs,600), (yarn.nodemanager.runtime.linux.runc.layer-mounts-to-keep,100), (yarn.nodemanager.runtime.linux.runc.manifest-to-resources-plugin,org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.HdfsManifestToResourcesPlugin), (yarn.nodemanager.runtime.linux.runc.privileged-containers.allowed,false), (yarn.nodemanager.runtime.linux.sandbox-mode,disabled), (yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions,read), (yarn.nodemanager.sleep-delay-before-sigkill.ms,250), (yarn.nodemanager.vmem-check-enabled,true), (yarn.nodemanager.vmem-pmem-ratio,2.1), (yarn.nodemanager.webapp.address,\\${yarn.nodemanager.hostname}:8042), (yarn.nodemanager.webapp.cross-origin.enabled,false), (yarn.nodemanager.webapp.https.address,0.0.0.0:8044), (yarn.nodemanager.webapp.rest-csrf.custom-header,X-XSRF-Header), (yarn.nodemanager.webapp.rest-csrf.enabled,false), (yarn.nodemanager.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (yarn.nodemanager.webapp.xfs-filter.xframe-options,SAMEORIGIN), (yarn.nodemanager.windows-container.cpu-limit.enabled,false), (yarn.nodemanager.windows-container.memory-limit.enabled,false), (yarn.registry.class,org.apache.hadoop.registry.client.impl.FSRegistryOperationsService), (yarn.resourcemanager.activities-manager.app-activities.max-queue-length,100), (yarn.resourcemanager.activities-manager.app-activities.ttl-ms,600000), (yarn.resourcemanager.activities-manager.cleanup-interval-ms,5000), (yarn.resourcemanager.activities-manager.scheduler-activities.ttl-ms,600000), (yarn.resourcemanager.address,\\${yarn.resourcemanager.hostname}:8032), (yarn.resourcemanager.admin.address,\\${yarn.resourcemanager.hostname}:8033), (yarn.resourcemanager.admin.client.thread-count,1), (yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs,*********(redacted)), (yarn.resourcemanager.am.max-attempts,2), (yarn.resourcemanager.amlauncher.thread-count,50), (yarn.resourcemanager.application-https.policy,NONE), (yarn.resourcemanager.application-tag-based-placement.enable,false), (yarn.resourcemanager.application-tag-based-placement.force-lowercase,true), (yarn.resourcemanager.application-timeouts.monitor.interval-ms,3000), (yarn.resourcemanager.application.max-tag.length,100), (yarn.resourcemanager.application.max-tags,10), (yarn.resourcemanager.auto-update.containers,false), (yarn.resourcemanager.client.thread-count,50), (yarn.resourcemanager.configuration.file-system-based-store,/yarn/conf), (yarn.resourcemanager.configuration.provider-class,org.apache.hadoop.yarn.LocalConfigurationProvider), (yarn.resourcemanager.connect.max-wait.ms,900000), (yarn.resourcemanager.connect.retry-interval.ms,30000), (yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs,*********(redacted)), (yarn.resourcemanager.container.liveness-monitor.interval-ms,600000), (yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs,20), (yarn.resourcemanager.delayed.delegation-token.removal-interval-ms,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-count,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-retry-interval,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-retry-max-attempts,*********(redacted)), (yarn.resourcemanager.delegation-token-renewer.thread-timeout,*********(redacted)), (yarn.resourcemanager.delegation-token.always-cancel,*********(redacted)), (yarn.resourcemanager.delegation-token.max-conf-size-bytes,*********(redacted)), (yarn.resourcemanager.delegation.key.update-interval,86400000), (yarn.resourcemanager.delegation.token.max-lifetime,*********(redacted)), (yarn.resourcemanager.delegation.token.remove-scan-interval,*********(redacted)), (yarn.resourcemanager.delegation.token.renew-interval,*********(redacted)), (yarn.resourcemanager.enable-node-untracked-without-include-path,false), (yarn.resourcemanager.epoch.range,0), (yarn.resourcemanager.fail-fast,\\${yarn.fail-fast}), (yarn.resourcemanager.fs.state-store.num-retries,0), (yarn.resourcemanager.fs.state-store.retry-interval-ms,1000), (yarn.resourcemanager.fs.state-store.uri,\\${hadoop.tmp.dir}/yarn/system/rmstore), (yarn.resourcemanager.ha.automatic-failover.embedded,true), (yarn.resourcemanager.ha.automatic-failover.enabled,true), (yarn.resourcemanager.ha.automatic-failover.zk-base-path,/yarn-leader-election), (yarn.resourcemanager.ha.enabled,false), (yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size,10), (yarn.resourcemanager.hostname,0.0.0.0), (yarn.resourcemanager.keytab,/etc/krb5.keytab), (yarn.resourcemanager.leveldb-state-store.compaction-interval-secs,3600), (yarn.resourcemanager.leveldb-state-store.path,\\${hadoop.tmp.dir}/yarn/system/rmstore), (yarn.resourcemanager.max-completed-applications,1000), (yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory,10), (yarn.resourcemanager.metrics.runtime.buckets,60,300,1440), (yarn.resourcemanager.nm-container-queuing.load-comparator,QUEUE_LENGTH), (yarn.resourcemanager.nm-container-queuing.max-queue-length,15), (yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms,100), (yarn.resourcemanager.nm-container-queuing.min-queue-length,5), (yarn.resourcemanager.nm-container-queuing.min-queue-wait-time-ms,10), (yarn.resourcemanager.nm-container-queuing.queue-limit-stdev,1.0f), (yarn.resourcemanager.nm-container-queuing.sorting-nodes-interval-ms,1000), (yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs,*********(redacted)), (yarn.resourcemanager.node-ip-cache.expiry-interval-secs,-1), (yarn.resourcemanager.node-labels.am.allow-non-exclusive-allocation,false), (yarn.resourcemanager.node-labels.provider.fetch-interval-ms,1800000), (yarn.resourcemanager.node-labels.provider.update-newly-registered-nodes-interval-ms,30000), (yarn.resourcemanager.node-removal-untracked.timeout-ms,60000), (yarn.resourcemanager.nodemanager-connect-retries,10), (yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs,3600), (yarn.resourcemanager.nodemanager.minimum.version,NONE), (yarn.resourcemanager.nodemanagers.heartbeat-interval-max-ms,1000), (yarn.resourcemanager.nodemanagers.heartbeat-interval-min-ms,1000), (yarn.resourcemanager.nodemanagers.heartbeat-interval-ms,1000), (yarn.resourcemanager.nodemanagers.heartbeat-interval-scaling-enable,false), (yarn.resourcemanager.nodemanagers.heartbeat-interval-slowdown-factor,1.0), (yarn.resourcemanager.nodemanagers.heartbeat-interval-speedup-factor,1.0), (yarn.resourcemanager.nodestore-rootdir.num-retries,1000), (yarn.resourcemanager.nodestore-rootdir.retry-interval-ms,1000), (yarn.resourcemanager.opportunistic-container-allocation.enabled,false), (yarn.resourcemanager.opportunistic-container-allocation.nodes-used,10), (yarn.resourcemanager.opportunistic.max.container-allocation.per.am.heartbeat,-1), (yarn.resourcemanager.placement-constraints.algorithm.class,org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.algorithm.DefaultPlacementAlgorithm), (yarn.resourcemanager.placement-constraints.algorithm.iterator,SERIAL), (yarn.resourcemanager.placement-constraints.algorithm.pool-size,1), (yarn.resourcemanager.placement-constraints.handler,disabled), (yarn.resourcemanager.placement-constraints.retry-attempts,3), (yarn.resourcemanager.placement-constraints.scheduler.pool-size,1), (yarn.resourcemanager.proxy-user-privileges.enabled,false), (yarn.resourcemanager.proxy.connection.timeout,60000), (yarn.resourcemanager.proxy.timeout.enabled,true), (yarn.resourcemanager.recovery.enabled,false), (yarn.resourcemanager.reservation-system.enable,false), (yarn.resourcemanager.reservation-system.planfollower.time-step,1000), (yarn.resourcemanager.resource-profiles.enabled,false), (yarn.resourcemanager.resource-profiles.source-file,resource-profiles.json), (yarn.resourcemanager.resource-tracker.address,\\${yarn.resourcemanager.hostname}:8031), (yarn.resourcemanager.resource-tracker.client.thread-count,50), (yarn.resourcemanager.resource-tracker.nm.ip-hostname-check,false), (yarn.resourcemanager.rm.container-allocation.expiry-interval-ms,600000), (yarn.resourcemanager.scheduler.address,\\${yarn.resourcemanager.hostname}:8030), (yarn.resourcemanager.scheduler.class,org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler), (yarn.resourcemanager.scheduler.client.thread-count,50), (yarn.resourcemanager.scheduler.monitor.enable,false), (yarn.resourcemanager.scheduler.monitor.policies,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy), (yarn.resourcemanager.state-store.max-completed-applications,\\${yarn.resourcemanager.max-completed-applications}), (yarn.resourcemanager.store.class,org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore), (yarn.resourcemanager.submission-preprocessor.enabled,false), (yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms,60000), (yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size,10), (yarn.resourcemanager.system-metrics-publisher.enabled,false), (yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.batch-size,1000), (yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.enable-batch,false), (yarn.resourcemanager.system-metrics-publisher.timeline-server-v1.interval-seconds,60), (yarn.resourcemanager.webapp.address,\\${yarn.resourcemanager.hostname}:8088), (yarn.resourcemanager.webapp.cross-origin.enabled,false), (yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled,*********(redacted)), (yarn.resourcemanager.webapp.https.address,\\${yarn.resourcemanager.hostname}:8090), (yarn.resourcemanager.webapp.rest-csrf.custom-header,X-XSRF-Header), (yarn.resourcemanager.webapp.rest-csrf.enabled,false), (yarn.resourcemanager.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (yarn.resourcemanager.webapp.ui-actions.enabled,true), (yarn.resourcemanager.webapp.xfs-filter.xframe-options,SAMEORIGIN), (yarn.resourcemanager.work-preserving-recovery.enabled,true), (yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms,10000), (yarn.resourcemanager.zk-appid-node.split-index,0), (yarn.resourcemanager.zk-client-ssl.enabled,false), (yarn.resourcemanager.zk-delegation-token-node.split-index,*********(redacted)), (yarn.resourcemanager.zk-max-znode-size.bytes,1048576), (yarn.resourcemanager.zk-state-store.parent-path,/rmstore), (yarn.rm.system-metrics-publisher.emit-container-events,false), (yarn.router.asc-interceptor-max-size,1MB), (yarn.router.clientrm.interceptor-class.pipeline,org.apache.hadoop.yarn.server.router.clientrm.DefaultClientRequestInterceptor), (yarn.router.deregister.subcluster.enabled,true), (yarn.router.interceptor.allow-partial-result.enable,false), (yarn.router.interceptor.user-thread-pool.allow-core-thread-time-out,false), (yarn.router.interceptor.user-thread-pool.keep-alive-time,30s), (yarn.router.interceptor.user-thread-pool.maximum-pool-size,5), (yarn.router.interceptor.user-thread-pool.minimum-pool-size,5), (yarn.router.interceptor.user.threadpool-size,5), (yarn.router.pipeline.cache-max-size,25), (yarn.router.rmadmin.interceptor-class.pipeline,org.apache.hadoop.yarn.server.router.rmadmin.DefaultRMAdminRequestInterceptor), (yarn.router.scheduled.executor.threads,1), (yarn.router.subcluster.cleaner.interval.time,60s), (yarn.router.subcluster.heartbeat.expiration.time,30m), (yarn.router.submit.interval.time,10ms), (yarn.router.webapp.address,0.0.0.0:8089), (yarn.router.webapp.appsinfo-cached-count,100), (yarn.router.webapp.appsinfo-enabled,false), (yarn.router.webapp.cross-origin.enabled,false), (yarn.router.webapp.https.address,0.0.0.0:8091), (yarn.router.webapp.interceptor-class.pipeline,org.apache.hadoop.yarn.server.router.webapp.DefaultRequestInterceptorREST), (yarn.router.webapp.proxy.enable,true), (yarn.scheduler.configuration.fs.path,file://\\${hadoop.tmp.dir}/yarn/system/schedconf), (yarn.scheduler.configuration.leveldb-store.compaction-interval-secs,86400), (yarn.scheduler.configuration.leveldb-store.path,\\${hadoop.tmp.dir}/yarn/system/confstore), (yarn.scheduler.configuration.max.version,100), (yarn.scheduler.configuration.mutation.acl-policy.class,org.apache.hadoop.yarn.server.resourcemanager.scheduler.DefaultConfigurationMutationACLPolicy), (yarn.scheduler.configuration.store.class,file), (yarn.scheduler.configuration.store.max-logs,1000), (yarn.scheduler.configuration.zk-store.parent-path,/confstore), (yarn.scheduler.include-port-in-node-name,false), (yarn.scheduler.maximum-allocation-mb,8192), (yarn.scheduler.maximum-allocation-vcores,4), (yarn.scheduler.minimum-allocation-mb,1024), (yarn.scheduler.minimum-allocation-vcores,1), (yarn.scheduler.queue-placement-rules,user-group), (yarn.scheduler.skip.node.multiplier,2), (yarn.sharedcache.admin.address,0.0.0.0:8047), (yarn.sharedcache.admin.thread-count,1), (yarn.sharedcache.app-checker.class,org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker), (yarn.sharedcache.checksum.algo.impl,org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl), (yarn.sharedcache.cleaner.initial-delay-mins,10), (yarn.sharedcache.cleaner.period-mins,1440), (yarn.sharedcache.cleaner.resource-sleep-ms,0), (yarn.sharedcache.client-server.address,0.0.0.0:8045), (yarn.sharedcache.client-server.thread-count,50), (yarn.sharedcache.enabled,false), (yarn.sharedcache.nested-level,3), (yarn.sharedcache.nm.uploader.replication.factor,10), (yarn.sharedcache.nm.uploader.thread-count,20), (yarn.sharedcache.root-dir,/sharedcache), (yarn.sharedcache.store.class,org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore), (yarn.sharedcache.store.in-memory.check-period-mins,720), (yarn.sharedcache.store.in-memory.initial-delay-mins,10), (yarn.sharedcache.store.in-memory.staleness-period-mins,10080), (yarn.sharedcache.uploader.server.address,0.0.0.0:8046), (yarn.sharedcache.uploader.server.thread-count,50), (yarn.sharedcache.webapp.address,0.0.0.0:8788), (yarn.system-metrics-publisher.enabled,false), (yarn.timeline-service.address,\\${yarn.timeline-service.hostname}:10200), (yarn.timeline-service.app-aggregation-interval-secs,15), (yarn.timeline-service.app-collector.linger-period.ms,60000), (yarn.timeline-service.client.best-effort,false), (yarn.timeline-service.client.drain-entities.timeout.ms,2000), (yarn.timeline-service.client.fd-clean-interval-secs,60), (yarn.timeline-service.client.fd-flush-interval-secs,10), (yarn.timeline-service.client.fd-retain-secs,300), (yarn.timeline-service.client.internal-timers-ttl-secs,420), (yarn.timeline-service.client.max-retries,30), (yarn.timeline-service.client.retry-interval-ms,1000), (yarn.timeline-service.enabled,false), (yarn.timeline-service.entity-group-fs-store.active-dir,/tmp/entity-file-history/active), (yarn.timeline-service.entity-group-fs-store.app-cache-size,10), (yarn.timeline-service.entity-group-fs-store.cache-store-class,org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore), (yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds,3600), (yarn.timeline-service.entity-group-fs-store.done-dir,/tmp/entity-file-history/done/), (yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size,10485760), (yarn.timeline-service.entity-group-fs-store.retain-seconds,604800), (yarn.timeline-service.entity-group-fs-store.scan-interval-seconds,60), (yarn.timeline-service.entity-group-fs-store.summary-store,org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore), (yarn.timeline-service.entity-group-fs-store.with-user-dir,false), (yarn.timeline-service.flowname.max-size,0), (yarn.timeline-service.generic-application-history.max-applications,10000), (yarn.timeline-service.handler-thread-count,10), (yarn.timeline-service.hbase-schema.prefix,prod.), (yarn.timeline-service.hbase.coprocessor.app-final-value-retention-milliseconds,259200000), (yarn.timeline-service.hbase.coprocessor.jar.hdfs.location,/hbase/coprocessor/hadoop-yarn-server-timelineservice.jar), (yarn.timeline-service.hostname,0.0.0.0), (yarn.timeline-service.http-authentication.simple.anonymous.allowed,true), (yarn.timeline-service.http-authentication.type,simple), (yarn.timeline-service.http-cross-origin.enabled,false), (yarn.timeline-service.keytab,/etc/krb5.keytab), (yarn.timeline-service.leveldb-state-store.path,\\${hadoop.tmp.dir}/yarn/timeline), (yarn.timeline-service.leveldb-timeline-store.path,\\${hadoop.tmp.dir}/yarn/timeline), (yarn.timeline-service.leveldb-timeline-store.read-cache-size,104857600), (yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size,10000), (yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size,10000), (yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms,300000), (yarn.timeline-service.reader.class,org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineReaderImpl), (yarn.timeline-service.reader.webapp.address,\\${yarn.timeline-service.webapp.address}), (yarn.timeline-service.reader.webapp.https.address,\\${yarn.timeline-service.webapp.https.address}), (yarn.timeline-service.recovery.enabled,false), (yarn.timeline-service.state-store-class,org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore), (yarn.timeline-service.store-class,org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore), (yarn.timeline-service.timeline-client.number-of-async-entities-to-merge,10), (yarn.timeline-service.ttl-enable,true), (yarn.timeline-service.ttl-ms,604800000), (yarn.timeline-service.version,1.0f), (yarn.timeline-service.webapp.address,\\${yarn.timeline-service.hostname}:8188), (yarn.timeline-service.webapp.https.address,\\${yarn.timeline-service.hostname}:8190), (yarn.timeline-service.webapp.rest-csrf.custom-header,X-XSRF-Header), (yarn.timeline-service.webapp.rest-csrf.enabled,false), (yarn.timeline-service.webapp.rest-csrf.methods-to-ignore,GET,OPTIONS,HEAD), (yarn.timeline-service.webapp.xfs-filter.xframe-options,SAMEORIGIN), (yarn.timeline-service.writer.async.queue.capacity,100), (yarn.timeline-service.writer.class,org.apache.hadoop.yarn.server.timelineservice.storage.HBaseTimelineWriterImpl), (yarn.timeline-service.writer.flush-interval-seconds,60), (yarn.webapp.api-service.enable,false), (yarn.webapp.enable-rest-app-submissions,true), (yarn.webapp.filter-entity-list-by-user,false), (yarn.webapp.filter-invalid-xml-chars,false), (yarn.webapp.ui1.tools.enable,true), (yarn.webapp.ui2.enable,false), (yarn.webapp.xfs-filter.enabled,true), (yarn.workflow-id.tag-prefix,workflowid:)), Metrics Properties -> Vector((*.sink.prometheusServlet.class,org.apache.spark.metrics.sink.PrometheusServlet), (*.sink.prometheusServlet.path,/metrics/prometheus), (*.sink.servlet.class,org.apache.spark.metrics.sink.MetricsServlet), (*.sink.servlet.path,/metrics/json), (applications.sink.prometheusServlet.path,/metrics/applications/prometheus), (applications.sink.servlet.path,/metrics/applications/json), (master.sink.prometheusServlet.path,/metrics/master/prometheus), (master.sink.servlet.path,/metrics/master/json)), System Properties -> Vector((SPARK_SUBMIT,true), (awt.toolkit,sun.awt.X11.XToolkit), (file.encoding,UTF-8), (file.encoding.pkg,sun.io), (file.separator,/), (java.awt.graphicsenv,sun.awt.X11GraphicsEnvironment), (java.awt.printerjob,sun.print.PSPrinterJob), (java.class.version,52.0), (java.endorsed.dirs,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/endorsed), (java.ext.dirs,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/java/packages/lib/ext), (java.home,/usr/lib/jvm/java-8-openjdk-amd64/jre), (java.io.tmpdir,/tmp), (java.library.path,/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib), (java.net.preferIPv4Stack,true), (java.runtime.name,OpenJDK Runtime Environment), (java.runtime.version,1.8.0_472-8u472-ga-1~22.04-b08), (java.specification.maintenance.version,6), (java.specification.name,Java Platform API Specification), (java.specification.vendor,Oracle Corporation), (java.specification.version,1.8), (java.vendor,Private Build), (java.vendor.url,http://java.oracle.com/), (java.vendor.url.bug,http://bugreport.sun.com/bugreport/), (java.version,1.8.0_472), (java.vm.info,mixed mode), (java.vm.name,OpenJDK 64-Bit Server VM), (java.vm.specification.name,Java Virtual Machine Specification), (java.vm.specification.vendor,Oracle Corporation), (java.vm.specification.version,1.8), (java.vm.vendor,Private Build), (java.vm.version,25.472-b08), (jdk.reflect.useDirectMethodHandle,false), (jetty.git.hash,cef3fbd6d736a21e7d541a5db490381d95a2047d), (kubernetes.request.retry.backoffLimit,3), (line.separator,\n",
       "), (os.arch,amd64), (os.name,Linux), (os.version,6.1.148-173.267.amzn2023.x86_64), (path.separator,:), (sun.arch.data.model,64), (sun.boot.class.path,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/resources.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/rt.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jsse.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jce.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/charsets.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/jfr.jar:/usr/lib/jvm/java-8-openjdk-amd64/jre/classes), (sun.boot.library.path,/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64), (sun.cpu.endian,little), (sun.cpu.isalist,), (sun.io.unicode.encoding,UnicodeLittle), (sun.java.command,*********(redacted)), (sun.java.launcher,SUN_STANDARD), (sun.jnu.encoding,UTF-8), (sun.management.compiler,HotSpot 64-Bit Tiered Compilers), (sun.os.patch.level,unknown), (user.country,US), (user.dir,/opt/kyuubi/work/default), (user.home,/root), (user.language,en), (user.name,root), (user.timezone,Etc/UTC)), JVM Information -> List((Java Home,/usr/lib/jvm/java-8-openjdk-amd64/jre), (Java Version,1.8.0_472 (Private Build)), (Scala Version,version 2.12.18)))) by listener InstallDriverPackageListener took 21.183168991s.\n",
       "26/02/03 23:41:18 INFO BlockManagerMasterEndpoint: Registering block manager 100.67.56.40:33389 with 9.0 GiB RAM, BlockManagerId(1, 100.67.56.40, 33389, None)\n",
       "26/02/03 23:41:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
       "26/02/03 23:41:19 INFO SharedState: Warehouse path is 's3a://kratos-spark-hive/datalake'.\n",
       "2026-02-03T23:41:20,048Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:20,048Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:20,049Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:20,049Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@3920ca49\n",
       "2026-02-03T23:41:20,049Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:20,049Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:20,050Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:41:20,050Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:20,050Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:20,050Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:20,051Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:45886, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:41:20,055Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a49070212, negotiated timeout = 120000\n",
       "2026-02-03T23:41:20,055Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:20,056Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:20,057Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:20,057Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:20,057Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:20,161Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a49070212\n",
       "2026-02-03T23:41:20,161Z INFO ZooKeeper: Session: 0x100f75a49070212 closed\n",
       "2026-02-03T23:41:20,165Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:20,420Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:21,546Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:21,547Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:21,547Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:21,548Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@5f7c9578\n",
       "2026-02-03T23:41:21,548Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:21,548Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:21,549Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:21,549Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:21,550Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-03T23:41:21,550Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:21,551Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:45200, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-03T23:41:21,555Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f3f2bf, negotiated timeout = 120000\n",
       "2026-02-03T23:41:21,555Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:21,556Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:21,556Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:21,557Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:21,557Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:21,662Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f3f2bf\n",
       "2026-02-03T23:41:21,662Z INFO ZooKeeper: Session: 0x200d89556f3f2bf closed\n",
       "2026-02-03T23:41:21,665Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:21,933Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "36.973: [GC (Allocation Failure) [PSYoungGen: 625152K->36833K(799744K)] 669467K->89789K(3888128K), 0.0770939 secs] [Times: user=0.07 sys=0.01, real=0.07 secs] \n",
       "2026-02-03T23:41:23,063Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:23,063Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:23,064Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:23,100Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@7eb073db\n",
       "2026-02-03T23:41:23,100Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:23,101Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:23,106Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:23,107Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:23,108Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:41:23,109Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:23,112Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:45902, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:41:23,120Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a49070235, negotiated timeout = 120000\n",
       "2026-02-03T23:41:23,120Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:23,125Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:23,127Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:23,127Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:23,127Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "26/02/03 23:41:22 WARN GpuOverrides: \n",
       "  @Expression <AttributeReference> namespace#0 could run on GPU\n",
       "\n",
       "26/02/03 23:41:22 INFO GpuOverrides: Plan conversion to the GPU took 73.81 ms\n",
       "26/02/03 23:41:22 INFO GpuOverrides: GPU plan transition optimization took 20.42 ms\n",
       "26/02/03 23:41:22 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.\n",
       "26/02/03 23:41:23 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is s3a://kratos-spark-hive/datalake\n",
       "2026-02-03T23:41:23,232Z INFO ZooKeeper: Session: 0x100f75a49070235 closed\n",
       "2026-02-03T23:41:23,232Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a49070235\n",
       "2026-02-03T23:41:23,237Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:23,519Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:23 INFO metastore: Trying to connect to metastore with URI thrift://nvspark-metastore.nvspark.svc.cluster.local:9083\n",
       "26/02/03 23:41:23 INFO metastore: Opened a connection to metastore, current connections: 1\n",
       "26/02/03 23:41:23 INFO metastore: Connected to metastore.\n",
       "26/02/03 23:41:23 INFO CodeGenerator: Code generated in 296.910558 ms\n",
       "26/02/03 23:41:24 WARN GpuOverrides: \n",
       "  ! <CommandResultExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.CommandResultExec\n",
       "    @Expression <AttributeReference> namespace#0 could run on GPU\n",
       "\n",
       "26/02/03 23:41:24 INFO GpuOverrides: Plan conversion to the GPU took 16.50 ms\n",
       "26/02/03 23:41:24 INFO GpuOverrides: GPU plan transition optimization took 21.51 ms\n",
       "26/02/03 23:41:24 INFO CodeGenerator: Code generated in 8.200348 ms\n",
       "2026-02-03T23:41:24,660Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:24,660Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:24,661Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:24,662Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4755004\n",
       "2026-02-03T23:41:24,662Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:24,662Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:24,663Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:41:24,663Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:24,663Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:24,663Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:24,664Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:45912, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:41:24,668Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a49070244, negotiated timeout = 120000\n",
       "2026-02-03T23:41:24,668Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:24,669Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:24,669Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:24,669Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:24,669Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:24,774Z INFO ZooKeeper: Session: 0x100f75a49070244 closed\n",
       "2026-02-03T23:41:24,774Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a49070244\n",
       "2026-02-03T23:41:24,777Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:25,042Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:24 INFO SparkContext: Starting job: isEmpty at KyuubiSparkUtil.scala:48\n",
       "26/02/03 23:41:24 INFO DAGScheduler: Got job 0 (isEmpty at KyuubiSparkUtil.scala:48) with 1 output partitions\n",
       "26/02/03 23:41:24 INFO DAGScheduler: Final stage: ResultStage 0 (isEmpty at KyuubiSparkUtil.scala:48)\n",
       "26/02/03 23:41:24 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/03 23:41:24 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/03 23:41:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[9] at isEmpty at KyuubiSparkUtil.scala:48), which has no missing parents\n",
       "39.462: [GC (Metadata GC Threshold) [PSYoungGen: 342781K->38384K(810496K)] 395737K->91411K(3898880K), 0.0853147 secs] [Times: user=0.09 sys=0.03, real=0.09 secs] \n",
       "39.547: [Full GC (Metadata GC Threshold) [PSYoungGen: 38384K->0K(810496K)] [ParOldGen: 53027K->72489K(3842560K)] 91411K->72489K(4653056K), [Metaspace: 150575K->150573K(1191936K)], 0.1872779 secs] [Times: user=0.32 sys=0.00, real=0.19 secs] \n",
       "26/02/03 23:41:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.1 KiB, free 8.4 GiB)\n",
       "26/02/03 23:41:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 8.4 GiB)\n",
       "26/02/03 23:41:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 100.67.56.160:7079 (size: 6.4 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:41:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/03 23:41:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at isEmpty at KyuubiSparkUtil.scala:48) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/03 23:41:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
       "26/02/03 23:41:25 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default\n",
       "2026-02-03T23:41:26,185Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:26,185Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:26,186Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:26,186Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@3bb079c0\n",
       "2026-02-03T23:41:26,186Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:26,186Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:26,187Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:26,187Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:26,187Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:26,187Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:26,189Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:54790, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:26,193Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f7c8, negotiated timeout = 120000\n",
       "2026-02-03T23:41:26,193Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:26,195Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:26,195Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:26,195Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:26,195Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:26,299Z INFO ZooKeeper: Session: 0x30263a585b4f7c8 closed\n",
       "2026-02-03T23:41:26,299Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f7c8\n",
       "2026-02-03T23:41:26,304Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:26,569Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:27,736Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:27,736Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:27,738Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:27,738Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@256515f5\n",
       "2026-02-03T23:41:27,738Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:27,738Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:27,739Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:27,739Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:27,739Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:27,739Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:27,741Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:39074, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:27,745Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f7d8, negotiated timeout = 120000\n",
       "2026-02-03T23:41:27,746Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:27,747Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:27,747Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:27,748Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:27,748Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:27,852Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f7d8\n",
       "2026-02-03T23:41:27,852Z INFO ZooKeeper: Session: 0x30263a585b4f7d8 closed\n",
       "2026-02-03T23:41:27,856Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:28,123Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (100.67.56.40, executor 1, partition 0, PROCESS_LOCAL, 33538 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/03 23:41:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 100.67.56.40:33389 (size: 6.4 KiB, free: 9.0 GiB)\n",
       "2026-02-03T23:41:29,251Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:29,251Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:29,252Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:29,252Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4cf52273\n",
       "2026-02-03T23:41:29,253Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:29,253Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:29,253Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:29,253Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-03T23:41:29,253Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:29,253Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:29,254Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:59694, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-03T23:41:29,259Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f3f324, negotiated timeout = 120000\n",
       "2026-02-03T23:41:29,259Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:29,260Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:29,260Z ERROR ZookeeperDiscoveryClient: Failed to get service node info message: KeeperErrorCode = NoNode for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:29,260Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:29,260Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:29,366Z INFO ZooKeeper: Session: 0x200d89556f3f324 closed\n",
       "2026-02-03T23:41:29,366Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f3f324\n",
       "2026-02-03T23:41:29,369Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:29,703Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/sessions/cfc8b837-b0f7-40ee-b388-391d8316c4dc\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1331 ms on 100.67.56.40 (executor 1) (1/1)\n",
       "26/02/03 23:41:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool default\n",
       "26/02/03 23:41:29 INFO DAGScheduler: ResultStage 0 (isEmpty at KyuubiSparkUtil.scala:48) finished in 4.846 s\n",
       "26/02/03 23:41:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/03 23:41:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
       "26/02/03 23:41:29 INFO DAGScheduler: Job 0 finished: isEmpty at KyuubiSparkUtil.scala:48, took 4.949453 s\n",
       "26/02/03 23:41:29 INFO Utils: Loading Kyuubi properties from /opt/kyuubi/conf/kyuubi-defaults.conf\n",
       "26/02/03 23:41:29 INFO ThreadUtils: SparkSQLSessionManager-exec-pool: pool size: 600, wait queue size: 1000, thread keepalive time: 60000 ms\n",
       "26/02/03 23:41:29 INFO SparkSQLOperationManager: Service[SparkSQLOperationManager] is initialized.\n",
       "26/02/03 23:41:29 INFO SparkSQLSessionManager: Service[SparkSQLSessionManager] is initialized.\n",
       "26/02/03 23:41:29 INFO SparkSQLBackendService: Service[SparkSQLBackendService] is initialized.\n",
       "26/02/03 23:41:29 INFO SparkTBinaryFrontendService: Initializing SparkTBinaryFrontend on cluster-20260203202803-yawkv5ak-driver:37281 with [9, 999] worker threads\n",
       "26/02/03 23:41:29 INFO CuratorFrameworkImpl: Starting\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:zookeeper.version=3.6.4--d65253dcf68e9097c6e95a126463fd5fdeb4521c, built on 12/18/2022 18:10 GMT\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:host.name=cluster-20260203202803-yawkv5ak-driver\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:java.version=1.8.0_472\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:java.vendor=Private Build\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:java.class.path=/opt/spark/conf/:/opt/spark/jars/HikariCP-2.5.1.jar:/opt/spark/jars/JLargeArrays-1.5.jar:/opt/spark/jars/JTransforms-3.1.jar:/opt/spark/jars/RoaringBitmap-0.9.45.jar:/opt/spark/jars/ST4-4.0.4.jar:/opt/spark/jars/activation-1.1.1.jar:/opt/spark/jars/aircompressor-0.27.jar:/opt/spark/jars/algebra_2.12-2.0.1.jar:/opt/spark/jars/annotations-17.0.0.jar:/opt/spark/jars/antlr-runtime-3.5.2.jar:/opt/spark/jars/antlr4-runtime-4.9.3.jar:/opt/spark/jars/aopalliance-repackaged-2.6.1.jar:/opt/spark/jars/arpack-3.0.3.jar:/opt/spark/jars/arpack_combined_all-0.1.jar:/opt/spark/jars/arrow-format-12.0.1.jar:/opt/spark/jars/arrow-memory-core-12.0.1.jar:/opt/spark/jars/arrow-memory-netty-12.0.1.jar:/opt/spark/jars/arrow-vector-12.0.1.jar:/opt/spark/jars/audience-annotations-0.5.0.jar:/opt/spark/jars/avro-1.11.2.jar:/opt/spark/jars/avro-ipc-1.11.2.jar:/opt/spark/jars/avro-mapred-1.11.2.jar:/opt/spark/jars/blas-3.0.3.jar:/opt/spark/jars/bonecp-0.8.0.RELEASE.jar:/opt/spark/jars/breeze-macros_2.12-2.1.0.jar:/opt/spark/jars/breeze_2.12-2.1.0.jar:/opt/spark/jars/cats-kernel_2.12-2.1.1.jar:/opt/spark/jars/chill-java-0.10.0.jar:/opt/spark/jars/chill_2.12-0.10.0.jar:/opt/spark/jars/commons-cli-1.5.0.jar:/opt/spark/jars/commons-codec-1.16.1.jar:/opt/spark/jars/commons-collections-3.2.2.jar:/opt/spark/jars/commons-collections4-4.4.jar:/opt/spark/jars/commons-compiler-3.1.9.jar:/opt/spark/jars/commons-compress-1.23.0.jar:/opt/spark/jars/commons-crypto-1.1.0.jar:/opt/spark/jars/commons-dbcp-1.4.jar:/opt/spark/jars/commons-io-2.16.1.jar:/opt/spark/jars/commons-lang-2.6.jar:/opt/spark/jars/commons-lang3-3.12.0.jar:/opt/spark/jars/commons-logging-1.1.3.jar:/opt/spark/jars/commons-math3-3.6.1.jar:/opt/spark/jars/commons-pool-1.5.4.jar:/opt/spark/jars/commons-text-1.10.0.jar:/opt/spark/jars/compress-lzf-1.1.2.jar:/opt/spark/jars/datanucleus-api-jdo-4.2.4.jar:/opt/spark/jars/datanucleus-core-4.1.17.jar:/opt/spark/jars/datanucleus-rdbms-4.1.19.jar:/opt/spark/jars/datasketches-java-3.3.0.jar:/opt/spark/jars/datasketches-memory-2.1.0.jar:/opt/spark/jars/derby-10.14.2.0.jar:/opt/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/opt/spark/jars/flatbuffers-java-1.12.0.jar:/opt/spark/jars/gson-2.2.4.jar:/opt/spark/jars/guava-14.0.1.jar:/opt/spark/jars/hive-beeline-2.3.9.jar:/opt/spark/jars/hive-cli-2.3.9.jar:/opt/spark/jars/hive-common-2.3.9.jar:/opt/spark/jars/hive-exec-2.3.9-core.jar:/opt/spark/jars/hive-jdbc-2.3.9.jar:/opt/spark/jars/hive-llap-common-2.3.9.jar:/opt/spark/jars/hive-metastore-2.3.9.jar:/opt/spark/jars/hive-serde-2.3.9.jar:/opt/spark/jars/hive-service-rpc-3.1.3.jar:/opt/spark/jars/hive-shims-0.23-2.3.9.jar:/opt/spark/jars/hive-shims-2.3.9.jar:/opt/spark/jars/hive-shims-common-2.3.9.jar:/opt/spark/jars/hive-shims-scheduler-2.3.9.jar:/opt/spark/jars/hive-storage-api-2.8.1.jar:/opt/spark/jars/hk2-api-2.6.1.jar:/opt/spark/jars/hk2-locator-2.6.1.jar:/opt/spark/jars/hk2-utils-2.6.1.jar:/opt/spark/jars/httpclient-4.5.14.jar:/opt/spark/jars/httpcore-4.4.16.jar:/opt/spark/jars/istack-commons-runtime-3.0.8.jar:/opt/spark/jars/ivy-2.5.1.jar:/opt/spark/jars/jackson-annotations-2.15.2.jar:/opt/spark/jars/jackson-core-2.15.2.jar:/opt/spark/jars/jackson-core-asl-1.9.13.jar:/opt/spark/jars/jackson-databind-2.15.2.jar:/opt/spark/jars/jackson-dataformat-yaml-2.15.2.jar:/opt/spark/jars/jackson-datatype-jsr310-2.15.2.jar:/opt/spark/jars/jackson-mapper-asl-1.9.13.jar:/opt/spark/jars/jackson-module-scala_2.12-2.15.2.jar:/opt/spark/jars/jakarta.annotation-api-1.3.5.jar:/opt/spark/jars/jakarta.inject-2.6.1.jar:/opt/spark/jars/jakarta.servlet-api-4.0.3.jar:/opt/spark/jars/jakarta.validation-api-2.0.2.jar:/opt/spark/jars/jakarta.ws.rs-api-2.1.6.jar:/opt/spark/jars/jakarta.xml.bind-api-2.3.2.jar:/opt/spark/jars/janino-3.1.9.jar:/opt/spark/jars/javassist-3.29.2-GA.jar:/opt/spark/jars/jpam-1.1.jar:/opt/spark/jars/javax.jdo-3.2.0-m3.jar:/opt/spark/jars/javolution-5.5.1.jar:/opt/spark/jars/jaxb-runtime-2.3.2.jar:/opt/spark/jars/jcl-over-slf4j-2.0.7.jar:/opt/spark/jars/jdo-api-3.0.1.jar:/opt/spark/jars/jersey-client-2.40.jar:/opt/spark/jars/jersey-common-2.40.jar:/opt/spark/jars/jersey-container-servlet-2.40.jar:/opt/spark/jars/jersey-container-servlet-core-2.40.jar:/opt/spark/jars/jersey-hk2-2.40.jar:/opt/spark/jars/jersey-server-2.40.jar:/opt/spark/jars/jline-2.14.6.jar:/opt/spark/jars/joda-time-2.12.5.jar:/opt/spark/jars/jodd-core-3.5.2.jar:/opt/spark/jars/json-1.8.jar:/opt/spark/jars/json4s-ast_2.12-3.7.0-M11.jar:/opt/spark/jars/json4s-core_2.12-3.7.0-M11.jar:/opt/spark/jars/json4s-jackson_2.12-3.7.0-M11.jar:/opt/spark/jars/json4s-scalap_2.12-3.7.0-M11.jar:/opt/spark/jars/jsr305-3.0.0.jar:/opt/spark/jars/jta-1.1.jar:/opt/spark/jars/jul-to-slf4j-2.0.7.jar:/opt/spark/jars/kryo-shaded-4.0.2.jar:/opt/spark/jars/kubernetes-client-6.7.2.jar:/opt/spark/jars/kubernetes-client-api-6.7.2.jar:/opt/spark/jars/kubernetes-httpclient-okhttp-6.7.2.jar:/opt/spark/jars/kubernetes-model-admissionregistration-6.7.2.jar:/opt/spark/jars/kubernetes-model-apiextensions-6.7.2.jar:/opt/spark/jars/kubernetes-model-apps-6.7.2.jar:/opt/spark/jars/kubernetes-model-autoscaling-6.7.2.jar:/opt/spark/jars/kubernetes-model-batch-6.7.2.jar:/opt/spark/jars/kubernetes-model-certificates-6.7.2.jar:/opt/spark/jars/kubernetes-model-common-6.7.2.jar:/opt/spark/jars/kubernetes-model-coordination-6.7.2.jar:/opt/spark/jars/kubernetes-model-core-6.7.2.jar:/opt/spark/jars/kubernetes-model-discovery-6.7.2.jar:/opt/spark/jars/kubernetes-model-events-6.7.2.jar:/opt/spark/jars/kubernetes-model-extensions-6.7.2.jar:/opt/spark/jars/kubernetes-model-flowcontrol-6.7.2.jar:/opt/spark/jars/kubernetes-model-gatewayapi-6.7.2.jar:/opt/spark/jars/kubernetes-model-metrics-6.7.2.jar:/opt/spark/jars/kubernetes-model-networking-6.7.2.jar:/opt/spark/jars/kubernetes-model-node-6.7.2.jar:/opt/spark/jars/kubernetes-model-policy-6.7.2.jar:/opt/spark/jars/kubernetes-model-rbac-6.7.2.jar:/opt/spark/jars/kubernetes-model-resource-6.7.2.jar:/opt/spark/jars/kubernetes-model-scheduling-6.7.2.jar:/opt/spark/jars/kubernetes-model-storageclass-6.7.2.jar:/opt/spark/jars/lapack-3.0.3.jar:/opt/spark/jars/leveldbjni-all-1.8.jar:/opt/spark/jars/libfb303-0.9.3.jar:/opt/spark/jars/libthrift-0.12.0.jar:/opt/spark/jars/log4j-1.2-api-2.20.0.jar:/opt/spark/jars/log4j-api-2.20.0.jar:/opt/spark/jars/log4j-core-2.20.0.jar:/opt/spark/jars/log4j-slf4j2-impl-2.20.0.jar:/opt/spark/jars/logging-interceptor-3.12.12.jar:/opt/spark/jars/lz4-java-1.8.0.jar:/opt/spark/jars/mesos-1.4.3-shaded-protobuf.jar:/opt/spark/jars/metrics-core-4.2.19.jar:/opt/spark/jars/metrics-graphite-4.2.19.jar:/opt/spark/jars/metrics-jmx-4.2.19.jar:/opt/spark/jars/metrics-json-4.2.19.jar:/opt/spark/jars/metrics-jvm-4.2.19.jar:/opt/spark/jars/minlog-1.3.0.jar:/opt/spark/jars/netty-all-4.1.96.Final.jar:/opt/spark/jars/netty-buffer-4.1.96.Final.jar:/opt/spark/jars/netty-codec-4.1.96.Final.jar:/opt/spark/jars/netty-codec-http-4.1.96.Final.jar:/opt/spark/jars/netty-codec-http2-4.1.96.Final.jar:/opt/spark/jars/netty-codec-socks-4.1.96.Final.jar:/opt/spark/jars/netty-common-4.1.96.Final.jar:/opt/spark/jars/netty-handler-4.1.96.Final.jar:/opt/spark/jars/netty-handler-proxy-4.1.96.Final.jar:/opt/spark/jars/netty-resolver-4.1.96.Final.jar:/opt/spark/jars/netty-transport-4.1.96.Final.jar:/opt/spark/jars/netty-transport-classes-epoll-4.1.96.Final.jar:/opt/spark/jars/netty-transport-classes-kqueue-4.1.96.Final.jar:/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar:/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar:/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar:/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar:/opt/spark/jars/netty-transport-native-unix-common-4.1.96.Final.jar:/opt/spark/jars/objenesis-3.3.jar:/opt/spark/jars/okhttp-3.12.12.jar:/opt/spark/jars/okio-1.17.6.jar:/opt/spark/jars/opencsv-2.3.jar:/opt/spark/jars/orc-core-1.9.4-shaded-protobuf.jar:/opt/spark/jars/orc-shims-1.9.4.jar:/opt/spark/jars/orc-mapreduce-1.9.4-shaded-protobuf.jar:/opt/spark/jars/oro-2.0.8.jar:/opt/spark/jars/osgi-resource-locator-1.0.3.jar:/opt/spark/jars/paranamer-2.8.jar:/opt/spark/jars/parquet-column-1.13.1.jar:/opt/spark/jars/parquet-common-1.13.1.jar:/opt/spark/jars/parquet-encoding-1.13.1.jar:/opt/spark/jars/parquet-format-structures-1.13.1.jar:/opt/spark/jars/parquet-hadoop-1.13.1.jar:/opt/spark/jars/parquet-jackson-1.13.1.jar:/opt/spark/jars/pickle-1.3.jar:/opt/spark/jars/py4j-0.10.9.7.jar:/opt/spark/jars/rocksdbjni-8.3.2.jar:/opt/spark/jars/scala-collection-compat_2.12-2.7.0.jar:/opt/spark/jars/scala-compiler-2.12.18.jar:/opt/spark/jars/scala-library-2.12.18.jar:/opt/spark/jars/scala-parser-combinators_2.12-2.3.0.jar:/opt/spark/jars/scala-reflect-2.12.18.jar:/opt/spark/jars/scala-xml_2.12-2.1.0.jar:/opt/spark/jars/shims-0.9.45.jar:/opt/spark/jars/slf4j-api-2.0.7.jar:/opt/spark/jars/snakeyaml-2.0.jar:/opt/spark/jars/snakeyaml-engine-2.6.jar:/opt/spark/jars/snappy-java-1.1.10.5.jar:/opt/spark/jars/spark-catalyst_2.12-3.5.3.jar:/opt/spark/jars/spark-common-utils_2.12-3.5.3.jar:/opt/spark/jars/spark-core_2.12-3.5.3.jar:/opt/spark/jars/spark-graphx_2.12-3.5.3.jar:/opt/spark/jars/spark-hive-thriftserver_2.12-3.5.3.jar:/opt/spark/jars/spark-hive_2.12-3.5.3.jar:/opt/spark/jars/spark-kubernetes_2.12-3.5.3.jar:/opt/spark/jars/spark-kvstore_2.12-3.5.3.jar:/opt/spark/jars/spark-launcher_2.12-3.5.3.jar:/opt/spark/jars/spark-mesos_2.12-3.5.3.jar:/opt/spark/jars/spark-mllib-local_2.12-3.5.3.jar:/opt/spark/jars/spark-mllib_2.12-3.5.3.jar:/opt/spark/jars/spark-network-common_2.12-3.5.3.jar:/opt/spark/jars/spark-network-shuffle_2.12-3.5.3.jar:/opt/spark/jars/spark-repl_2.12-3.5.3.jar:/opt/spark/jars/spark-sketch_2.12-3.5.3.jar:/opt/spark/jars/spark-sql-api_2.12-3.5.3.jar:/opt/spark/jars/spark-sql_2.12-3.5.3.jar:/opt/spark/jars/spark-streaming_2.12-3.5.3.jar:/opt/spark/jars/spark-tags_2.12-3.5.3.jar:/opt/spark/jars/spark-unsafe_2.12-3.5.3.jar:/opt/spark/jars/spark-yarn_2.12-3.5.3.jar:/opt/spark/jars/spire-macros_2.12-0.17.0.jar:/opt/spark/jars/spire-platform_2.12-0.17.0.jar:/opt/spark/jars/spire-util_2.12-0.17.0.jar:/opt/spark/jars/spire_2.12-0.17.0.jar:/opt/spark/jars/stax-api-1.0.1.jar:/opt/spark/jars/stream-2.9.6.jar:/opt/spark/jars/super-csv-2.2.0.jar:/opt/spark/jars/threeten-extra-1.7.1.jar:/opt/spark/jars/tink-1.9.0.jar:/opt/spark/jars/transaction-api-1.1.jar:/opt/spark/jars/univocity-parsers-2.9.1.jar:/opt/spark/jars/xbean-asm9-shaded-4.23.jar:/opt/spark/jars/xz-1.9.jar:/opt/spark/jars/zjsonpatch-0.3.0.jar:/opt/spark/jars/zookeeper-3.6.3.jar:/opt/spark/jars/zookeeper-jute-3.6.3.jar:/opt/spark/jars/zstd-jni-1.5.5-4.jar:/opt/spark/jars/commons-pool2-2.11.1.jar:/opt/spark/jars/kafka-clients-3.4.1.jar:/opt/spark/jars/slf4j-log4j12-1.7.16.jar:/opt/spark/jars/spark-avro_2.12-3.5.3.jar:/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.3.jar:/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.3.jar:/opt/spark/jars/kratos-spark-sql-functions_2.12-0.0.4-3.5.3-java17-SNAPSHOT.jar:/opt/spark/jars/app-analytics-listener_3.5.3-0.1.4.jar:/opt/spark/jars/hadoop-client-runtime-3.4.1.jar:/opt/spark/jars/hadoop-client-api-3.4.1.jar:/opt/spark/jars/aws-java-sdk-bundle-1.12.780.jar:/opt/spark/jars/bundle-2.24.6.jar:/opt/spark/jars/curator-client-5.7.1.jar:/opt/spark/jars/curator-framework-5.7.1.jar:/opt/spark/jars/curator-recipes-5.7.1.jar:/opt/spark/jars/delta-spark_2.12-3.3.2-25-12-10.jar:/opt/spark/jars/delta-storage-3.3.2-25-12-10.jar:/opt/spark/jars/delta-storage-s3-dynamodb-3.3.2-25-12-10.jar:/opt/spark/jars/dummy-shs-filter-25.08.0.jar:/opt/spark/jars/gethostname4j-1.0.0.jar:/opt/spark/jars/hadoop-aws-3.4.1.jar:/opt/spark/jars/hadoop-shaded-guava-1.3.0.jar:/opt/spark/jars/jackson-jaxrs-1.9.9.jar:/opt/spark/jars/jersey-client-1.19.4.jar:/opt/spark/jars/jersey-core-1.9.1.jar:/opt/spark/jars/jna-5.13.0.jar:/opt/spark/jars/jna-platform-5.13.0.jar:/opt/spark/jars/json-file-credentials-provider-25.08.0.jar:/opt/spark/jars/kyuubi-spark-authz_2.12-1.7.0.jar:/opt/spark/jars/mysql-connector-j-8.0.33.jar:/opt/spark/jars/nvsparkaas-k8s-plugin-3.5.3-25.08.5.jar:/opt/spark/jars/postgresql-42.6.0.jar:/opt/spark/jars/ranger-plugins-audit-2.3.0.jar:/opt/spark/jars/ranger-plugins-common-2.3.0.jar:/opt/spark/jars/rapids-4-spark_2.12-25.10.1-cuda12.jar:/opt/spark/jars/shs-filter-25.08.0.jar:/opt/spark/jars/slf4j-api-1.7.36.jar:/opt/spark/jars/slf4j-reload4j-1.7.36.jar:/opt/spark/jars/spark-hadoop-cloud_2.12-3.5.3.jar:/opt/spark/jars/static-config-group-provider-25.08.0-jar-with-dependencies.jar:/opt/spark/jars/static-config-group-provider-25.08.0.jar:/opt/spark/jars/test-credentials-provider-25.08.0-jar-with-dependencies.jar\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:java.library.path=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:java.compiler=<NA>\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:os.name=Linux\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:os.arch=amd64\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:os.version=6.1.148-173.267.amzn2023.x86_64\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:user.name=root\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:user.home=/root\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:user.dir=/opt/kyuubi/work/default\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:os.memory.free=4327MB\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:os.memory.max=14564MB\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Client environment:os.memory.total=4544MB\n",
       "26/02/03 23:41:29 INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4c04188\n",
       "26/02/03 23:41:29 INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n",
       "26/02/03 23:41:29 INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "26/02/03 23:41:29 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "26/02/03 23:41:29 INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "26/02/03 23:41:29 INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "26/02/03 23:41:29 INFO CuratorFrameworkImpl: Default schema\n",
       "26/02/03 23:41:29 INFO EngineServiceDiscovery: Service[EngineServiceDiscovery] is initialized.\n",
       "26/02/03 23:41:29 INFO SparkTBinaryFrontendService: Service[SparkTBinaryFrontend] is initialized.\n",
       "26/02/03 23:41:29 INFO SparkSQLEngine: Service[SparkSQLEngine] is initialized.\n",
       "26/02/03 23:41:29 INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:39084, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "26/02/03 23:41:29 INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f7f0, negotiated timeout = 120000\n",
       "26/02/03 23:41:29 INFO SparkSQLOperationManager: Service[SparkSQLOperationManager] is started.\n",
       "26/02/03 23:41:29 INFO SparkSQLSessionManager: Service[SparkSQLSessionManager] is started.\n",
       "26/02/03 23:41:29 INFO SparkSQLBackendService: Service[SparkSQLBackendService] is started.\n",
       "26/02/03 23:41:29 INFO ConnectionStateManager: State change: CONNECTED\n",
       "26/02/03 23:41:29 INFO ZookeeperDiscoveryClient: Zookeeper client connection state changed to: CONNECTED\n",
       "26/02/03 23:41:29 INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "26/02/03 23:41:29 INFO Compatibility: Using org.apache.zookeeper.server.quorum.MultipleAddresses\n",
       "26/02/03 23:41:29 INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "26/02/03 23:41:29 INFO ZookeeperDiscoveryClient: Created a /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak/serviceUri=100.67.56.160:37281;version=1.8.0.5-SNAPSHOT;spark.driver.memory=16g;spark.executor.memory=16g;kyuubi.engine.id=spark-cdd9e5d0115345edb431647f51f82517;kyuubi.engine.url=100.67.56.160:4040;refId=cfc8b837-b0f7-40ee-b388-391d8316c4dc;sequence=0000000000 on ZooKeeper for KyuubiServer uri: 100.67.56.160:37281\n",
       "26/02/03 23:41:29 INFO EngineServiceDiscovery: Registered EngineServiceDiscovery in namespace /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak.\n",
       "26/02/03 23:41:29 INFO EngineServiceDiscovery: Service[EngineServiceDiscovery] is started.\n",
       "26/02/03 23:41:29 INFO SparkTBinaryFrontendService: Service[SparkTBinaryFrontend] is started.\n",
       "26/02/03 23:41:29 INFO SparkSQLEngine: Service[SparkSQLEngine] is started.\n",
       "26/02/03 23:41:29 INFO SparkSQLEngine: \n",
       "    Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "          application ID:  spark-cdd9e5d0115345edb431647f51f82517\n",
       "          application tags: \n",
       "          application web UI: http://100.67.56.160:4040\n",
       "          master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "          version: 3.5.3\n",
       "          driver: [cpu: 2, mem: 16g]\n",
       "          executor: [cpu: 4, mem: 16g, maxNum: 4]\n",
       "    Start time: Tue Feb 03 23:40:48 UTC 2026\n",
       "    \n",
       "    User: spring (shared mode: GROUP)\n",
       "    State: STARTED\n",
       "    \n",
       "2026-02-03T23:41:30,711Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:30,845Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:30,845Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:30,846Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:30,847Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4f18644c\n",
       "2026-02-03T23:41:30,847Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:30,847Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:30,847Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:30,847Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:30,847Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-03T23:41:30,847Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:30,848Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:60946, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-03T23:41:30,852Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a49070293, negotiated timeout = 120000\n",
       "2026-02-03T23:41:30,853Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:30,853Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:30,854Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:30,854Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:30,854Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:30,960Z INFO ZooKeeper: Session: 0x100f75a49070293 closed\n",
       "2026-02-03T23:41:31,000Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a49070293\n",
       "2026-02-03T23:41:31,003Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:30 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/03 23:41:30 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/03 23:41:30 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.request.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.shuffleTracking.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.warehouse.dir.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.submission.waitAppCompletion\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.extraJavaOptions\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.host\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.sustainedSchedulerBacklogTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.shuffleTracking.timeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.namespace\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.podTemplateFile\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.shuffle.manager\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memoryOverhead\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.reusePersistentVolumeClaim\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.jars\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.blockManager.port\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.cachedExecutorIdleTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.ownPersistentVolumeClaim\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.cache.serializer.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.request.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.schedulerBacklogTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.submitInDriver\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.scheduler.mode\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.locality.wait\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.memory\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.submit.pyFiles\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.memoryOverheadFactor\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.rolling.maxFileSize\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.container.image.pullPolicy\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.resource.type\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.maxExecutors\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.container.image\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.podTemplateFile\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.limit.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.extraJavaOptions\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.limit.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.submit.deployMode\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.ui.prometheus.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.authenticate.driver.serviceAccountName\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.podNamePrefix\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.authenticate\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.acls.enable\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.ui.view.acls.groups\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memory\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.dir\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.plugins\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.task.cpus\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.processTreeMetrics.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.minExecutors\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.ui.proxyRedirectUri\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.initialExecutors\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.file.upload.path\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.streaming.streamingQueryListeners.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.extensions.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.redaction.regex\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.executorIdleTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.pod.featureSteps\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.rolling.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:31 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.pod.name\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "2026-02-03T23:41:31,568Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.216.117\n",
       "2026-02-03T23:41:31,568Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-03T23:41:31,569Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-03T23:41:31,571Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/22470745-2fb5-49b4-bfa1-49ddd7a7f947\n",
       "2026-02-03T23:41:31,571Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]/kernel-v38feb1b0b9f19ee2a784f50d0f9f8f896d87d022f is opened, current opening sessions 2\n",
       "2026-02-03T23:41:31,571Z INFO LaunchEngine: Processing anonymous's query[22470745-2fb5-49b4-bfa1-49ddd7a7f947]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:41:31,572Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-03T23:41:31,572Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@6c1a1d0c\n",
       "2026-02-03T23:41:31,572Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:31,573Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-03T23:41:31,573Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-03T23:41:31,573Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-03T23:41:31,573Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-03T23:41:31,574Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-03T23:41:31,576Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:39086, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-03T23:41:31,579Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b4f810, negotiated timeout = 120000\n",
       "2026-02-03T23:41:31,580Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-03T23:41:31,601Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:31,602Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-03T23:41:31,603Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-03T23:41:31,767Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260203234024-mmysc0ll]: associated with Kyuubi SessionHandle: [f7df5191-9229-40df-86fa-ee5b5502e2f5]\n",
       "2026-02-03T23:41:31,838Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/b7ac13a9-396d-4f34-abf5-fa0f8cf43149\n",
       "2026-02-03T23:41:31,838Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5] - Starting to wait the launch engine operation finished\n",
       "26/02/03 23:41:31 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/03 23:41:31 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/03 23:41:31 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]\n",
       "2026-02-03T23:41:32,457Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]]\n",
       "2026-02-03T23:41:32,458Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:32,463Z INFO KyuubiSessionImpl: [anonymous:100.67.56.160] SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]]\n",
       "2026-02-03T23:41:32,463Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-03T23:41:32,563Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b4f810\n",
       "2026-02-03T23:41:32,563Z INFO ZooKeeper: Session: 0x30263a585b4f810 closed\n",
       "2026-02-03T23:41:32,564Z INFO LaunchEngine: Processing anonymous's query[22470745-2fb5-49b4-bfa1-49ddd7a7f947]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.991 seconds\n",
       "2026-02-03T23:41:32,564Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-03T23:41:32,604Z INFO ZooKeeper: Session: 0x100f75a49070041 closed\n",
       "2026-02-03T23:41:32,604Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a49070041\n",
       "2026-02-03T23:41:32,605Z INFO LaunchEngine: Processing anonymous's query[56d8d163-f51b-4169-a367-b4eb66ed7e92]: RUNNING_STATE -> FINISHED_STATE, time taken: 48.466 seconds\n",
       "2026-02-03T23:41:32,605Z INFO KyuubiSessionImpl: [anonymous:100.67.56.160] SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc] - Engine has been launched, elapsed time: 40 s\n",
       "2026-02-03T23:41:32,644Z INFO ExecuteStatement: Processing anonymous's query[b7ac13a9-396d-4f34-abf5-fa0f8cf43149]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:41:32,701Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260203234024-mmysc0ll\n",
       "2026-02-03T23:41:32,808Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/b7ac13a9-396d-4f34-abf5-fa0f8cf43149/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:32 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]/hongy-default-20260203202802-startup is opened, current opening sessions 2\n",
       "26/02/03 23:41:32 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]/kernel-v38feb1b0b9f19ee2a784f50d0f9f8f896d87d022f is opened, current opening sessions 2\n",
       "26/02/03 23:41:32 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/03 23:41:32 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/03 23:41:32 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/03 23:41:32 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/03 23:41:32 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [842f5dda-d065-4528-b30f-336a8fc57fab]\n",
       "26/02/03 23:41:32 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [5fe59e9d-06b9-4a4d-9106-05d917d255d8]\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.request.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.shuffleTracking.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.warehouse.dir.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.submission.waitAppCompletion\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.extraJavaOptions\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.host\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.sustainedSchedulerBacklogTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.shuffleTracking.timeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [5fe59e9d-06b9-4a4d-9106-05d917d255d8]/f7df5191-9229-40df-86fa-ee5b5502e2f5_aliveness_probe is opened, current opening sessions 3\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.namespace\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.podTemplateFile\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.shuffle.manager\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memoryOverhead\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.reusePersistentVolumeClaim\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.jars\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.blockManager.port\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.cachedExecutorIdleTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.ownPersistentVolumeClaim\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.cache.serializer.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.request.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.schedulerBacklogTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.submitInDriver\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.scheduler.mode\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.locality.wait\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.memory\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.submit.pyFiles\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.memoryOverheadFactor\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.rolling.maxFileSize\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.container.image.pullPolicy\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.resource.type\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.maxExecutors\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.container.image\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.podTemplateFile\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.limit.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.driver.extraJavaOptions\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.limit.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.submit.deployMode\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.ui.prometheus.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.authenticate.driver.serviceAccountName\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.podNamePrefix\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.authenticate\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.acls.enable\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.ui.view.acls.groups\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.memory\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.dir\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.cores\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.plugins\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.task.cpus\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.executor.processTreeMetrics.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.minExecutors\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.ui.proxyRedirectUri\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.initialExecutors\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.file.upload.path\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.streaming.streamingQueryListeners.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: Cannot modify the value of a static config: spark.sql.extensions.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.redaction.regex\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.dynamicAllocation.executorIdleTimeout\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.executor.pod.featureSteps\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.eventLog.rolling.enabled\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 WARN SparkSessionImpl: [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.kubernetes.driver.pod.name\".\n",
       "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'.\n",
       "26/02/03 23:41:32 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [842f5dda-d065-4528-b30f-336a8fc57fab]/cfc8b837-b0f7-40ee-b388-391d8316c4dc_aliveness_probe is opened, current opening sessions 4\n",
       "26/02/03 23:41:32 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260203234024-mmysc0ll)]: associated with spark-sql operation session: [f7df5191-9229-40df-86fa-ee5b5502e2f5]\n",
       "26/02/03 23:41:32 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-bcac7475-d06c-4921-8097-4d15a6ffeeed/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:b:c:a:c:7:4:7:5:-:d:0:6:c:-:4:9:2:1:-:8:0:9:7:-:4:d:1:5:a:6:f:f:e:e:e:d\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-bcac7475-d06c-4921-8097-4d15a6ffeeed/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=f7df5191-9229-40df-86fa-ee5b5502e2f5\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/03 23:41:32 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/b7ac13a9-396d-4f34-abf5-fa0f8cf43149\n",
       "2026-02-03T23:41:33,472Z INFO ExecuteStatement: Query[b7ac13a9-396d-4f34-abf5-fa0f8cf43149] in FINISHED_STATE\n",
       "2026-02-03T23:41:33,472Z INFO ExecuteStatement: Processing anonymous's query[b7ac13a9-396d-4f34-abf5-fa0f8cf43149]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.827 seconds\n",
       "2026-02-03T23:41:33,815Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/b7ac13a9-396d-4f34-abf5-fa0f8cf43149/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:33,917Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/b7ac13a9-396d-4f34-abf5-fa0f8cf43149/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:34,074Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/e784e757-8392-428f-8ccf-3ee9936a7680\n",
       "2026-02-03T23:41:34,077Z INFO ExecuteStatement: Processing anonymous's query[e784e757-8392-428f-8ccf-3ee9936a7680]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:41:34,077Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:34,084Z INFO ExecuteStatement: Query[e784e757-8392-428f-8ccf-3ee9936a7680] in FINISHED_STATE\n",
       "2026-02-03T23:41:34,084Z INFO ExecuteStatement: Processing anonymous's query[e784e757-8392-428f-8ccf-3ee9936a7680]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.007 seconds\n",
       "26/02/03 23:41:33 INFO ExecutePython: Processing anonymous's query[b7ac13a9-396d-4f34-abf5-fa0f8cf43149]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/03 23:41:33 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/03 23:41:33 INFO ExecutePython: Processing anonymous's query[b7ac13a9-396d-4f34-abf5-fa0f8cf43149]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.018 seconds\n",
       "26/02/03 23:41:33 INFO DAGScheduler: Asked to cancel job group b7ac13a9-396d-4f34-abf5-fa0f8cf43149\n",
       "26/02/03 23:41:33 INFO DAGScheduler: Asked to cancel job group b7ac13a9-396d-4f34-abf5-fa0f8cf43149\n",
       "26/02/03 23:41:34 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/e784e757-8392-428f-8ccf-3ee9936a7680\n",
       "26/02/03 23:41:34 INFO ExecutePython: Processing anonymous's query[e784e757-8392-428f-8ccf-3ee9936a7680]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/03 23:41:34 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/03 23:41:34 INFO ExecutePython: Processing anonymous's query[e784e757-8392-428f-8ccf-3ee9936a7680]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/03 23:41:34 INFO DAGScheduler: Asked to cancel job group e784e757-8392-428f-8ccf-3ee9936a7680\n",
       "2026-02-03T23:41:34,681Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/2ea448cd-5153-4b59-a0aa-31f2cb23512a\n",
       "2026-02-03T23:41:34,684Z INFO ExecuteStatement: Processing anonymous's query[2ea448cd-5153-4b59-a0aa-31f2cb23512a]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:41:34,685Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:34,733Z INFO ExecuteStatement: Query[2ea448cd-5153-4b59-a0aa-31f2cb23512a] in FINISHED_STATE\n",
       "2026-02-03T23:41:34,733Z INFO ExecuteStatement: Processing anonymous's query[2ea448cd-5153-4b59-a0aa-31f2cb23512a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.048 seconds\n",
       "2026-02-03T23:41:35,184Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/2ea448cd-5153-4b59-a0aa-31f2cb23512a/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:34 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/2ea448cd-5153-4b59-a0aa-31f2cb23512a\n",
       "26/02/03 23:41:34 INFO ExecutePython: Processing anonymous's query[2ea448cd-5153-4b59-a0aa-31f2cb23512a]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/03 23:41:34 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/03 23:41:34 INFO ExecutePython: Processing anonymous's query[2ea448cd-5153-4b59-a0aa-31f2cb23512a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.043 seconds\n",
       "26/02/03 23:41:34 INFO DAGScheduler: Asked to cancel job group 2ea448cd-5153-4b59-a0aa-31f2cb23512a\n",
       "2026-02-03T23:41:35,360Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/2ea448cd-5153-4b59-a0aa-31f2cb23512a/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:35 INFO DAGScheduler: Asked to cancel job group 2ea448cd-5153-4b59-a0aa-31f2cb23512a\n",
       "2026-02-03T23:41:37,526Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/bfa1ed39-c2f8-49dd-b124-dba2bde98e33\n",
       "2026-02-03T23:41:37,529Z INFO ExecuteStatement: Processing anonymous's query[bfa1ed39-c2f8-49dd-b124-dba2bde98e33]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:41:37,530Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:37,536Z INFO ExecuteStatement: Query[bfa1ed39-c2f8-49dd-b124-dba2bde98e33] in FINISHED_STATE\n",
       "2026-02-03T23:41:37,536Z INFO ExecuteStatement: Processing anonymous's query[bfa1ed39-c2f8-49dd-b124-dba2bde98e33]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.006 seconds\n",
       "2026-02-03T23:41:38,043Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/8ef36179-4b5e-483a-9001-70ae45405598\n",
       "2026-02-03T23:41:38,046Z INFO ExecuteStatement: Processing anonymous's query[8ef36179-4b5e-483a-9001-70ae45405598]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:41:38,047Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:37 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/bfa1ed39-c2f8-49dd-b124-dba2bde98e33\n",
       "26/02/03 23:41:37 INFO ExecutePython: Processing anonymous's query[bfa1ed39-c2f8-49dd-b124-dba2bde98e33]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/03 23:41:37 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/03 23:41:37 INFO ExecutePython: Processing anonymous's query[bfa1ed39-c2f8-49dd-b124-dba2bde98e33]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.002 seconds\n",
       "26/02/03 23:41:37 INFO DAGScheduler: Asked to cancel job group bfa1ed39-c2f8-49dd-b124-dba2bde98e33\n",
       "26/02/03 23:41:38 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/8ef36179-4b5e-483a-9001-70ae45405598\n",
       "26/02/03 23:41:38 INFO ExecutePython: Processing anonymous's query[8ef36179-4b5e-483a-9001-70ae45405598]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/03 23:41:38 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/03 23:41:38 INFO GpuOverrides: Plan conversion to the GPU took 0.28 ms\n",
       "26/02/03 23:41:38 INFO GpuOverrides: GPU plan transition optimization took 0.28 ms\n",
       "2026-02-03T23:41:38,554Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:38 INFO GpuOverrides: Plan conversion to the GPU took 3.87 ms\n",
       "26/02/03 23:41:38 INFO GpuOverrides: GPU plan transition optimization took 0.27 ms\n",
       "2026-02-03T23:41:39,699Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:39 INFO InMemoryFileIndex: It took 72 ms to list leaf files for 1 paths.\n",
       "26/02/03 23:41:39 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
       "26/02/03 23:41:39 INFO DAGScheduler: Got job 1 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/03 23:41:39 INFO DAGScheduler: Final stage: ResultStage 1 (sql at NativeMethodAccessorImpl.java:0)\n",
       "26/02/03 23:41:39 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/03 23:41:39 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/03 23:41:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/03 23:41:39 INFO SQLOperationListener: Query [8ef36179-4b5e-483a-9001-70ae45405598]: Job 1 started with 1 stages, 1 active jobs running\n",
       "26/02/03 23:41:39 INFO SQLOperationListener: Query [8ef36179-4b5e-483a-9001-70ae45405598]: Stage 1.0 started with 1 tasks, 1 active stages running\n",
       "26/02/03 23:41:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/03 23:41:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/03 23:41:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:41:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/03 23:41:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/03 23:41:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
       "26/02/03 23:41:39 WARN FairSchedulableBuilder: A job was submitted with scheduler pool , which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain . Created  with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)\n",
       "26/02/03 23:41:39 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool \n",
       "26/02/03 23:41:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (100.67.56.40, executor 1, partition 0, PROCESS_LOCAL, 9369 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/03 23:41:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 100.67.56.40:33389 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "2026-02-03T23:41:40,837Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:41,985Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:42,371Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-03T23:41:43,102Z INFO ExecuteStatement: Query[8ef36179-4b5e-483a-9001-70ae45405598] in RUNNING_STATE\n",
       "2026-02-03T23:41:43,203Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3321 ms on 100.67.56.40 (executor 1) (1/1)\n",
       "26/02/03 23:41:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
       "26/02/03 23:41:42 INFO DAGScheduler: ResultStage 1 (sql at NativeMethodAccessorImpl.java:0) finished in 3.351 s\n",
       "26/02/03 23:41:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/03 23:41:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
       "26/02/03 23:41:42 INFO DAGScheduler: Job 1 finished: sql at NativeMethodAccessorImpl.java:0, took 3.356105 s\n",
       "26/02/03 23:41:42 INFO SQLOperationListener: Finished stage: Stage(1, 0); Name: 'sql at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 3351 msec\n",
       "26/02/03 23:41:42 INFO StatsReportListener: task runtime:(count: 1, mean: 3321.000000, stdev: 0.000000, max: 3321.000000, min: 3321.000000)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t3.3 s\t3.3 s\t3.3 s\t3.3 s\t3.3 s\t3.3 s\t3.3 s\t3.3 s\t3.3 s\n",
       "26/02/03 23:41:42 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/03 23:41:42 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/03 23:41:42 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/03 23:41:42 INFO StatsReportListener: task result size:(count: 1, mean: 2899.000000, stdev: 0.000000, max: 2899.000000, min: 2899.000000)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t2.8 KiB\t2.8 KiB\t2.8 KiB\t2.8 KiB\t2.8 KiB\t2.8 KiB\t2.8 KiB\t2.8 KiB\t2.8 KiB\n",
       "26/02/03 23:41:42 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 95.844625, stdev: 0.000000, max: 95.844625, min: 95.844625)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t96 %\t96 %\t96 %\t96 %\t96 %\t96 %\t96 %\t96 %\t96 %\n",
       "26/02/03 23:41:42 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/03 23:41:42 INFO StatsReportListener: other time pct: (count: 1, mean: 4.155375, stdev: 0.000000, max: 4.155375, min: 4.155375)\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:41:42 INFO StatsReportListener: \t 4 %\t 4 %\t 4 %\t 4 %\t 4 %\t 4 %\t 4 %\t 4 %\t 4 %\n",
       "26/02/03 23:41:42 INFO SparkSQLEngineListener: Job end. Job 1 state is JobSucceeded\n",
       "26/02/03 23:41:42 INFO SQLOperationListener: Query [8ef36179-4b5e-483a-9001-70ae45405598]: Job 1 succeeded, 0 active jobs running\n",
       "26/02/03 23:41:42 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`maestro_snst_insights2\\` into Hive metastore in Hive compatible format.\n",
       "26/02/03 23:41:42 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=c4e6057c-0bc3-4aa4-ac8c-a2f339150203, clientType=HIVECLI]\n",
       "26/02/03 23:41:42 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
       "26/02/03 23:41:42 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
       "26/02/03 23:41:42 INFO metastore: Closed a connection to metastore, current connections: 0\n",
       "26/02/03 23:41:42 INFO metastore: Trying to connect to metastore with URI thrift://nvspark-metastore.nvspark.svc.cluster.local:9083\n",
       "26/02/03 23:41:42 INFO metastore: Opened a connection to metastore, current connections: 1\n",
       "26/02/03 23:41:42 INFO metastore: Connected to metastore.\n",
       "26/02/03 23:41:43 INFO metastore: Trying to connect to metastore with URI thrift://nvspark-metastore.nvspark.svc.cluster.local:9083\n",
       "26/02/03 23:41:43 INFO metastore: Opened a connection to metastore, current connections: 2\n",
       "26/02/03 23:41:43 INFO metastore: Connected to metastore.\n",
       "26/02/03 23:41:43 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`maestro_snst_insights2\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 61 more\n",
       "2026-02-03T23:41:44,350Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:44,715Z INFO ExecuteStatement: Query[8ef36179-4b5e-483a-9001-70ae45405598] in FINISHED_STATE\n",
       "2026-02-03T23:41:44,716Z INFO ExecuteStatement: Processing anonymous's query[8ef36179-4b5e-483a-9001-70ae45405598]: RUNNING_STATE -> FINISHED_STATE, time taken: 6.669 seconds\n",
       "26/02/03 23:41:44 INFO GpuOverrides: Plan conversion to the GPU took 0.49 ms\n",
       "26/02/03 23:41:44 INFO GpuOverrides: GPU plan transition optimization took 0.21 ms\n",
       "26/02/03 23:41:44 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#56 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#57 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#58 could run on GPU\n",
       "\n",
       "26/02/03 23:41:44 INFO GpuOverrides: Plan conversion to the GPU took 0.73 ms\n",
       "26/02/03 23:41:44 INFO GpuOverrides: GPU plan transition optimization took 0.20 ms\n",
       "26/02/03 23:41:44 INFO CodeGenerator: Code generated in 10.001483 ms\n",
       "26/02/03 23:41:44 INFO CodeGenerator: Code generated in 12.461333 ms\n",
       "26/02/03 23:41:44 INFO ExecutePython: Processing anonymous's query[8ef36179-4b5e-483a-9001-70ae45405598]: RUNNING_STATE -> FINISHED_STATE, time taken: 6.661 seconds\n",
       "26/02/03 23:41:44 INFO ExecutePython: +-------------------------+---------+-------+\n",
       "26/02/03 23:41:44 INFO ExecutePython: |col_name                 |data_type|comment|\n",
       "26/02/03 23:41:44 INFO ExecutePython: +-------------------------+---------+-------+\n",
       "26/02/03 23:41:44 INFO ExecutePython: |id                       |bigint   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |created_at               |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |updated_at               |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |report_id                |bigint   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |node                     |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |host_id                  |bigint   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |link_to_run              |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |status                   |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |eud                      |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |healthcheck              |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |single_node_nonmcpu      |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |range_node_nools         |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |range_node_nools_low_band|int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt                   |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_dgemm             |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_fma               |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_multichase        |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_stream            |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_l1l3              |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_cublas            |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_nccl              |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_nvbandwidth       |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nvssvt_fio               |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |multinode_perf           |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |multinode_nmse           |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |node_crashed             |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |logs                     |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |raw_data                 |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |object_store_path        |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |nautobot_id              |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |total_test_types         |bigint   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |total_failed             |bigint   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |failed_tests             |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |single_node_nccl         |string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |single_node_nccl_loopback|string   |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: |single_node_nemotron     |int      |NULL   |\n",
       "26/02/03 23:41:44 INFO ExecutePython: +-------------------------+---------+-------+\n",
       "26/02/03 23:41:44 INFO DAGScheduler: Asked to cancel job group 8ef36179-4b5e-483a-9001-70ae45405598\n",
       "2026-02-03T23:41:45,531Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:41:45,692Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/8ef36179-4b5e-483a-9001-70ae45405598/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:41:45 INFO DAGScheduler: Asked to cancel job group 8ef36179-4b5e-483a-9001-70ae45405598\n",
       "26/02/03 23:41:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:42:12,371Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:42:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:42:34,993Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/40984e2b-caca-4818-b105-79f42df6f74a\n",
       "2026-02-03T23:42:34,996Z INFO ExecuteStatement: Processing anonymous's query[40984e2b-caca-4818-b105-79f42df6f74a]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:42:34,997Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:42:35,004Z INFO ExecuteStatement: Query[40984e2b-caca-4818-b105-79f42df6f74a] in FINISHED_STATE\n",
       "2026-02-03T23:42:35,004Z INFO ExecuteStatement: Processing anonymous's query[40984e2b-caca-4818-b105-79f42df6f74a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.007 seconds\n",
       "26/02/03 23:42:34 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/40984e2b-caca-4818-b105-79f42df6f74a\n",
       "26/02/03 23:42:34 INFO ExecutePython: Processing anonymous's query[40984e2b-caca-4818-b105-79f42df6f74a]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/03 23:42:34 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/03 23:42:34 INFO ExecutePython: Processing anonymous's query[40984e2b-caca-4818-b105-79f42df6f74a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/03 23:42:35 INFO DAGScheduler: Asked to cancel job group 40984e2b-caca-4818-b105-79f42df6f74a\n",
       "2026-02-03T23:42:35,535Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/107d0cd6-85c0-47fd-bad4-496f67f64bbb\n",
       "2026-02-03T23:42:35,539Z INFO ExecuteStatement: Processing anonymous's query[107d0cd6-85c0-47fd-bad4-496f67f64bbb]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-03T23:42:35,539Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:42:36,105Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/107d0cd6-85c0-47fd-bad4-496f67f64bbb/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:42:35 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/107d0cd6-85c0-47fd-bad4-496f67f64bbb\n",
       "26/02/03 23:42:35 INFO ExecutePython: Processing anonymous's query[107d0cd6-85c0-47fd-bad4-496f67f64bbb]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/03 23:42:35 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/03 23:42:36 INFO InMemoryFileIndex: It took 169 ms to list leaf files for 1 paths.\n",
       "26/02/03 23:42:36 INFO ExecutePython: root\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- id: long (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- created_at: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- updated_at: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- report_id: long (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- node: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- host_id: long (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- link_to_run: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- status: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- eud: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- healthcheck: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- single_node_nonmcpu: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- range_node_nools: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- range_node_nools_low_band: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_dgemm: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_fma: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_multichase: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_stream: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_l1l3: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_cublas: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_nccl: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_nvbandwidth: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nvssvt_fio: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- multinode_perf: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- multinode_nmse: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- node_crashed: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- logs: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- raw_data: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- object_store_path: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- nautobot_id: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- total_test_types: long (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- total_failed: long (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- failed_tests: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- single_node_nccl: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- single_node_nccl_loopback: string (nullable = true)\n",
       "26/02/03 23:42:36 INFO ExecutePython:  |-- single_node_nemotron: integer (nullable = true)\n",
       "26/02/03 23:42:36 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/03 23:42:36 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/03 23:42:36 INFO GpuOverrides: Plan conversion to the GPU took 125.94 ms\n",
       "26/02/03 23:42:36 INFO GpuOverrides: GPU plan transition optimization took 3.92 ms\n",
       "26/02/03 23:42:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
       "26/02/03 23:42:37 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/03 23:42:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "2026-02-03T23:42:37,307Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/107d0cd6-85c0-47fd-bad4-496f67f64bbb/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:42:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:42:37 INFO SparkContext: Created broadcast 2 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "112.577: [GC (Allocation Failure) [PSYoungGen: 762880K->45267K(1077248K)] 835369K->117772K(4919808K), 0.0839854 secs] [Times: user=0.09 sys=0.02, real=0.08 secs] \n",
       "26/02/03 23:42:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 100.67.56.40:33389 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/03 23:42:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:42:37 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/03 23:42:37 INFO DAGScheduler: Registering RDD 20 (RDD at GpuExec.scala:58) as input to shuffle 0\n",
       "26/02/03 23:42:37 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/03 23:42:37 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/03 23:42:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
       "26/02/03 23:42:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)\n",
       "26/02/03 23:42:37 INFO SQLOperationListener: Query [107d0cd6-85c0-47fd-bad4-496f67f64bbb]: Job 2 started with 2 stages, 1 active jobs running\n",
       "26/02/03 23:42:37 INFO DAGScheduler: Submitting ShuffleMapStage 2 (GpuOpTimeTrackingRDD[20] at RDD at GpuExec.scala:58), which has no missing parents\n",
       "26/02/03 23:42:37 INFO SQLOperationListener: Query [107d0cd6-85c0-47fd-bad4-496f67f64bbb]: Stage 2.0 started with 1 tasks, 1 active stages running\n",
       "26/02/03 23:42:37 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 36.1 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:37 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 100.67.56.160:7079 (size: 17.0 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:42:37 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/03 23:42:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (GpuOpTimeTrackingRDD[20] at RDD at GpuExec.scala:58) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/03 23:42:37 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
       "26/02/03 23:42:37 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool \n",
       "26/02/03 23:42:37 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (100.67.56.40, executor 1, partition 0, PROCESS_LOCAL, 10132 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/03 23:42:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 100.67.56.40:33389 (size: 17.0 KiB, free: 9.0 GiB)\n",
       "2026-02-03T23:42:38,464Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/107d0cd6-85c0-47fd-bad4-496f67f64bbb/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:42:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 100.67.56.40:33389 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "2026-02-03T23:42:39,611Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/107d0cd6-85c0-47fd-bad4-496f67f64bbb/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:42:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2077 ms on 100.67.56.40 (executor 1) (1/1)\n",
       "26/02/03 23:42:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
       "26/02/03 23:42:39 INFO DAGScheduler: ShuffleMapStage 2 (RDD at GpuExec.scala:58) finished in 2.156 s\n",
       "26/02/03 23:42:39 INFO SQLOperationListener: Finished stage: Stage(2, 0); Name: 'RDD at GpuExec.scala:58'; Status: succeeded; numTasks: 1; Took: 2156 msec\n",
       "26/02/03 23:42:39 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/03 23:42:39 INFO StatsReportListener: task runtime:(count: 1, mean: 2077.000000, stdev: 0.000000, max: 2077.000000, min: 2077.000000)\n",
       "26/02/03 23:42:39 INFO DAGScheduler: running: Set()\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t2.1 s\t2.1 s\t2.1 s\t2.1 s\t2.1 s\t2.1 s\t2.1 s\t2.1 s\t2.1 s\n",
       "26/02/03 23:42:39 INFO DAGScheduler: waiting: Set(ResultStage 3)\n",
       "26/02/03 23:42:39 INFO DAGScheduler: failed: Set()\n",
       "26/02/03 23:42:39 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 316040.000000, stdev: 0.000000, max: 316040.000000, min: 316040.000000)\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t308.6 KiB\t308.6 KiB\t308.6 KiB\t308.6 KiB\t308.6 KiB\t308.6 KiB\t308.6 KiB\t308.6 KiB\t308.6 KiB\n",
       "26/02/03 23:42:39 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/03 23:42:39 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/03 23:42:39 INFO StatsReportListener: task result size:(count: 1, mean: 6313.000000, stdev: 0.000000, max: 6313.000000, min: 6313.000000)\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t6.2 KiB\t6.2 KiB\t6.2 KiB\t6.2 KiB\t6.2 KiB\t6.2 KiB\t6.2 KiB\t6.2 KiB\t6.2 KiB\n",
       "26/02/03 23:42:39 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 80.356283, stdev: 0.000000, max: 80.356283, min: 80.356283)\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\n",
       "26/02/03 23:42:39 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/03 23:42:39 INFO StatsReportListener: other time pct: (count: 1, mean: 19.643717, stdev: 0.000000, max: 19.643717, min: 19.643717)\n",
       "26/02/03 23:42:39 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:39 INFO StatsReportListener: \t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\n",
       "26/02/03 23:42:40 INFO SQLOperationListener: Query [107d0cd6-85c0-47fd-bad4-496f67f64bbb]: Stage 3.0 started with 1 tasks, 1 active stages running\n",
       "26/02/03 23:42:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 57.8 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 100.67.56.160:7079 (size: 25.0 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:42:40 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/03 23:42:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/03 23:42:40 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
       "26/02/03 23:42:40 INFO FairSchedulableBuilder: Added task set TaskSet_3.0 tasks to pool \n",
       "26/02/03 23:42:40 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (100.67.56.40, executor 1, partition 0, NODE_LOCAL, 9238 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/03 23:42:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 100.67.56.40:33389 (size: 25.0 KiB, free: 9.0 GiB)\n",
       "26/02/03 23:42:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 100.67.56.40:41540\n",
       "2026-02-03T23:42:40,542Z INFO ExecuteStatement: Query[107d0cd6-85c0-47fd-bad4-496f67f64bbb] in RUNNING_STATE\n",
       "2026-02-03T23:42:40,771Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/107d0cd6-85c0-47fd-bad4-496f67f64bbb/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:42:40 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 408 ms on 100.67.56.40 (executor 1) (1/1)\n",
       "26/02/03 23:42:40 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
       "26/02/03 23:42:40 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.422 s\n",
       "26/02/03 23:42:40 INFO SQLOperationListener: Finished stage: Stage(3, 0); Name: 'showString at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 422 msec\n",
       "26/02/03 23:42:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/03 23:42:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
       "26/02/03 23:42:40 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 2.621121 s\n",
       "26/02/03 23:42:40 INFO StatsReportListener: task runtime:(count: 1, mean: 408.000000, stdev: 0.000000, max: 408.000000, min: 408.000000)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t408.0 ms\t408.0 ms\t408.0 ms\t408.0 ms\t408.0 ms\t408.0 ms\t408.0 ms\t408.0 ms\t408.0 ms\n",
       "26/02/03 23:42:40 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/03 23:42:40 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/03 23:42:40 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/03 23:42:40 INFO StatsReportListener: task result size:(count: 1, mean: 327325.000000, stdev: 0.000000, max: 327325.000000, min: 327325.000000)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t319.7 KiB\t319.7 KiB\t319.7 KiB\t319.7 KiB\t319.7 KiB\t319.7 KiB\t319.7 KiB\t319.7 KiB\t319.7 KiB\n",
       "26/02/03 23:42:40 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 80.882353, stdev: 0.000000, max: 80.882353, min: 80.882353)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\n",
       "26/02/03 23:42:40 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/03 23:42:40 INFO StatsReportListener: other time pct: (count: 1, mean: 19.117647, stdev: 0.000000, max: 19.117647, min: 19.117647)\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/03 23:42:40 INFO StatsReportListener: \t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\n",
       "26/02/03 23:42:40 INFO SparkSQLEngineListener: Job end. Job 2 state is JobSucceeded\n",
       "26/02/03 23:42:40 INFO SQLOperationListener: Query [107d0cd6-85c0-47fd-bad4-496f67f64bbb]: Job 2 succeeded, 0 active jobs running\n",
       "26/02/03 23:42:40 INFO CodeGenerator: Code generated in 96.67657 ms\n",
       "26/02/03 23:42:40 INFO ExecutePython: +------+--------------------+--------------------+---------+--------------------+-------+-----------+------+----+-----------+-------------------+----------------+-------------------------+------+------------+----------+-----------------+-------------+-----------+-------------+-----------+------------------+----------+--------------+--------------+------------+--------------------+--------------------+--------------------+--------------------+----------------+------------+------------+----------------+-------------------------+--------------------+\n",
       "26/02/03 23:42:40 INFO ExecutePython: |    id|          created_at|          updated_at|report_id|                node|host_id|link_to_run|status| eud|healthcheck|single_node_nonmcpu|range_node_nools|range_node_nools_low_band|nvssvt|nvssvt_dgemm|nvssvt_fma|nvssvt_multichase|nvssvt_stream|nvssvt_l1l3|nvssvt_cublas|nvssvt_nccl|nvssvt_nvbandwidth|nvssvt_fio|multinode_perf|multinode_nmse|node_crashed|                logs|            raw_data|   object_store_path|         nautobot_id|total_test_types|total_failed|failed_tests|single_node_nccl|single_node_nccl_loopback|single_node_nemotron|\n",
       "26/02/03 23:42:40 INFO ExecutePython: +------+--------------------+--------------------+---------+--------------------+-------+-----------+------+----+-----------+-------------------+----------------+-------------------------+------+------------+----------+-----------------+-------------+-----------+-------------+-----------+------------------+----------+--------------+--------------+------------+--------------------+--------------------+--------------------+--------------------+----------------+------------+------------+----------------+-------------------------+--------------------+\n",
       "26/02/03 23:42:40 INFO ExecutePython: |103326|2026-01-20T17:06:...|2026-01-24T03:00:...|     9357|nvl72138-T13.oci-...|   4441|       NULL|  pass|PASS|       NULL|               NULL|            NULL|                     NULL|  PASS|        NULL|      NULL|             NULL|         NULL|       NULL|         NULL|       PASS|              NULL|      NULL|          NULL|          NULL|        NULL|9357 - validation...|{\"id\": 9357, \"hos...|validation/result...|019bdbe9-56e9-775...|               8|           0|        NULL|            PASS|                     PASS|                NULL|\n",
       "26/02/03 23:42:40 INFO ExecutePython: |102993|2026-01-20T17:06:...|2026-01-24T03:00:...|     9359|nvl72008-T06.oci-...|   1103|       NULL|  pass|PASS|       NULL|               NULL|            NULL|                     NULL|  PASS|        NULL|      NULL|             NULL|         NULL|       NULL|         NULL|       PASS|              NULL|      NULL|          NULL|          NULL|        NULL|9359 - validation...|{\"id\": 9359, \"hos...|validation/result...|019bdbe8-df6f-70b...|               8|           0|        NULL|            PASS|                     PASS|                NULL|\n",
       "26/02/03 23:42:40 INFO ExecutePython: |102989|2026-01-20T17:06:...|2026-01-24T03:00:...|     9402|nvl72007-T15.oci-...|   1092|       NULL|  pass|PASS|       NULL|               NULL|            NULL|                     NULL|  PASS|        NULL|      NULL|             NULL|         NULL|       NULL|         NULL|       PASS|              NULL|      NULL|          NULL|          NULL|        NULL|9402 - validation...|{\"id\": 9402, \"hos...|validation/result...|019ba324-78cd-7a8...|               8|           0|        NULL|            PASS|                     PASS|                NULL|\n",
       "26/02/03 23:42:40 INFO ExecutePython: +------+--------------------+--------------------+---------+--------------------+-------+-----------+------+----+-----------+-------------------+----------------+-------------------------+------+------------+----------+-----------------+-------------+-----------+-------------+-----------+------------------+----------+--------------+--------------+------------+--------------------+--------------------+--------------------+--------------------+----------------+------------+------------+----------------+-------------------------+--------------------+\n",
       "26/02/03 23:42:40 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/03 23:42:40 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/03 23:42:41 INFO GpuOverrides: Plan conversion to the GPU took 68.68 ms\n",
       "26/02/03 23:42:41 INFO GpuOverrides: Plan conversion to the GPU took 2.50 ms\n",
       "26/02/03 23:42:41 INFO GpuOverrides: Plan conversion to the GPU took 0.24 ms\n",
       "26/02/03 23:42:41 INFO GpuOverrides: Plan conversion to the GPU took 2.78 ms\n",
       "26/02/03 23:42:41 INFO GpuOverrides: GPU plan transition optimization took 0.18 ms\n",
       "26/02/03 23:42:41 INFO GpuOverrides: Plan conversion to the GPU took 2.92 ms\n",
       "26/02/03 23:42:41 INFO GpuOverrides: GPU plan transition optimization took 1.80 ms\n",
       "26/02/03 23:42:41 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/03 23:42:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO SparkContext: Created broadcast 5 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "2026-02-03T23:42:41,809Z INFO ExecuteStatement: Query[107d0cd6-85c0-47fd-bad4-496f67f64bbb] in FINISHED_STATE\n",
       "2026-02-03T23:42:41,809Z INFO ExecuteStatement: Processing anonymous's query[107d0cd6-85c0-47fd-bad4-496f67f64bbb]: RUNNING_STATE -> FINISHED_STATE, time taken: 6.27 seconds\n",
       "2026-02-03T23:42:41,923Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/107d0cd6-85c0-47fd-bad4-496f67f64bbb/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-03T23:42:42,083Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/107d0cd6-85c0-47fd-bad4-496f67f64bbb/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/03 23:42:41 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Registering RDD 39 (RDD at GpuExec.scala:58) as input to shuffle 1\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Final stage: ResultStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Submitting ShuffleMapStage 4 (GpuOpTimeTrackingRDD[39] at RDD at GpuExec.scala:58), which has no missing parents\n",
       "26/02/03 23:42:41 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 31.7 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 100.67.56.160:7079 (size: 14.7 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (GpuOpTimeTrackingRDD[39] at RDD at GpuExec.scala:58) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/03 23:42:41 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
       "26/02/03 23:42:41 INFO FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool \n",
       "26/02/03 23:42:41 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (100.67.56.40, executor 1, partition 0, PROCESS_LOCAL, 10132 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/03 23:42:41 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 100.67.56.40:33389 (size: 14.7 KiB, free: 9.0 GiB)\n",
       "26/02/03 23:42:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 100.67.56.40:33389 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/03 23:42:41 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 219 ms on 100.67.56.40 (executor 1) (1/1)\n",
       "26/02/03 23:42:41 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
       "26/02/03 23:42:41 INFO DAGScheduler: ShuffleMapStage 4 (RDD at GpuExec.scala:58) finished in 0.256 s\n",
       "26/02/03 23:42:41 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/03 23:42:41 INFO DAGScheduler: running: Set()\n",
       "26/02/03 23:42:41 INFO DAGScheduler: waiting: Set(ResultStage 5)\n",
       "26/02/03 23:42:41 INFO DAGScheduler: failed: Set()\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[52] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/03 23:42:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 49.8 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 100.67.56.160:7079 (size: 23.6 KiB, free: 8.4 GiB)\n",
       "26/02/03 23:42:41 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[52] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/03 23:42:41 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
       "26/02/03 23:42:41 INFO FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool \n",
       "26/02/03 23:42:41 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (100.67.56.40, executor 1, partition 0, NODE_LOCAL, 9238 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/03 23:42:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 100.67.56.40:33389 (size: 23.6 KiB, free: 9.0 GiB)\n",
       "26/02/03 23:42:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 100.67.56.40:41540\n",
       "26/02/03 23:42:41 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 257 ms on 100.67.56.40 (executor 1) (1/1)\n",
       "26/02/03 23:42:41 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
       "26/02/03 23:42:41 INFO DAGScheduler: ResultStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.272 s\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/03 23:42:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.537879 s\n",
       "26/02/03 23:42:41 INFO SparkSQLEngineListener: Job end. Job 3 state is JobSucceeded\n",
       "26/02/03 23:42:41 INFO ExecutePython: 3\n",
       "26/02/03 23:42:41 INFO ExecutePython: Processing anonymous's query[107d0cd6-85c0-47fd-bad4-496f67f64bbb]: RUNNING_STATE -> FINISHED_STATE, time taken: 6.263 seconds\n",
       "26/02/03 23:42:41 INFO DAGScheduler: Asked to cancel job group 107d0cd6-85c0-47fd-bad4-496f67f64bbb\n",
       "26/02/03 23:42:42 INFO DAGScheduler: Asked to cancel job group 107d0cd6-85c0-47fd-bad4-496f67f64bbb\n",
       "2026-02-03T23:42:42,371Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-03T23:42:42,372Z WARN KyuubiOperationManager: Operation OperationHandle [b7ac13a9-396d-4f34-abf5-fa0f8cf43149] is timed-out and will be closed\n",
       "2026-02-03T23:42:42,372Z WARN KyuubiOperationManager: Operation OperationHandle [e784e757-8392-428f-8ccf-3ee9936a7680] is timed-out and will be closed\n",
       "2026-02-03T23:42:42,372Z WARN KyuubiOperationManager: Operation OperationHandle [bfa1ed39-c2f8-49dd-b124-dba2bde98e33] is timed-out and will be closed\n",
       "2026-02-03T23:42:42,372Z WARN KyuubiOperationManager: Operation OperationHandle [2ea448cd-5153-4b59-a0aa-31f2cb23512a] is timed-out and will be closed\n",
       "2026-02-03T23:42:42,372Z WARN KyuubiOperationManager: Operation OperationHandle [22470745-2fb5-49b4-bfa1-49ddd7a7f947] is timed-out and will be closed\n",
       "2026-02-03T23:42:42,403Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:B7 AC 13 A9 39 6D 4F 34 AB F5 FA 0F 8C F4 31 49, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-03T23:42:42,405Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:E7 84 E7 57 83 92 42 8F 8C CF 3E E9 93 6A 76 80, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-03T23:42:42,407Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:BF A1 ED 39 C2 F8 49 DD B1 24 DB A2 BD E9 8E 33, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-03T23:42:42,409Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:2E A4 48 CD 51 53 4B 59 A0 AA 31 F2 CB 23 51 2A, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-03T23:42:42,410Z WARN KyuubiOperationManager: Operation OperationHandle [56d8d163-f51b-4169-a367-b4eb66ed7e92] is timed-out and will be closed\n",
       "26/02/03 23:42:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/03 23:42:59 WARN SparkSQLOperationManager: Operation OperationHandle [8ef36179-4b5e-483a-9001-70ae45405598] is timed-out and will be closed\n",
       "2026-02-03T23:43:12,410Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-03T23:43:12,411Z WARN KyuubiOperationManager: Operation OperationHandle [8ef36179-4b5e-483a-9001-70ae45405598] is timed-out and will be closed\n",
       "2026-02-03T23:43:12,440Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:8E F3 61 79 4B 5E 48 3A 90 01 70 AE 45 40 55 98, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [8ef36179-4b5e-483a-9001-70ae45405598]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/03 23:43:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [8ef36179-4b5e-483a-9001-70ae45405598]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/03 23:43:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:43:42,441Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-03T23:43:42,441Z WARN KyuubiOperationManager: Operation OperationHandle [40984e2b-caca-4818-b105-79f42df6f74a] is timed-out and will be closed\n",
       "2026-02-03T23:43:42,441Z WARN KyuubiOperationManager: Operation OperationHandle [107d0cd6-85c0-47fd-bad4-496f67f64bbb] is timed-out and will be closed\n",
       "2026-02-03T23:43:42,443Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:40 98 4E 2B CA CA 48 18 B1 05 79 F4 2D F6 F7 4A, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-03T23:43:42,444Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:10 7D 0C D6 85 C0 47 FD BA D4 49 6F 67 F6 4B BB, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/03 23:43:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:44:12,445Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:44:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/03 23:44:41 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.56.40:45892\n",
       "2026-02-03T23:44:42,445Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:44:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:45:12,445Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:45:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:45:42,446Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:45:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:46:12,446Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:46:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:46:42,447Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:46:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:47:12,447Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:47:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:47:42,447Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:47:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:48:12,448Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:48:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:48:42,448Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:48:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:49:12,449Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:49:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:49:42,449Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:49:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:50:12,449Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:50:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:50:42,450Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:50:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:51:12,450Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:51:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:51:42,450Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:51:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:52:12,451Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:52:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:52:42,451Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:52:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:53:12,452Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:53:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:53:42,452Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:53:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:54:12,452Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:54:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:54:42,453Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:54:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:55:12,453Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:55:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:55:42,453Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:55:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:56:12,454Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:56:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:56:42,454Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:56:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:57:12,455Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:57:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/03 23:57:41 INFO KubernetesClusterSchedulerBackend: Requesting to kill executor(s) 1\n",
       "26/02/03 23:57:41 INFO KubernetesClusterSchedulerBackend: Actual list of executor(s) to be killed is 1\n",
       "26/02/03 23:57:41 INFO TaskSchedulerImpl: Executor 1 on 100.67.56.40 killed by driver.\n",
       "26/02/03 23:57:41 INFO ExecutorAllocationManager: Executors 1 removed due to idle timeout.\n",
       "26/02/03 23:57:41 INFO DAGScheduler: Executor lost: 1 (epoch 2)\n",
       "26/02/03 23:57:41 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.\n",
       "26/02/03 23:57:41 INFO ExecutorMonitor: Executor 1 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 1, unexpectedly exited: 0).\n",
       "26/02/03 23:57:41 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 100.67.56.40, 33389, None)\n",
       "26/02/03 23:57:41 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor\n",
       "26/02/03 23:57:41 INFO DAGScheduler: Shuffle files lost for executor: 1 (epoch 2)\n",
       "2026-02-03T23:57:42,455Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:57:43 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.56.40:41540\n",
       "26/02/03 23:57:44 INFO BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.\n",
       "26/02/03 23:57:44 INFO BlockManagerMaster: Removal of executor 1 requested\n",
       "26/02/03 23:57:44 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: Asked to remove non-existent executor 1\n",
       "26/02/03 23:57:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:58:12,455Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:58:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:58:42,456Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:58:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:59:12,456Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:59:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-03T23:59:42,456Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/03 23:59:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:00:12,457Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:00:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:00:42,457Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:00:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:01:12,458Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:01:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:01:39,391Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/fcd6dd86-d213-48b7-a200-51ec67ff5d1f\n",
       "2026-02-04T00:01:39,394Z INFO ExecuteStatement: Processing anonymous's query[fcd6dd86-d213-48b7-a200-51ec67ff5d1f]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:01:39,394Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:01:39,399Z INFO ExecuteStatement: Query[fcd6dd86-d213-48b7-a200-51ec67ff5d1f] in FINISHED_STATE\n",
       "2026-02-04T00:01:39,399Z INFO ExecuteStatement: Processing anonymous's query[fcd6dd86-d213-48b7-a200-51ec67ff5d1f]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T00:01:39,905Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/44dcaf8c-2362-443d-be90-f74487393b76\n",
       "2026-02-04T00:01:39,909Z INFO ExecuteStatement: Processing anonymous's query[44dcaf8c-2362-443d-be90-f74487393b76]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:01:39,909Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:01:39,974Z INFO ExecuteStatement: Query[44dcaf8c-2362-443d-be90-f74487393b76] in FINISHED_STATE\n",
       "2026-02-04T00:01:39,974Z INFO ExecuteStatement: Processing anonymous's query[44dcaf8c-2362-443d-be90-f74487393b76]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.065 seconds\n",
       "26/02/04 00:01:39 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/fcd6dd86-d213-48b7-a200-51ec67ff5d1f\n",
       "26/02/04 00:01:39 INFO ExecutePython: Processing anonymous's query[fcd6dd86-d213-48b7-a200-51ec67ff5d1f]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:01:39 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:01:39 INFO ExecutePython: Processing anonymous's query[fcd6dd86-d213-48b7-a200-51ec67ff5d1f]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:01:39 INFO DAGScheduler: Asked to cancel job group fcd6dd86-d213-48b7-a200-51ec67ff5d1f\n",
       "1254.442: [GC (Allocation Failure) [PSYoungGen: 1072851K->32951K(1050112K)] 1145356K->105472K(4892672K), 0.0285662 secs] [Times: user=0.06 sys=0.01, real=0.03 secs] \n",
       "26/02/04 00:01:39 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 100.67.56.160:7079 in memory (size: 14.7 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:01:39 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 100.67.56.160:7079 in memory (size: 17.0 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:01:39 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 100.67.56.160:7079 in memory (size: 25.0 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:01:39 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 100.67.56.160:7079 in memory (size: 23.6 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:01:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:01:39 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:01:39 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/44dcaf8c-2362-443d-be90-f74487393b76\n",
       "26/02/04 00:01:39 INFO ExecutePython: Processing anonymous's query[44dcaf8c-2362-443d-be90-f74487393b76]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:01:39 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:01:39 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:01:39 INFO ExecutePython: PyArrow Export: PostgreSQL -> Parquet (JSON method)\n",
       "26/02/04 00:01:39 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:01:39 INFO ExecutePython: Timestamp: 20260204_000139\n",
       "26/02/04 00:01:39 INFO ExecutePython: Tables: ['slurm_nodes']\n",
       "26/02/04 00:01:39 INFO ExecutePython: Limit per table: 10 rows\n",
       "26/02/04 00:01:39 INFO ExecutePython: ----------------------------------------\n",
       "26/02/04 00:01:39 INFO ExecutePython: Exporting slurm_nodes...\n",
       "26/02/04 00:01:39 INFO ExecutePython:   Limit: 10 rows, Batch size: 5\n",
       "26/02/04 00:01:39 INFO ExecutePython:   Batch 1/2: Rows 1-5...\n",
       "26/02/04 00:01:39 INFO ExecutePython:    Failed to export slurm_nodes: [Errno 2] No such file or directory: 'kubectl'\n",
       "26/02/04 00:01:39 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:01:39 INFO ExecutePython: Summary\n",
       "26/02/04 00:01:39 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:01:39 INFO ExecutePython:  No tables exported successfully\n",
       "26/02/04 00:01:39 INFO ExecutePython: Processing anonymous's query[44dcaf8c-2362-443d-be90-f74487393b76]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.06 seconds\n",
       "26/02/04 00:01:39 INFO DAGScheduler: Asked to cancel job group 44dcaf8c-2362-443d-be90-f74487393b76\n",
       "2026-02-04T00:01:40,449Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/44dcaf8c-2362-443d-be90-f74487393b76/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:01:40,608Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/44dcaf8c-2362-443d-be90-f74487393b76/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:01:40 INFO DAGScheduler: Asked to cancel job group 44dcaf8c-2362-443d-be90-f74487393b76\n",
       "2026-02-04T00:01:42,458Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:01:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:02:12,458Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:02:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:02:42,459Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:02:42,459Z WARN KyuubiOperationManager: Operation OperationHandle [44dcaf8c-2362-443d-be90-f74487393b76] is timed-out and will be closed\n",
       "2026-02-04T00:02:42,459Z WARN KyuubiOperationManager: Operation OperationHandle [fcd6dd86-d213-48b7-a200-51ec67ff5d1f] is timed-out and will be closed\n",
       "2026-02-04T00:02:42,461Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:44 DC AF 8C 23 62 44 3D BE 90 F7 44 87 39 3B 76, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T00:02:42,463Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:FC D6 DD 86 D2 13 48 B7 A2 00 51 EC 67 FF 5D 1F, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 00:02:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:03:12,463Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:03:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:03:42,464Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:03:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:04:12,464Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:04:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:04:42,464Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:04:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:05:12,465Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:05:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:05:42,465Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:05:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:06:12,465Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:06:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:06:42,466Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:06:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:07:12,466Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:07:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:07:42,466Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:07:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:08:12,467Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:08:22,694Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/75f7c2a7-f194-4f69-8e4b-5ecb03699347\n",
       "2026-02-04T00:08:22,697Z INFO ExecuteStatement: Processing anonymous's query[75f7c2a7-f194-4f69-8e4b-5ecb03699347]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:08:22,698Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:08:22,703Z INFO ExecuteStatement: Query[75f7c2a7-f194-4f69-8e4b-5ecb03699347] in FINISHED_STATE\n",
       "2026-02-04T00:08:22,703Z INFO ExecuteStatement: Processing anonymous's query[75f7c2a7-f194-4f69-8e4b-5ecb03699347]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.006 seconds\n",
       "2026-02-04T00:08:23,222Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/02e820f8-cb5f-41db-86e4-c0e2fd7d8058\n",
       "2026-02-04T00:08:23,225Z INFO ExecuteStatement: Processing anonymous's query[02e820f8-cb5f-41db-86e4-c0e2fd7d8058]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:08:23,225Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:08:23,236Z INFO ExecuteStatement: Query[02e820f8-cb5f-41db-86e4-c0e2fd7d8058] in FINISHED_STATE\n",
       "2026-02-04T00:08:23,236Z INFO ExecuteStatement: Processing anonymous's query[02e820f8-cb5f-41db-86e4-c0e2fd7d8058]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.011 seconds\n",
       "26/02/04 00:08:22 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/75f7c2a7-f194-4f69-8e4b-5ecb03699347\n",
       "26/02/04 00:08:22 INFO ExecutePython: Processing anonymous's query[75f7c2a7-f194-4f69-8e4b-5ecb03699347]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:08:22 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:08:22 INFO ExecutePython: Processing anonymous's query[75f7c2a7-f194-4f69-8e4b-5ecb03699347]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:08:22 INFO DAGScheduler: Asked to cancel job group 75f7c2a7-f194-4f69-8e4b-5ecb03699347\n",
       "26/02/04 00:08:23 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/02e820f8-cb5f-41db-86e4-c0e2fd7d8058\n",
       "26/02/04 00:08:23 INFO ExecutePython: Processing anonymous's query[02e820f8-cb5f-41db-86e4-c0e2fd7d8058]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:08:23 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:08:23 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:23 INFO ExecutePython: PyArrow Export: PostgreSQL -> Parquet (JSON method)\n",
       "26/02/04 00:08:23 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:23 INFO ExecutePython: Timestamp: 20260204_000823\n",
       "26/02/04 00:08:23 INFO ExecutePython: Tables: ['slurm_nodes']\n",
       "26/02/04 00:08:23 INFO ExecutePython: Limit per table: 10 rows\n",
       "26/02/04 00:08:23 INFO ExecutePython: ----------------------------------------\n",
       "26/02/04 00:08:23 INFO ExecutePython: Exporting slurm_nodes...\n",
       "26/02/04 00:08:23 INFO ExecutePython:   Limit: 10 rows, Batch size: 5\n",
       "26/02/04 00:08:23 INFO ExecutePython:   Batch 1/2: Rows 1-5...\n",
       "26/02/04 00:08:23 INFO ExecutePython:    Failed to export slurm_nodes: [Errno 2] No such file or directory: 'kubectl'\n",
       "26/02/04 00:08:23 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:23 INFO ExecutePython: Summary\n",
       "26/02/04 00:08:23 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:23 INFO ExecutePython:  No tables exported successfully\n",
       "26/02/04 00:08:23 INFO ExecutePython: Processing anonymous's query[02e820f8-cb5f-41db-86e4-c0e2fd7d8058]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 00:08:23 INFO DAGScheduler: Asked to cancel job group 02e820f8-cb5f-41db-86e4-c0e2fd7d8058\n",
       "2026-02-04T00:08:23,715Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/02e820f8-cb5f-41db-86e4-c0e2fd7d8058/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:08:23,868Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/02e820f8-cb5f-41db-86e4-c0e2fd7d8058/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:08:23 INFO DAGScheduler: Asked to cancel job group 02e820f8-cb5f-41db-86e4-c0e2fd7d8058\n",
       "26/02/04 00:08:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:08:42,467Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:08:47,792Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/3a17a526-f5d8-43d2-91d2-7c6827e0ca8a\n",
       "2026-02-04T00:08:47,794Z INFO ExecuteStatement: Processing anonymous's query[3a17a526-f5d8-43d2-91d2-7c6827e0ca8a]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:08:47,794Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:08:47,799Z INFO ExecuteStatement: Query[3a17a526-f5d8-43d2-91d2-7c6827e0ca8a] in FINISHED_STATE\n",
       "2026-02-04T00:08:47,800Z INFO ExecuteStatement: Processing anonymous's query[3a17a526-f5d8-43d2-91d2-7c6827e0ca8a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.006 seconds\n",
       "2026-02-04T00:08:48,337Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/74441b4c-0a57-4668-a7e1-d7b8e06c9e2a\n",
       "2026-02-04T00:08:48,339Z INFO ExecuteStatement: Processing anonymous's query[74441b4c-0a57-4668-a7e1-d7b8e06c9e2a]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:08:48,340Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:08:48,348Z INFO ExecuteStatement: Query[74441b4c-0a57-4668-a7e1-d7b8e06c9e2a] in FINISHED_STATE\n",
       "2026-02-04T00:08:48,349Z INFO ExecuteStatement: Processing anonymous's query[74441b4c-0a57-4668-a7e1-d7b8e06c9e2a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.009 seconds\n",
       "26/02/04 00:08:47 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/3a17a526-f5d8-43d2-91d2-7c6827e0ca8a\n",
       "26/02/04 00:08:47 INFO ExecutePython: Processing anonymous's query[3a17a526-f5d8-43d2-91d2-7c6827e0ca8a]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:08:47 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:08:47 INFO ExecutePython: Processing anonymous's query[3a17a526-f5d8-43d2-91d2-7c6827e0ca8a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.0 seconds\n",
       "26/02/04 00:08:47 INFO DAGScheduler: Asked to cancel job group 3a17a526-f5d8-43d2-91d2-7c6827e0ca8a\n",
       "26/02/04 00:08:48 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/74441b4c-0a57-4668-a7e1-d7b8e06c9e2a\n",
       "26/02/04 00:08:48 INFO ExecutePython: Processing anonymous's query[74441b4c-0a57-4668-a7e1-d7b8e06c9e2a]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:08:48 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:08:48 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:48 INFO ExecutePython: PyArrow Export: PostgreSQL -> Parquet (JSON method)\n",
       "26/02/04 00:08:48 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:48 INFO ExecutePython: Timestamp: 20260204_000848\n",
       "26/02/04 00:08:48 INFO ExecutePython: Tables: ['slurm_nodes']\n",
       "26/02/04 00:08:48 INFO ExecutePython: Limit per table: 10 rows\n",
       "26/02/04 00:08:48 INFO ExecutePython: ----------------------------------------\n",
       "26/02/04 00:08:48 INFO ExecutePython: Exporting slurm_nodes...\n",
       "26/02/04 00:08:48 INFO ExecutePython:   Limit: 10 rows, Batch size: 5\n",
       "26/02/04 00:08:48 INFO ExecutePython:   Batch 1/2: Rows 1-5...\n",
       "26/02/04 00:08:48 INFO ExecutePython:    Failed to export slurm_nodes: [Errno 2] No such file or directory: 'kubectl'\n",
       "26/02/04 00:08:48 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:48 INFO ExecutePython: Summary\n",
       "26/02/04 00:08:48 INFO ExecutePython: ============================================================\n",
       "26/02/04 00:08:48 INFO ExecutePython:  No tables exported successfully\n",
       "26/02/04 00:08:48 INFO ExecutePython: Processing anonymous's query[74441b4c-0a57-4668-a7e1-d7b8e06c9e2a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 00:08:48 INFO DAGScheduler: Asked to cancel job group 74441b4c-0a57-4668-a7e1-d7b8e06c9e2a\n",
       "2026-02-04T00:08:48,834Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/74441b4c-0a57-4668-a7e1-d7b8e06c9e2a/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:08:48,975Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/74441b4c-0a57-4668-a7e1-d7b8e06c9e2a/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:08:48 INFO DAGScheduler: Asked to cancel job group 74441b4c-0a57-4668-a7e1-d7b8e06c9e2a\n",
       "26/02/04 00:08:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:09:12,467Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:09:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:09:29 WARN SparkSQLOperationManager: Operation OperationHandle [75f7c2a7-f194-4f69-8e4b-5ecb03699347] is timed-out and will be closed\n",
       "26/02/04 00:09:29 WARN SparkSQLOperationManager: Operation OperationHandle [02e820f8-cb5f-41db-86e4-c0e2fd7d8058] is timed-out and will be closed\n",
       "2026-02-04T00:09:42,468Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:09:42,468Z WARN KyuubiOperationManager: Operation OperationHandle [75f7c2a7-f194-4f69-8e4b-5ecb03699347] is timed-out and will be closed\n",
       "2026-02-04T00:09:42,468Z WARN KyuubiOperationManager: Operation OperationHandle [02e820f8-cb5f-41db-86e4-c0e2fd7d8058] is timed-out and will be closed\n",
       "2026-02-04T00:09:42,470Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:75 F7 C2 A7 F1 94 4F 69 8E 4B 5E CB 03 69 93 47, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [75f7c2a7-f194-4f69-8e4b-5ecb03699347]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:09:42,472Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:02 E8 20 F8 CB 5F 41 DB 86 E4 C0 E2 FD 7D 80 58, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [02e820f8-cb5f-41db-86e4-c0e2fd7d8058]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:09:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [75f7c2a7-f194-4f69-8e4b-5ecb03699347]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:09:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [02e820f8-cb5f-41db-86e4-c0e2fd7d8058]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:09:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:09:59 WARN SparkSQLOperationManager: Operation OperationHandle [74441b4c-0a57-4668-a7e1-d7b8e06c9e2a] is timed-out and will be closed\n",
       "26/02/04 00:09:59 WARN SparkSQLOperationManager: Operation OperationHandle [3a17a526-f5d8-43d2-91d2-7c6827e0ca8a] is timed-out and will be closed\n",
       "2026-02-04T00:10:12,473Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:10:12,473Z WARN KyuubiOperationManager: Operation OperationHandle [74441b4c-0a57-4668-a7e1-d7b8e06c9e2a] is timed-out and will be closed\n",
       "2026-02-04T00:10:12,473Z WARN KyuubiOperationManager: Operation OperationHandle [3a17a526-f5d8-43d2-91d2-7c6827e0ca8a] is timed-out and will be closed\n",
       "2026-02-04T00:10:12,475Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:74 44 1B 4C 0A 57 46 68 A7 E1 D7 B8 E0 6C 9E 2A, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [74441b4c-0a57-4668-a7e1-d7b8e06c9e2a]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:10:12,476Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:3A 17 A5 26 F5 D8 43 D2 91 D2 7C 68 27 E0 CA 8A, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [3a17a526-f5d8-43d2-91d2-7c6827e0ca8a]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:10:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [74441b4c-0a57-4668-a7e1-d7b8e06c9e2a]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:10:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [3a17a526-f5d8-43d2-91d2-7c6827e0ca8a]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:10:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:10:42,515Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "1812.596: [GC (System.gc()) [PSYoungGen: 340624K->21301K(1069056K)] 413146K->93830K(4911616K), 0.0169352 secs] [Times: user=0.06 sys=0.00, real=0.02 secs] \n",
       "1812.613: [Full GC (System.gc()) [PSYoungGen: 21301K->0K(1069056K)] [ParOldGen: 72529K->72384K(3842560K)] 93830K->72384K(4911616K), [Metaspace: 202230K->202204K(1243136K)], 0.4166075 secs] [Times: user=0.77 sys=0.01, real=0.41 secs] \n",
       "26/02/04 00:10:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 100.67.56.160:7079 in memory (size: 6.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:10:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:11:12,516Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:11:15,021Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/f03a94b0-e9f7-4526-81f1-119a4bf81ab6\n",
       "2026-02-04T00:11:15,024Z INFO ExecuteStatement: Processing anonymous's query[f03a94b0-e9f7-4526-81f1-119a4bf81ab6]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:11:15,025Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:11:15,029Z INFO ExecuteStatement: Query[f03a94b0-e9f7-4526-81f1-119a4bf81ab6] in FINISHED_STATE\n",
       "2026-02-04T00:11:15,029Z INFO ExecuteStatement: Processing anonymous's query[f03a94b0-e9f7-4526-81f1-119a4bf81ab6]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 00:11:15 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/f03a94b0-e9f7-4526-81f1-119a4bf81ab6\n",
       "26/02/04 00:11:15 INFO ExecutePython: Processing anonymous's query[f03a94b0-e9f7-4526-81f1-119a4bf81ab6]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:11:15 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:11:15 INFO ExecutePython: Processing anonymous's query[f03a94b0-e9f7-4526-81f1-119a4bf81ab6]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.0 seconds\n",
       "26/02/04 00:11:15 INFO DAGScheduler: Asked to cancel job group f03a94b0-e9f7-4526-81f1-119a4bf81ab6\n",
       "2026-02-04T00:11:15,599Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/b9e198a8-fe00-46bb-93fe-46578be4f5f7\n",
       "2026-02-04T00:11:15,602Z INFO ExecuteStatement: Processing anonymous's query[b9e198a8-fe00-46bb-93fe-46578be4f5f7]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:11:15,602Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:11:15,643Z INFO ExecuteStatement: Query[b9e198a8-fe00-46bb-93fe-46578be4f5f7] in ERROR_STATE\n",
       "2026-02-04T00:11:15,645Z INFO ExecuteStatement: Processing anonymous's query[b9e198a8-fe00-46bb-93fe-46578be4f5f7]: RUNNING_STATE -> ERROR_STATE, time taken: 0.043 seconds\n",
       "2026-02-04T00:11:16,222Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/b9e198a8-fe00-46bb-93fe-46578be4f5f7/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:11:16,372Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/b9e198a8-fe00-46bb-93fe-46578be4f5f7/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:11:15 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/b9e198a8-fe00-46bb-93fe-46578be4f5f7\n",
       "26/02/04 00:11:15 INFO ExecutePython: Processing anonymous's query[b9e198a8-fe00-46bb-93fe-46578be4f5f7]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:11:15 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:11:15 INFO ExecutePython: Processing anonymous's query[b9e198a8-fe00-46bb-93fe-46578be4f5f7]: RUNNING_STATE -> ERROR_STATE, time taken: 0.037 seconds\n",
       "26/02/04 00:11:15 INFO DAGScheduler: Asked to cancel job group b9e198a8-fe00-46bb-93fe-46578be4f5f7\n",
       "26/02/04 00:11:16 INFO DAGScheduler: Asked to cancel job group b9e198a8-fe00-46bb-93fe-46578be4f5f7\n",
       "26/02/04 00:11:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:11:42,516Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:11:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:12:12,517Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:12:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:12:29 WARN SparkSQLOperationManager: Operation OperationHandle [b9e198a8-fe00-46bb-93fe-46578be4f5f7] is timed-out and will be closed\n",
       "26/02/04 00:12:29 WARN SparkSQLOperationManager: Operation OperationHandle [f03a94b0-e9f7-4526-81f1-119a4bf81ab6] is timed-out and will be closed\n",
       "2026-02-04T00:12:42,517Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:12:42,517Z WARN KyuubiOperationManager: Operation OperationHandle [b9e198a8-fe00-46bb-93fe-46578be4f5f7] is timed-out and will be closed\n",
       "2026-02-04T00:12:42,517Z WARN KyuubiOperationManager: Operation OperationHandle [f03a94b0-e9f7-4526-81f1-119a4bf81ab6] is timed-out and will be closed\n",
       "2026-02-04T00:12:42,519Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:B9 E1 98 A8 FE 00 46 BB 93 FE 46 57 8B E4 F5 F7, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [b9e198a8-fe00-46bb-93fe-46578be4f5f7]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:12:42,521Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:F0 3A 94 B0 E9 F7 45 26 81 F1 11 9A 4B F8 1A B6, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [f03a94b0-e9f7-4526-81f1-119a4bf81ab6]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:12:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [b9e198a8-fe00-46bb-93fe-46578be4f5f7]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:12:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [f03a94b0-e9f7-4526-81f1-119a4bf81ab6]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:12:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:13:12,521Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:13:21,588Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/ad5af81e-2ae9-4d52-a1a7-8f8be6790438\n",
       "2026-02-04T00:13:21,591Z INFO ExecuteStatement: Processing anonymous's query[ad5af81e-2ae9-4d52-a1a7-8f8be6790438]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:13:21,591Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:21,595Z INFO ExecuteStatement: Query[ad5af81e-2ae9-4d52-a1a7-8f8be6790438] in FINISHED_STATE\n",
       "2026-02-04T00:13:21,596Z INFO ExecuteStatement: Processing anonymous's query[ad5af81e-2ae9-4d52-a1a7-8f8be6790438]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "2026-02-04T00:13:22,119Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/18ba639b-0608-41d0-ae4c-46e83d313999\n",
       "2026-02-04T00:13:22,122Z INFO ExecuteStatement: Processing anonymous's query[18ba639b-0608-41d0-ae4c-46e83d313999]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:13:22,122Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:13:21 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/ad5af81e-2ae9-4d52-a1a7-8f8be6790438\n",
       "26/02/04 00:13:21 INFO ExecutePython: Processing anonymous's query[ad5af81e-2ae9-4d52-a1a7-8f8be6790438]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:13:21 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:13:21 INFO ExecutePython: Processing anonymous's query[ad5af81e-2ae9-4d52-a1a7-8f8be6790438]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:13:21 INFO DAGScheduler: Asked to cancel job group ad5af81e-2ae9-4d52-a1a7-8f8be6790438\n",
       "26/02/04 00:13:22 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/18ba639b-0608-41d0-ae4c-46e83d313999\n",
       "26/02/04 00:13:22 INFO ExecutePython: Processing anonymous's query[18ba639b-0608-41d0-ae4c-46e83d313999]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:13:22 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:13:22 INFO GpuOverrides: Plan conversion to the GPU took 0.24 ms\n",
       "26/02/04 00:13:22 INFO GpuOverrides: GPU plan transition optimization took 0.16 ms\n",
       "26/02/04 00:13:22 INFO GpuOverrides: Plan conversion to the GPU took 0.28 ms\n",
       "26/02/04 00:13:22 INFO GpuOverrides: GPU plan transition optimization took 0.12 ms\n",
       "26/02/04 00:13:22 WARN HadoopFSUtils: The directory s3://dcartm-team/hongy/maestro_restore/export_TIMESTAMP/slurm_nodes.parquet was not found. Was it deleted very recently?\n",
       "26/02/04 00:13:22 INFO InMemoryFileIndex: It took 82 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:13:22 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 00:13:22 INFO DAGScheduler: Got job 4 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 00:13:22 INFO DAGScheduler: Final stage: ResultStage 6 (sql at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 00:13:22 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:13:22 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:13:22 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[54] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 00:13:22 INFO SQLOperationListener: Query [18ba639b-0608-41d0-ae4c-46e83d313999]: Job 4 started with 1 stages, 1 active jobs running\n",
       "26/02/04 00:13:22 INFO SQLOperationListener: Query [18ba639b-0608-41d0-ae4c-46e83d313999]: Stage 6.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 00:13:22 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:13:22 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:13:22 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:13:22 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:13:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[54] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:13:22 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:13:22 INFO FairSchedulableBuilder: Added task set TaskSet_6.0 tasks to pool \n",
       "2026-02-04T00:13:22,570Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:23,709Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:24,853Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:25,999Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:27,124Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in RUNNING_STATE\n",
       "2026-02-04T00:13:27,140Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:28,286Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:29,442Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:13:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:13:30,585Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:31,728Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:32,125Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in RUNNING_STATE\n",
       "26/02/04 00:13:32 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
       "26/02/04 00:13:32 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
       "26/02/04 00:13:32 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : log4j2.properties,nvoauth.conf,metrics.properties\n",
       "26/02/04 00:13:32 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
       "26/02/04 00:13:32 INFO SparkExecutorFsGroupFeatureStep: SparkExecutorFsGroupFeatureStep configure security context fsGroup\n",
       "2026-02-04T00:13:32,884Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:13:32 INFO ExecutorPodsAllocator: Trying to create PersistentVolumeClaim cluster-20260203202803-yawkv5ak-exec-2-pvc-0 with StorageClass gp3\n",
       "2026-02-04T00:13:34,031Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:35,174Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:36,316Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:37,126Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in RUNNING_STATE\n",
       "2026-02-04T00:13:37,460Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:38,605Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:39,753Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:40,910Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:42,053Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:42,127Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in RUNNING_STATE\n",
       "2026-02-04T00:13:42,522Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:13:43,199Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:44,363Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:45,504Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:46,655Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:47,129Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in RUNNING_STATE\n",
       "2026-02-04T00:13:47,804Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:48,947Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:50,094Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:51,311Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:13:50 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.38.155:53506\n",
       "26/02/04 00:13:51 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (100.67.38.155:53518) with ID 2,  ResourceProfileId 0\n",
       "26/02/04 00:13:51 INFO ExecutorMonitor: New executor 2 has registered (new total is 1)\n",
       "26/02/04 00:13:51 INFO BlockManagerMasterEndpoint: Registering block manager 100.67.38.155:35047 with 9.0 GiB RAM, BlockManagerId(2, 100.67.38.155, 35047, None)\n",
       "2026-02-04T00:13:52,130Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in RUNNING_STATE\n",
       "2026-02-04T00:13:52,466Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:53,619Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:54,762Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:55,906Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:57,049Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:57,131Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in RUNNING_STATE\n",
       "2026-02-04T00:13:58,200Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:13:59,344Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:13:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:14:00 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 9221 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:14:00 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 100.67.38.155:35047 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T00:14:00,498Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:14:00,809Z INFO ExecuteStatement: Query[18ba639b-0608-41d0-ae4c-46e83d313999] in ERROR_STATE\n",
       "2026-02-04T00:14:00,809Z INFO ExecuteStatement: Processing anonymous's query[18ba639b-0608-41d0-ae4c-46e83d313999]: RUNNING_STATE -> ERROR_STATE, time taken: 38.687 seconds\n",
       "26/02/04 00:14:00 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 678 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:14:00 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:14:00 INFO DAGScheduler: ResultStage 6 (sql at NativeMethodAccessorImpl.java:0) finished in 38.442 s\n",
       "26/02/04 00:14:00 INFO SQLOperationListener: Finished stage: Stage(6, 0); Name: 'sql at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 38442 msec\n",
       "26/02/04 00:14:00 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:14:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
       "26/02/04 00:14:00 INFO DAGScheduler: Job 4 finished: sql at NativeMethodAccessorImpl.java:0, took 38.445296 s\n",
       "26/02/04 00:14:00 INFO StatsReportListener: task runtime:(count: 1, mean: 678.000000, stdev: 0.000000, max: 678.000000, min: 678.000000)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t678.0 ms\t678.0 ms\t678.0 ms\t678.0 ms\t678.0 ms\t678.0 ms\t678.0 ms\t678.0 ms\t678.0 ms\n",
       "26/02/04 00:14:00 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:14:00 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 00:14:00 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:14:00 INFO StatsReportListener: task result size:(count: 1, mean: 913.000000, stdev: 0.000000, max: 913.000000, min: 913.000000)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t913.0 B\t913.0 B\t913.0 B\t913.0 B\t913.0 B\t913.0 B\t913.0 B\t913.0 B\t913.0 B\n",
       "26/02/04 00:14:00 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 8.702065, stdev: 0.000000, max: 8.702065, min: 8.702065)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t 9 %\t 9 %\t 9 %\t 9 %\t 9 %\t 9 %\t 9 %\t 9 %\t 9 %\n",
       "26/02/04 00:14:00 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 00:14:00 INFO StatsReportListener: other time pct: (count: 1, mean: 91.297935, stdev: 0.000000, max: 91.297935, min: 91.297935)\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:14:00 INFO StatsReportListener: \t91 %\t91 %\t91 %\t91 %\t91 %\t91 %\t91 %\t91 %\t91 %\n",
       "26/02/04 00:14:00 INFO SparkSQLEngineListener: Job end. Job 4 state is JobSucceeded\n",
       "26/02/04 00:14:00 INFO SQLOperationListener: Query [18ba639b-0608-41d0-ae4c-46e83d313999]: Job 4 succeeded, 0 active jobs running\n",
       "26/02/04 00:14:00 INFO ExecutePython: Processing anonymous's query[18ba639b-0608-41d0-ae4c-46e83d313999]: RUNNING_STATE -> ERROR_STATE, time taken: 38.682 seconds\n",
       "26/02/04 00:14:00 INFO DAGScheduler: Asked to cancel job group 18ba639b-0608-41d0-ae4c-46e83d313999\n",
       "2026-02-04T00:14:01,644Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:14:01,792Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/18ba639b-0608-41d0-ae4c-46e83d313999/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:14:01 INFO DAGScheduler: Asked to cancel job group 18ba639b-0608-41d0-ae4c-46e83d313999\n",
       "2026-02-04T00:14:12,522Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:14:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:14:29 WARN SparkSQLOperationManager: Operation OperationHandle [ad5af81e-2ae9-4d52-a1a7-8f8be6790438] is timed-out and will be closed\n",
       "2026-02-04T00:14:42,523Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:14:42,523Z WARN KyuubiOperationManager: Operation OperationHandle [ad5af81e-2ae9-4d52-a1a7-8f8be6790438] is timed-out and will be closed\n",
       "2026-02-04T00:14:42,525Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:AD 5A F8 1E 2A E9 4D 52 A1 A7 8F 8B E6 79 04 38, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [ad5af81e-2ae9-4d52-a1a7-8f8be6790438]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:14:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [ad5af81e-2ae9-4d52-a1a7-8f8be6790438]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:14:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:15:12,525Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:15:12,526Z WARN KyuubiOperationManager: Operation OperationHandle [18ba639b-0608-41d0-ae4c-46e83d313999] is timed-out and will be closed\n",
       "2026-02-04T00:15:12,527Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:18 BA 63 9B 06 08 41 D0 AE 4C 46 E8 3D 31 39 99, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 00:15:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:15:42,527Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:15:51 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.38.155:53528\n",
       "26/02/04 00:15:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:16:12,528Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:16:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:16:42,528Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:16:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:17:12,528Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:17:18,606Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/97894d30-86aa-4b57-8bc7-1a8275f43798\n",
       "2026-02-04T00:17:18,608Z INFO ExecuteStatement: Processing anonymous's query[97894d30-86aa-4b57-8bc7-1a8275f43798]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:17:18,609Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:17:18,614Z INFO ExecuteStatement: Query[97894d30-86aa-4b57-8bc7-1a8275f43798] in FINISHED_STATE\n",
       "2026-02-04T00:17:18,614Z INFO ExecuteStatement: Processing anonymous's query[97894d30-86aa-4b57-8bc7-1a8275f43798]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T00:17:19,119Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/582a915d-83ba-40e6-a8fb-eba0b6a97e36\n",
       "2026-02-04T00:17:19,121Z INFO ExecuteStatement: Processing anonymous's query[582a915d-83ba-40e6-a8fb-eba0b6a97e36]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:17:19,122Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:17:19,172Z INFO ExecuteStatement: Query[582a915d-83ba-40e6-a8fb-eba0b6a97e36] in ERROR_STATE\n",
       "2026-02-04T00:17:19,172Z INFO ExecuteStatement: Processing anonymous's query[582a915d-83ba-40e6-a8fb-eba0b6a97e36]: RUNNING_STATE -> ERROR_STATE, time taken: 0.051 seconds\n",
       "26/02/04 00:17:18 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/97894d30-86aa-4b57-8bc7-1a8275f43798\n",
       "26/02/04 00:17:18 INFO ExecutePython: Processing anonymous's query[97894d30-86aa-4b57-8bc7-1a8275f43798]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:17:18 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:17:18 INFO ExecutePython: Processing anonymous's query[97894d30-86aa-4b57-8bc7-1a8275f43798]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:17:18 INFO DAGScheduler: Asked to cancel job group 97894d30-86aa-4b57-8bc7-1a8275f43798\n",
       "26/02/04 00:17:19 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/582a915d-83ba-40e6-a8fb-eba0b6a97e36\n",
       "26/02/04 00:17:19 INFO ExecutePython: Processing anonymous's query[582a915d-83ba-40e6-a8fb-eba0b6a97e36]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:17:19 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:17:19 INFO GpuOverrides: Plan conversion to the GPU took 0.20 ms\n",
       "26/02/04 00:17:19 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 00:17:19 INFO ExecutePython: Processing anonymous's query[582a915d-83ba-40e6-a8fb-eba0b6a97e36]: RUNNING_STATE -> ERROR_STATE, time taken: 0.045 seconds\n",
       "26/02/04 00:17:19 INFO DAGScheduler: Asked to cancel job group 582a915d-83ba-40e6-a8fb-eba0b6a97e36\n",
       "2026-02-04T00:17:19,599Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/582a915d-83ba-40e6-a8fb-eba0b6a97e36/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:17:19,755Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/582a915d-83ba-40e6-a8fb-eba0b6a97e36/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:17:19 INFO DAGScheduler: Asked to cancel job group 582a915d-83ba-40e6-a8fb-eba0b6a97e36\n",
       "26/02/04 00:17:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:17:42,529Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:17:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:18:12,529Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:18:19,773Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/6e4ef60a-3a65-48cd-897e-4b2cc960e8b8\n",
       "2026-02-04T00:18:19,775Z INFO ExecuteStatement: Processing anonymous's query[6e4ef60a-3a65-48cd-897e-4b2cc960e8b8]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:18:19,776Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:18:19,781Z INFO ExecuteStatement: Query[6e4ef60a-3a65-48cd-897e-4b2cc960e8b8] in FINISHED_STATE\n",
       "2026-02-04T00:18:19,782Z INFO ExecuteStatement: Processing anonymous's query[6e4ef60a-3a65-48cd-897e-4b2cc960e8b8]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T00:18:20,300Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/866d0ee4-978b-4021-8de7-942f84585853\n",
       "2026-02-04T00:18:20,303Z INFO ExecuteStatement: Processing anonymous's query[866d0ee4-978b-4021-8de7-942f84585853]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:18:20,304Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:18:19 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/6e4ef60a-3a65-48cd-897e-4b2cc960e8b8\n",
       "26/02/04 00:18:19 INFO ExecutePython: Processing anonymous's query[6e4ef60a-3a65-48cd-897e-4b2cc960e8b8]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:18:19 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:18:19 INFO ExecutePython: Processing anonymous's query[6e4ef60a-3a65-48cd-897e-4b2cc960e8b8]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:18:19 INFO DAGScheduler: Asked to cancel job group 6e4ef60a-3a65-48cd-897e-4b2cc960e8b8\n",
       "26/02/04 00:18:20 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/866d0ee4-978b-4021-8de7-942f84585853\n",
       "26/02/04 00:18:20 INFO ExecutePython: Processing anonymous's query[866d0ee4-978b-4021-8de7-942f84585853]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:18:20 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:18:20 INFO GpuOverrides: Plan conversion to the GPU took 0.25 ms\n",
       "26/02/04 00:18:20 INFO GpuOverrides: GPU plan transition optimization took 0.17 ms\n",
       "26/02/04 00:18:20 INFO GpuOverrides: Plan conversion to the GPU took 0.41 ms\n",
       "26/02/04 00:18:20 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "2026-02-04T00:18:20,811Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/866d0ee4-978b-4021-8de7-942f84585853/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:18:20 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:18:20 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 00:18:20 INFO DAGScheduler: Got job 5 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 00:18:20 INFO DAGScheduler: Final stage: ResultStage 7 (sql at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 00:18:20 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:18:20 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:18:20 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[56] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 00:18:20 INFO SQLOperationListener: Query [866d0ee4-978b-4021-8de7-942f84585853]: Job 5 started with 1 stages, 1 active jobs running\n",
       "26/02/04 00:18:20 INFO SQLOperationListener: Query [866d0ee4-978b-4021-8de7-942f84585853]: Stage 7.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 00:18:20 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:18:20 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:18:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:18:20 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:18:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[56] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:18:20 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:18:20 INFO FairSchedulableBuilder: Added task set TaskSet_7.0 tasks to pool \n",
       "26/02/04 00:18:20 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:18:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 100.67.38.155:35047 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T00:18:21,955Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/866d0ee4-978b-4021-8de7-942f84585853/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:18:23,096Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/866d0ee4-978b-4021-8de7-942f84585853/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:18:23,679Z INFO ExecuteStatement: Query[866d0ee4-978b-4021-8de7-942f84585853] in ERROR_STATE\n",
       "2026-02-04T00:18:23,679Z INFO ExecuteStatement: Processing anonymous's query[866d0ee4-978b-4021-8de7-942f84585853]: RUNNING_STATE -> ERROR_STATE, time taken: 3.376 seconds\n",
       "2026-02-04T00:18:24,242Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/866d0ee4-978b-4021-8de7-942f84585853/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:18:24,391Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/866d0ee4-978b-4021-8de7-942f84585853/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:18:23 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 3150 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:18:23 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:18:23 INFO DAGScheduler: ResultStage 7 (sql at NativeMethodAccessorImpl.java:0) finished in 3.165 s\n",
       "26/02/04 00:18:23 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:18:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
       "26/02/04 00:18:23 INFO SQLOperationListener: Finished stage: Stage(7, 0); Name: 'sql at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 3165 msec\n",
       "26/02/04 00:18:23 INFO DAGScheduler: Job 5 finished: sql at NativeMethodAccessorImpl.java:0, took 3.171619 s\n",
       "26/02/04 00:18:23 INFO StatsReportListener: task runtime:(count: 1, mean: 3150.000000, stdev: 0.000000, max: 3150.000000, min: 3150.000000)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t3.2 s\t3.2 s\t3.2 s\t3.2 s\t3.2 s\t3.2 s\t3.2 s\t3.2 s\t3.2 s\n",
       "26/02/04 00:18:23 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:18:23 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 00:18:23 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:18:23 INFO StatsReportListener: task result size:(count: 1, mean: 1975.000000, stdev: 0.000000, max: 1975.000000, min: 1975.000000)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\n",
       "26/02/04 00:18:23 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 98.761905, stdev: 0.000000, max: 98.761905, min: 98.761905)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t99 %\t99 %\t99 %\t99 %\t99 %\t99 %\t99 %\t99 %\t99 %\n",
       "26/02/04 00:18:23 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 00:18:23 INFO StatsReportListener: other time pct: (count: 1, mean: 1.238095, stdev: 0.000000, max: 1.238095, min: 1.238095)\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:18:23 INFO StatsReportListener: \t 1 %\t 1 %\t 1 %\t 1 %\t 1 %\t 1 %\t 1 %\t 1 %\t 1 %\n",
       "26/02/04 00:18:23 INFO SparkSQLEngineListener: Job end. Job 5 state is JobSucceeded\n",
       "26/02/04 00:18:23 INFO SQLOperationListener: Query [866d0ee4-978b-4021-8de7-942f84585853]: Job 5 succeeded, 0 active jobs running\n",
       "26/02/04 00:18:23 INFO ExecutePython: Processing anonymous's query[866d0ee4-978b-4021-8de7-942f84585853]: RUNNING_STATE -> ERROR_STATE, time taken: 3.37 seconds\n",
       "26/02/04 00:18:23 INFO DAGScheduler: Asked to cancel job group 866d0ee4-978b-4021-8de7-942f84585853\n",
       "26/02/04 00:18:24 INFO DAGScheduler: Asked to cancel job group 866d0ee4-978b-4021-8de7-942f84585853\n",
       "26/02/04 00:18:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:18:29 WARN SparkSQLOperationManager: Operation OperationHandle [582a915d-83ba-40e6-a8fb-eba0b6a97e36] is timed-out and will be closed\n",
       "26/02/04 00:18:29 WARN SparkSQLOperationManager: Operation OperationHandle [97894d30-86aa-4b57-8bc7-1a8275f43798] is timed-out and will be closed\n",
       "2026-02-04T00:18:42,529Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:18:42,530Z WARN KyuubiOperationManager: Operation OperationHandle [582a915d-83ba-40e6-a8fb-eba0b6a97e36] is timed-out and will be closed\n",
       "2026-02-04T00:18:42,530Z WARN KyuubiOperationManager: Operation OperationHandle [97894d30-86aa-4b57-8bc7-1a8275f43798] is timed-out and will be closed\n",
       "2026-02-04T00:18:42,532Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:58 2A 91 5D 83 BA 40 E6 A8 FB EB A0 B6 A9 7E 36, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [582a915d-83ba-40e6-a8fb-eba0b6a97e36]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:18:42,533Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:97 89 4D 30 86 AA 4B 57 8B C7 1A 82 75 F4 37 98, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [97894d30-86aa-4b57-8bc7-1a8275f43798]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:18:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [582a915d-83ba-40e6-a8fb-eba0b6a97e36]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:18:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [97894d30-86aa-4b57-8bc7-1a8275f43798]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:18:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:19:12,534Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:19:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:19:29 WARN SparkSQLOperationManager: Operation OperationHandle [866d0ee4-978b-4021-8de7-942f84585853] is timed-out and will be closed\n",
       "26/02/04 00:19:29 WARN SparkSQLOperationManager: Operation OperationHandle [6e4ef60a-3a65-48cd-897e-4b2cc960e8b8] is timed-out and will be closed\n",
       "2026-02-04T00:19:42,534Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:19:42,534Z WARN KyuubiOperationManager: Operation OperationHandle [866d0ee4-978b-4021-8de7-942f84585853] is timed-out and will be closed\n",
       "2026-02-04T00:19:42,534Z WARN KyuubiOperationManager: Operation OperationHandle [6e4ef60a-3a65-48cd-897e-4b2cc960e8b8] is timed-out and will be closed\n",
       "2026-02-04T00:19:42,536Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:86 6D 0E E4 97 8B 40 21 8D E7 94 2F 84 58 58 53, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [866d0ee4-978b-4021-8de7-942f84585853]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:19:42,537Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:6E 4E F6 0A 3A 65 48 CD 89 7E 4B 2C C9 60 E8 B8, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [6e4ef60a-3a65-48cd-897e-4b2cc960e8b8]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:19:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [866d0ee4-978b-4021-8de7-942f84585853]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:19:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [6e4ef60a-3a65-48cd-897e-4b2cc960e8b8]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:19:50,553Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/76b9df53-5471-4a3d-8ec2-107d304faba7\n",
       "2026-02-04T00:19:50,555Z INFO ExecuteStatement: Processing anonymous's query[76b9df53-5471-4a3d-8ec2-107d304faba7]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:19:50,556Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:50,560Z INFO ExecuteStatement: Query[76b9df53-5471-4a3d-8ec2-107d304faba7] in FINISHED_STATE\n",
       "2026-02-04T00:19:50,560Z INFO ExecuteStatement: Processing anonymous's query[76b9df53-5471-4a3d-8ec2-107d304faba7]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "2026-02-04T00:19:51,101Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/a2150688-8725-4e6f-af65-2cb2315095c5\n",
       "2026-02-04T00:19:51,103Z INFO ExecuteStatement: Processing anonymous's query[a2150688-8725-4e6f-af65-2cb2315095c5]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:19:51,104Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:19:50 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/76b9df53-5471-4a3d-8ec2-107d304faba7\n",
       "26/02/04 00:19:50 INFO ExecutePython: Processing anonymous's query[76b9df53-5471-4a3d-8ec2-107d304faba7]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:19:50 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:19:50 INFO ExecutePython: Processing anonymous's query[76b9df53-5471-4a3d-8ec2-107d304faba7]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.0 seconds\n",
       "26/02/04 00:19:50 INFO DAGScheduler: Asked to cancel job group 76b9df53-5471-4a3d-8ec2-107d304faba7\n",
       "26/02/04 00:19:51 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/a2150688-8725-4e6f-af65-2cb2315095c5\n",
       "26/02/04 00:19:51 INFO ExecutePython: Processing anonymous's query[a2150688-8725-4e6f-af65-2cb2315095c5]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:19:51 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:19:51 INFO GpuOverrides: Plan conversion to the GPU took 0.29 ms\n",
       "26/02/04 00:19:51 INFO GpuOverrides: GPU plan transition optimization took 0.23 ms\n",
       "2026-02-04T00:19:51,679Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:52,823Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:53,973Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:55,121Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:56,106Z INFO ExecuteStatement: Query[a2150688-8725-4e6f-af65-2cb2315095c5] in RUNNING_STATE\n",
       "2026-02-04T00:19:56,356Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:57,500Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:58,643Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:19:59,925Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:19:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:20:01,090Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:01,107Z INFO ExecuteStatement: Query[a2150688-8725-4e6f-af65-2cb2315095c5] in RUNNING_STATE\n",
       "2026-02-04T00:20:02,245Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:03,454Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:04,650Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:05,859Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:06,108Z INFO ExecuteStatement: Query[a2150688-8725-4e6f-af65-2cb2315095c5] in RUNNING_STATE\n",
       "2026-02-04T00:20:07,007Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:08,149Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:09,297Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:10,506Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:11,110Z INFO ExecuteStatement: Query[a2150688-8725-4e6f-af65-2cb2315095c5] in RUNNING_STATE\n",
       "2026-02-04T00:20:11,650Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:12,538Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:20:12,810Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:13,987Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:14,196Z INFO ExecuteStatement: Query[a2150688-8725-4e6f-af65-2cb2315095c5] in ERROR_STATE\n",
       "2026-02-04T00:20:14,196Z INFO ExecuteStatement: Processing anonymous's query[a2150688-8725-4e6f-af65-2cb2315095c5]: RUNNING_STATE -> ERROR_STATE, time taken: 23.092 seconds\n",
       "26/02/04 00:20:14 INFO ExecutePython: Processing anonymous's query[a2150688-8725-4e6f-af65-2cb2315095c5]: RUNNING_STATE -> ERROR_STATE, time taken: 23.088 seconds\n",
       "26/02/04 00:20:14 INFO DAGScheduler: Asked to cancel job group a2150688-8725-4e6f-af65-2cb2315095c5\n",
       "2026-02-04T00:20:15,129Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:20:15,330Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/a2150688-8725-4e6f-af65-2cb2315095c5/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:20:15 INFO DAGScheduler: Asked to cancel job group a2150688-8725-4e6f-af65-2cb2315095c5\n",
       "26/02/04 00:20:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:20:42,538Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:20:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:20:59 WARN SparkSQLOperationManager: Operation OperationHandle [76b9df53-5471-4a3d-8ec2-107d304faba7] is timed-out and will be closed\n",
       "2026-02-04T00:21:03,673Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/3e063550-3584-4d0b-82ee-d9752c57b5ab\n",
       "2026-02-04T00:21:03,675Z INFO ExecuteStatement: Processing anonymous's query[3e063550-3584-4d0b-82ee-d9752c57b5ab]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:21:03,675Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:21:03,679Z INFO ExecuteStatement: Query[3e063550-3584-4d0b-82ee-d9752c57b5ab] in FINISHED_STATE\n",
       "2026-02-04T00:21:03,680Z INFO ExecuteStatement: Processing anonymous's query[3e063550-3584-4d0b-82ee-d9752c57b5ab]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T00:21:04,220Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/19c96f55-ba21-4a99-aaf5-53c038a01a61\n",
       "2026-02-04T00:21:04,222Z INFO ExecuteStatement: Processing anonymous's query[19c96f55-ba21-4a99-aaf5-53c038a01a61]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:21:04,223Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:21:03 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/3e063550-3584-4d0b-82ee-d9752c57b5ab\n",
       "26/02/04 00:21:03 INFO ExecutePython: Processing anonymous's query[3e063550-3584-4d0b-82ee-d9752c57b5ab]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:21:03 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:21:03 INFO ExecutePython: Processing anonymous's query[3e063550-3584-4d0b-82ee-d9752c57b5ab]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:21:03 INFO DAGScheduler: Asked to cancel job group 3e063550-3584-4d0b-82ee-d9752c57b5ab\n",
       "26/02/04 00:21:04 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/19c96f55-ba21-4a99-aaf5-53c038a01a61\n",
       "26/02/04 00:21:04 INFO ExecutePython: Processing anonymous's query[19c96f55-ba21-4a99-aaf5-53c038a01a61]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:21:04 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:21:04 INFO GpuOverrides: Plan conversion to the GPU took 0.24 ms\n",
       "26/02/04 00:21:04 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "2026-02-04T00:21:04,726Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/19c96f55-ba21-4a99-aaf5-53c038a01a61/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:21:04,977Z INFO ExecuteStatement: Query[19c96f55-ba21-4a99-aaf5-53c038a01a61] in ERROR_STATE\n",
       "2026-02-04T00:21:04,977Z INFO ExecuteStatement: Processing anonymous's query[19c96f55-ba21-4a99-aaf5-53c038a01a61]: RUNNING_STATE -> ERROR_STATE, time taken: 0.754 seconds\n",
       "26/02/04 00:21:04 INFO GpuOverrides: Plan conversion to the GPU took 0.29 ms\n",
       "26/02/04 00:21:04 INFO GpuOverrides: GPU plan transition optimization took 0.14 ms\n",
       "26/02/04 00:21:04 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:21:04 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Got job 6 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Final stage: ResultStage 8 (sql at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[58] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 00:21:04 INFO SQLOperationListener: Query [19c96f55-ba21-4a99-aaf5-53c038a01a61]: Job 6 started with 1 stages, 1 active jobs running\n",
       "26/02/04 00:21:04 INFO SQLOperationListener: Query [19c96f55-ba21-4a99-aaf5-53c038a01a61]: Stage 8.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 00:21:04 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:21:04 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:21:04 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:21:04 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[58] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:21:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:21:04 INFO FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool \n",
       "26/02/04 00:21:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:21:04 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 100.67.38.155:35047 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:21:04 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 159 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:21:04 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:21:04 INFO DAGScheduler: ResultStage 8 (sql at NativeMethodAccessorImpl.java:0) finished in 0.174 s\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:21:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
       "26/02/04 00:21:04 INFO SQLOperationListener: Finished stage: Stage(8, 0); Name: 'sql at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 174 msec\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Job 6 finished: sql at NativeMethodAccessorImpl.java:0, took 0.178270 s\n",
       "26/02/04 00:21:04 INFO StatsReportListener: task runtime:(count: 1, mean: 159.000000, stdev: 0.000000, max: 159.000000, min: 159.000000)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t159.0 ms\t159.0 ms\t159.0 ms\t159.0 ms\t159.0 ms\t159.0 ms\t159.0 ms\t159.0 ms\t159.0 ms\n",
       "26/02/04 00:21:04 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:21:04 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 00:21:04 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:21:04 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 00:21:04 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 81.761006, stdev: 0.000000, max: 81.761006, min: 81.761006)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\n",
       "26/02/04 00:21:04 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 00:21:04 INFO StatsReportListener: other time pct: (count: 1, mean: 18.238994, stdev: 0.000000, max: 18.238994, min: 18.238994)\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:04 INFO StatsReportListener: \t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\n",
       "26/02/04 00:21:04 INFO SparkSQLEngineListener: Job end. Job 6 state is JobSucceeded\n",
       "26/02/04 00:21:04 INFO SQLOperationListener: Query [19c96f55-ba21-4a99-aaf5-53c038a01a61]: Job 6 succeeded, 0 active jobs running\n",
       "26/02/04 00:21:04 INFO ExecutePython: Processing anonymous's query[19c96f55-ba21-4a99-aaf5-53c038a01a61]: RUNNING_STATE -> ERROR_STATE, time taken: 0.75 seconds\n",
       "26/02/04 00:21:04 INFO DAGScheduler: Asked to cancel job group 19c96f55-ba21-4a99-aaf5-53c038a01a61\n",
       "2026-02-04T00:21:05,874Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/19c96f55-ba21-4a99-aaf5-53c038a01a61/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:21:06,024Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/19c96f55-ba21-4a99-aaf5-53c038a01a61/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:21:06 INFO DAGScheduler: Asked to cancel job group 19c96f55-ba21-4a99-aaf5-53c038a01a61\n",
       "2026-02-04T00:21:12,538Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:21:12,539Z WARN KyuubiOperationManager: Operation OperationHandle [76b9df53-5471-4a3d-8ec2-107d304faba7] is timed-out and will be closed\n",
       "2026-02-04T00:21:12,541Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:76 B9 DF 53 54 71 4A 3D 8E C2 10 7D 30 4F AB A7, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [76b9df53-5471-4a3d-8ec2-107d304faba7]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:21:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [76b9df53-5471-4a3d-8ec2-107d304faba7]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:21:18,023Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/f6495948-d27a-497f-866a-0b8ec0e431a2\n",
       "2026-02-04T00:21:18,025Z INFO ExecuteStatement: Processing anonymous's query[f6495948-d27a-497f-866a-0b8ec0e431a2]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:21:18,025Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:21:18,030Z INFO ExecuteStatement: Query[f6495948-d27a-497f-866a-0b8ec0e431a2] in FINISHED_STATE\n",
       "2026-02-04T00:21:18,030Z INFO ExecuteStatement: Processing anonymous's query[f6495948-d27a-497f-866a-0b8ec0e431a2]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 00:21:18 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/f6495948-d27a-497f-866a-0b8ec0e431a2\n",
       "26/02/04 00:21:18 INFO ExecutePython: Processing anonymous's query[f6495948-d27a-497f-866a-0b8ec0e431a2]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:21:18 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:21:18 INFO ExecutePython: Processing anonymous's query[f6495948-d27a-497f-866a-0b8ec0e431a2]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Asked to cancel job group f6495948-d27a-497f-866a-0b8ec0e431a2\n",
       "2026-02-04T00:21:18,564Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/930999e5-bd78-447d-ba53-76e113e8be19\n",
       "2026-02-04T00:21:18,567Z INFO ExecuteStatement: Processing anonymous's query[930999e5-bd78-447d-ba53-76e113e8be19]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:21:18,567Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/f7df5191-9229-40df-86fa-ee5b5502e2f5/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:21:19,050Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/930999e5-bd78-447d-ba53-76e113e8be19/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:21:19,431Z INFO ExecuteStatement: Query[930999e5-bd78-447d-ba53-76e113e8be19] in FINISHED_STATE\n",
       "2026-02-04T00:21:19,431Z INFO ExecuteStatement: Processing anonymous's query[930999e5-bd78-447d-ba53-76e113e8be19]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.864 seconds\n",
       "26/02/04 00:21:18 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/f7df5191-9229-40df-86fa-ee5b5502e2f5/930999e5-bd78-447d-ba53-76e113e8be19\n",
       "26/02/04 00:21:18 INFO ExecutePython: Processing anonymous's query[930999e5-bd78-447d-ba53-76e113e8be19]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:21:18 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:21:18 INFO GpuOverrides: Plan conversion to the GPU took 0.27 ms\n",
       "26/02/04 00:21:18 INFO GpuOverrides: GPU plan transition optimization took 0.19 ms\n",
       "26/02/04 00:21:18 INFO GpuOverrides: Plan conversion to the GPU took 0.38 ms\n",
       "26/02/04 00:21:18 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "26/02/04 00:21:18 INFO InMemoryFileIndex: It took 36 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:21:18 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Got job 7 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Final stage: ResultStage 9 (sql at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[60] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 00:21:18 INFO SQLOperationListener: Query [930999e5-bd78-447d-ba53-76e113e8be19]: Job 7 started with 1 stages, 1 active jobs running\n",
       "26/02/04 00:21:18 INFO SQLOperationListener: Query [930999e5-bd78-447d-ba53-76e113e8be19]: Stage 9.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 00:21:18 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:21:18 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:21:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:21:18 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[60] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:21:18 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:21:18 INFO FairSchedulableBuilder: Added task set TaskSet_9.0 tasks to pool \n",
       "26/02/04 00:21:18 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:21:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 100.67.38.155:35047 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:21:18 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 142 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:21:18 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:21:18 INFO DAGScheduler: ResultStage 9 (sql at NativeMethodAccessorImpl.java:0) finished in 0.157 s\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:21:18 INFO SQLOperationListener: Finished stage: Stage(9, 0); Name: 'sql at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 157 msec\n",
       "26/02/04 00:21:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
       "26/02/04 00:21:18 INFO DAGScheduler: Job 7 finished: sql at NativeMethodAccessorImpl.java:0, took 0.159732 s\n",
       "26/02/04 00:21:18 INFO StatsReportListener: task runtime:(count: 1, mean: 142.000000, stdev: 0.000000, max: 142.000000, min: 142.000000)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t142.0 ms\t142.0 ms\t142.0 ms\t142.0 ms\t142.0 ms\t142.0 ms\t142.0 ms\t142.0 ms\t142.0 ms\n",
       "26/02/04 00:21:18 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:21:18 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 00:21:18 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:21:18 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 00:21:18 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 79.577465, stdev: 0.000000, max: 79.577465, min: 79.577465)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\n",
       "26/02/04 00:21:18 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 00:21:18 INFO StatsReportListener: other time pct: (count: 1, mean: 20.422535, stdev: 0.000000, max: 20.422535, min: 20.422535)\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:21:18 INFO StatsReportListener: \t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\n",
       "26/02/04 00:21:18 INFO SparkSQLEngineListener: Job end. Job 7 state is JobSucceeded\n",
       "26/02/04 00:21:18 INFO SQLOperationListener: Query [930999e5-bd78-447d-ba53-76e113e8be19]: Job 7 succeeded, 0 active jobs running\n",
       "26/02/04 00:21:18 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 00:21:18 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 61 more\n",
       "26/02/04 00:21:19 INFO ExecutePython: DataFrame[]\n",
       "26/02/04 00:21:19 INFO ExecutePython: Processing anonymous's query[930999e5-bd78-447d-ba53-76e113e8be19]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.86 seconds\n",
       "26/02/04 00:21:19 INFO DAGScheduler: Asked to cancel job group 930999e5-bd78-447d-ba53-76e113e8be19\n",
       "2026-02-04T00:21:20,194Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/930999e5-bd78-447d-ba53-76e113e8be19/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:21:20,342Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/930999e5-bd78-447d-ba53-76e113e8be19/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:21:20 INFO DAGScheduler: Asked to cancel job group 930999e5-bd78-447d-ba53-76e113e8be19\n",
       "26/02/04 00:21:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:21:29 WARN SparkSQLOperationManager: Operation OperationHandle [a2150688-8725-4e6f-af65-2cb2315095c5] is timed-out and will be closed\n",
       "2026-02-04T00:21:42,541Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:21:42,542Z WARN KyuubiOperationManager: Operation OperationHandle [a2150688-8725-4e6f-af65-2cb2315095c5] is timed-out and will be closed\n",
       "2026-02-04T00:21:42,543Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:A2 15 06 88 87 25 4E 6F AF 65 2C B2 31 50 95 C5, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [a2150688-8725-4e6f-af65-2cb2315095c5]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:21:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [a2150688-8725-4e6f-af65-2cb2315095c5]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:21:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:22:12,544Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:22:12,544Z WARN KyuubiOperationManager: Operation OperationHandle [19c96f55-ba21-4a99-aaf5-53c038a01a61] is timed-out and will be closed\n",
       "2026-02-04T00:22:12,544Z WARN KyuubiOperationManager: Operation OperationHandle [3e063550-3584-4d0b-82ee-d9752c57b5ab] is timed-out and will be closed\n",
       "2026-02-04T00:22:12,546Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:19 C9 6F 55 BA 21 4A 99 AA F5 53 C0 38 A0 1A 61, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T00:22:12,547Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:3E 06 35 50 35 84 4D 0B 82 EE D9 75 2C 57 B5 AB, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 00:22:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:22:29 WARN SparkSQLOperationManager: Operation OperationHandle [930999e5-bd78-447d-ba53-76e113e8be19] is timed-out and will be closed\n",
       "26/02/04 00:22:29 WARN SparkSQLOperationManager: Operation OperationHandle [f6495948-d27a-497f-866a-0b8ec0e431a2] is timed-out and will be closed\n",
       "2026-02-04T00:22:42,547Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T00:22:42,548Z WARN KyuubiOperationManager: Operation OperationHandle [930999e5-bd78-447d-ba53-76e113e8be19] is timed-out and will be closed\n",
       "2026-02-04T00:22:42,548Z WARN KyuubiOperationManager: Operation OperationHandle [f6495948-d27a-497f-866a-0b8ec0e431a2] is timed-out and will be closed\n",
       "2026-02-04T00:22:42,549Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:93 09 99 E5 BD 78 44 7D BA 53 76 E1 13 E8 BE 19, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [930999e5-bd78-447d-ba53-76e113e8be19]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:22:42,551Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:F6 49 59 48 D2 7A 49 7F 86 6A 0B 8E C0 E4 31 A2, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [f6495948-d27a-497f-866a-0b8ec0e431a2]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:22:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [930999e5-bd78-447d-ba53-76e113e8be19]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:22:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [f6495948-d27a-497f-866a-0b8ec0e431a2]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:22:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:23:12,552Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:23:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:23:42,552Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:23:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:24:12,552Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:24:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:24:42,553Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:24:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:25:12,553Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:25:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:25:42,553Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:25:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:26:12,554Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:26:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:26:42,554Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:26:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:27:12,554Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:27:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:27:42,555Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:27:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:28:12,555Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:28:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:28:42,555Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:28:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:29:12,556Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:29:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:29:42,556Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:29:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:30:12,556Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:30:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:30:42,557Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:30:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:31:12,557Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:31:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:31:42,557Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:31:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:32:12,558Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 00:32:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:32:39,628Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T00:32:39,628Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T00:32:39,629Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T00:32:39,629Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4c4a95f7\n",
       "2026-02-04T00:32:39,629Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T00:32:39,629Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T00:32:39,630Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T00:32:39,630Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:32:39,632Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-04T00:32:39,632Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T00:32:39,633Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:42248, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-04T00:32:39,637Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b568cc, negotiated timeout = 120000\n",
       "2026-02-04T00:32:39,638Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T00:32:39,639Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:32:39,641Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:32:39,641Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T00:32:39,641Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:32:39,745Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b568cc\n",
       "2026-02-04T00:32:39,745Z INFO ZooKeeper: Session: 0x30263a585b568cc closed\n",
       "2026-02-04T00:32:39,753Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:39,987Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.107.230\n",
       "2026-02-04T00:32:39,987Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T00:32:39,987Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T00:32:39,988Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/26f0645d-4ded-4d87-9a93-b5cb27c482e8\n",
       "2026-02-04T00:32:39,988Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]/kernel-v382dc89fda55e489b27f77a22164229a625d88e58 is opened, current opening sessions 3\n",
       "2026-02-04T00:32:39,989Z INFO LaunchEngine: Processing anonymous's query[26f0645d-4ded-4d87-9a93-b5cb27c482e8]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:32:39,989Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:39,989Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T00:32:39,989Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@29520d05\n",
       "2026-02-04T00:32:39,990Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T00:32:39,990Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T00:32:39,990Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-04T00:32:39,990Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T00:32:39,991Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T00:32:39,993Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:42252, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-04T00:32:39,997Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b568cf, negotiated timeout = 120000\n",
       "2026-02-04T00:32:39,997Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T00:32:39,999Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:32:39,999Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:32:40,001Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:32:40,015Z INFO KyuubiSessionImpl: [anonymous:100.67.107.230] SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]]\n",
       "2026-02-04T00:32:40,015Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T00:32:40,119Z INFO ZooKeeper: Session: 0x30263a585b568cf closed\n",
       "2026-02-04T00:32:40,119Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b568cf\n",
       "2026-02-04T00:32:40,120Z INFO LaunchEngine: Processing anonymous's query[26f0645d-4ded-4d87-9a93-b5cb27c482e8]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.131 seconds\n",
       "2026-02-04T00:32:40,199Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204003239-qlhxoxm6]: associated with Kyuubi SessionHandle: [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]\n",
       "2026-02-04T00:32:40,199Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/718294ee-b0bb-43bc-af9d-77d2c890c52b\n",
       "2026-02-04T00:32:40,200Z INFO KyuubiSessionImpl: [anonymous:100.67.107.230] SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T00:32:40,200Z INFO KyuubiSessionImpl: [anonymous:100.67.107.230] SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-04T00:32:40,206Z INFO ExecuteStatement: Processing anonymous's query[718294ee-b0bb-43bc-af9d-77d2c890c52b]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:32:40,206Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204003239-qlhxoxm6\n",
       "2026-02-04T00:32:40,210Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/718294ee-b0bb-43bc-af9d-77d2c890c52b/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:32:40 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 00:32:40 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 00:32:40 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]\n",
       "26/02/04 00:32:40 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]/kernel-v382dc89fda55e489b27f77a22164229a625d88e58 is opened, current opening sessions 5\n",
       "26/02/04 00:32:40 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 00:32:40 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 00:32:40 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [c0907992-9162-4a68-84d0-6dd58873e251]\n",
       "26/02/04 00:32:40 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [c0907992-9162-4a68-84d0-6dd58873e251]/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b_aliveness_probe is opened, current opening sessions 6\n",
       "26/02/04 00:32:40 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204003239-qlhxoxm6)]: associated with spark-sql operation session: [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]\n",
       "26/02/04 00:32:40 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-252bd97e-7af7-41da-a638-d039c911365a/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:2:5:2:b:d:9:7:e:-:7:a:f:7:-:4:1:d:a:-:a:6:3:8:-:d:0:3:9:c:9:1:1:3:6:5:a\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-252bd97e-7af7-41da-a638-d039c911365a/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=eb9b5e08-fcce-4e25-bf46-bccbda8aed8b\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/04 00:32:40 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/718294ee-b0bb-43bc-af9d-77d2c890c52b\n",
       "2026-02-04T00:32:40,662Z INFO ExecuteStatement: Query[718294ee-b0bb-43bc-af9d-77d2c890c52b] in FINISHED_STATE\n",
       "2026-02-04T00:32:40,662Z INFO ExecuteStatement: Processing anonymous's query[718294ee-b0bb-43bc-af9d-77d2c890c52b]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.456 seconds\n",
       "2026-02-04T00:32:41,215Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/718294ee-b0bb-43bc-af9d-77d2c890c52b/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:41,225Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/718294ee-b0bb-43bc-af9d-77d2c890c52b/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:41,369Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/7c0aa26a-8cf6-4513-a7cb-5e2873c918c9\n",
       "2026-02-04T00:32:41,371Z INFO ExecuteStatement: Processing anonymous's query[7c0aa26a-8cf6-4513-a7cb-5e2873c918c9]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:32:41,371Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:41,375Z INFO ExecuteStatement: Query[7c0aa26a-8cf6-4513-a7cb-5e2873c918c9] in FINISHED_STATE\n",
       "2026-02-04T00:32:41,375Z INFO ExecuteStatement: Processing anonymous's query[7c0aa26a-8cf6-4513-a7cb-5e2873c918c9]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "26/02/04 00:32:40 INFO ExecutePython: Processing anonymous's query[718294ee-b0bb-43bc-af9d-77d2c890c52b]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:32:40 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:32:40 INFO ExecutePython: Processing anonymous's query[718294ee-b0bb-43bc-af9d-77d2c890c52b]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.008 seconds\n",
       "26/02/04 00:32:40 INFO DAGScheduler: Asked to cancel job group 718294ee-b0bb-43bc-af9d-77d2c890c52b\n",
       "26/02/04 00:32:41 INFO DAGScheduler: Asked to cancel job group 718294ee-b0bb-43bc-af9d-77d2c890c52b\n",
       "26/02/04 00:32:41 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/7c0aa26a-8cf6-4513-a7cb-5e2873c918c9\n",
       "26/02/04 00:32:41 INFO ExecutePython: Processing anonymous's query[7c0aa26a-8cf6-4513-a7cb-5e2873c918c9]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:32:41 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:32:41 INFO ExecutePython: Processing anonymous's query[7c0aa26a-8cf6-4513-a7cb-5e2873c918c9]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:32:41 INFO DAGScheduler: Asked to cancel job group 7c0aa26a-8cf6-4513-a7cb-5e2873c918c9\n",
       "2026-02-04T00:32:41,903Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/dae50191-6a60-401c-924e-59ffa9753709\n",
       "2026-02-04T00:32:41,905Z INFO ExecuteStatement: Processing anonymous's query[dae50191-6a60-401c-924e-59ffa9753709]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:32:41,906Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:41,951Z INFO ExecuteStatement: Query[dae50191-6a60-401c-924e-59ffa9753709] in FINISHED_STATE\n",
       "2026-02-04T00:32:41,952Z INFO ExecuteStatement: Processing anonymous's query[dae50191-6a60-401c-924e-59ffa9753709]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.046 seconds\n",
       "2026-02-04T00:32:42,353Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/dae50191-6a60-401c-924e-59ffa9753709/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:32:41 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/dae50191-6a60-401c-924e-59ffa9753709\n",
       "26/02/04 00:32:41 INFO ExecutePython: Processing anonymous's query[dae50191-6a60-401c-924e-59ffa9753709]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:32:41 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:32:41 INFO ExecutePython: Processing anonymous's query[dae50191-6a60-401c-924e-59ffa9753709]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.043 seconds\n",
       "26/02/04 00:32:41 INFO DAGScheduler: Asked to cancel job group dae50191-6a60-401c-924e-59ffa9753709\n",
       "2026-02-04T00:32:42,514Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/dae50191-6a60-401c-924e-59ffa9753709/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:42,558Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 00:32:42 INFO DAGScheduler: Asked to cancel job group dae50191-6a60-401c-924e-59ffa9753709\n",
       "2026-02-04T00:32:44,663Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/2e345116-c68a-41ef-b4f4-d6243131f21c\n",
       "2026-02-04T00:32:44,665Z INFO ExecuteStatement: Processing anonymous's query[2e345116-c68a-41ef-b4f4-d6243131f21c]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:32:44,666Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:44,669Z INFO ExecuteStatement: Query[2e345116-c68a-41ef-b4f4-d6243131f21c] in FINISHED_STATE\n",
       "2026-02-04T00:32:44,670Z INFO ExecuteStatement: Processing anonymous's query[2e345116-c68a-41ef-b4f4-d6243131f21c]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "2026-02-04T00:32:45,195Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/9b9b251e-5868-4161-8c1d-60b8aaf4a39d\n",
       "2026-02-04T00:32:45,198Z INFO ExecuteStatement: Processing anonymous's query[9b9b251e-5868-4161-8c1d-60b8aaf4a39d]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:32:45,198Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:32:44 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/2e345116-c68a-41ef-b4f4-d6243131f21c\n",
       "26/02/04 00:32:44 INFO ExecutePython: Processing anonymous's query[2e345116-c68a-41ef-b4f4-d6243131f21c]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:32:44 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:32:44 INFO ExecutePython: Processing anonymous's query[2e345116-c68a-41ef-b4f4-d6243131f21c]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.0 seconds\n",
       "26/02/04 00:32:44 INFO DAGScheduler: Asked to cancel job group 2e345116-c68a-41ef-b4f4-d6243131f21c\n",
       "26/02/04 00:32:45 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b/9b9b251e-5868-4161-8c1d-60b8aaf4a39d\n",
       "26/02/04 00:32:45 INFO ExecutePython: Processing anonymous's query[9b9b251e-5868-4161-8c1d-60b8aaf4a39d]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:32:45 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:32:45 INFO GpuOverrides: Plan conversion to the GPU took 0.27 ms\n",
       "26/02/04 00:32:45 INFO GpuOverrides: GPU plan transition optimization took 0.17 ms\n",
       "2026-02-04T00:32:45,756Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/9b9b251e-5868-4161-8c1d-60b8aaf4a39d/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:32:45 INFO GpuOverrides: Plan conversion to the GPU took 0.43 ms\n",
       "26/02/04 00:32:45 INFO GpuOverrides: GPU plan transition optimization took 0.18 ms\n",
       "26/02/04 00:32:45 INFO InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:32:45 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 00:32:45 INFO DAGScheduler: Got job 8 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 00:32:45 INFO DAGScheduler: Final stage: ResultStage 10 (sql at <unknown>:0)\n",
       "26/02/04 00:32:45 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:32:45 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:32:45 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[62] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 00:32:45 INFO SQLOperationListener: Query [9b9b251e-5868-4161-8c1d-60b8aaf4a39d]: Job 8 started with 1 stages, 1 active jobs running\n",
       "3120.942: [GC (Allocation Failure) [PSYoungGen: 1016832K->8612K(1014784K)] 1089216K->81004K(4857344K), 0.0273841 secs] [Times: user=0.02 sys=0.04, real=0.03 secs] \n",
       "26/02/04 00:32:45 INFO SQLOperationListener: Query [9b9b251e-5868-4161-8c1d-60b8aaf4a39d]: Stage 10.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:32:45 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:32:45 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:32:45 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:32:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[62] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:32:45 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:32:45 INFO FairSchedulableBuilder: Added task set TaskSet_10.0 tasks to pool \n",
       "26/02/04 00:32:45 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 100.67.38.155:35047 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 100.67.38.155:35047 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 100.67.38.155:35047 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 100.67.38.155:35047 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:32:45 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 100.67.38.155:35047 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:32:46 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 133 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:32:46 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:32:46 INFO DAGScheduler: ResultStage 10 (sql at <unknown>:0) finished in 0.190 s\n",
       "26/02/04 00:32:46 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:32:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
       "26/02/04 00:32:46 INFO DAGScheduler: Job 8 finished: sql at <unknown>:0, took 0.193535 s\n",
       "26/02/04 00:32:46 INFO SQLOperationListener: Finished stage: Stage(10, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 190 msec\n",
       "26/02/04 00:32:46 INFO StatsReportListener: task runtime:(count: 1, mean: 133.000000, stdev: 0.000000, max: 133.000000, min: 133.000000)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t133.0 ms\t133.0 ms\t133.0 ms\t133.0 ms\t133.0 ms\t133.0 ms\t133.0 ms\t133.0 ms\t133.0 ms\n",
       "26/02/04 00:32:46 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:32:46 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 00:32:46 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:32:46 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 00:32:46 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 79.699248, stdev: 0.000000, max: 79.699248, min: 79.699248)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\n",
       "26/02/04 00:32:46 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 00:32:46 INFO StatsReportListener: other time pct: (count: 1, mean: 20.300752, stdev: 0.000000, max: 20.300752, min: 20.300752)\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:32:46 INFO StatsReportListener: \t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\n",
       "26/02/04 00:32:46 INFO SparkSQLEngineListener: Job end. Job 8 state is JobSucceeded\n",
       "26/02/04 00:32:46 INFO SQLOperationListener: Query [9b9b251e-5868-4161-8c1d-60b8aaf4a39d]: Job 8 succeeded, 0 active jobs running\n",
       "26/02/04 00:32:46 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 00:32:46 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "2026-02-04T00:32:46,789Z INFO ExecuteStatement: Query[9b9b251e-5868-4161-8c1d-60b8aaf4a39d] in FINISHED_STATE\n",
       "2026-02-04T00:32:46,789Z INFO ExecuteStatement: Processing anonymous's query[9b9b251e-5868-4161-8c1d-60b8aaf4a39d]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.591 seconds\n",
       "2026-02-04T00:32:46,899Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/9b9b251e-5868-4161-8c1d-60b8aaf4a39d/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:32:47,055Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/9b9b251e-5868-4161-8c1d-60b8aaf4a39d/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:32:46 INFO GpuOverrides: Plan conversion to the GPU took 0.34 ms\n",
       "26/02/04 00:32:46 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 00:32:46 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#350 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#351 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#352 could run on GPU\n",
       "\n",
       "26/02/04 00:32:46 INFO GpuOverrides: Plan conversion to the GPU took 1.13 ms\n",
       "26/02/04 00:32:46 INFO GpuOverrides: GPU plan transition optimization took 0.20 ms\n",
       "26/02/04 00:32:46 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:32:46 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 00:32:46 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:32:46 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 00:32:46 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:32:46 INFO ExecutePython: Processing anonymous's query[9b9b251e-5868-4161-8c1d-60b8aaf4a39d]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.585 seconds\n",
       "26/02/04 00:32:46 INFO DAGScheduler: Asked to cancel job group 9b9b251e-5868-4161-8c1d-60b8aaf4a39d\n",
       "26/02/04 00:32:47 INFO DAGScheduler: Asked to cancel job group 9b9b251e-5868-4161-8c1d-60b8aaf4a39d\n",
       "26/02/04 00:32:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T00:33:12,559Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 00:33:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T00:33:42,559Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "2026-02-04T00:33:42,559Z WARN KyuubiOperationManager: Operation OperationHandle [7c0aa26a-8cf6-4513-a7cb-5e2873c918c9] is timed-out and will be closed\n",
       "2026-02-04T00:33:42,559Z WARN KyuubiOperationManager: Operation OperationHandle [26f0645d-4ded-4d87-9a93-b5cb27c482e8] is timed-out and will be closed\n",
       "2026-02-04T00:33:42,559Z WARN KyuubiOperationManager: Operation OperationHandle [718294ee-b0bb-43bc-af9d-77d2c890c52b] is timed-out and will be closed\n",
       "2026-02-04T00:33:42,559Z WARN KyuubiOperationManager: Operation OperationHandle [dae50191-6a60-401c-924e-59ffa9753709] is timed-out and will be closed\n",
       "2026-02-04T00:33:42,561Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:7C 0A A2 6A 8C F6 45 13 A7 CB 5E 28 73 C9 18 C9, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T00:33:42,563Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:71 82 94 EE B0 BB 43 BC AF 9D 77 D2 C8 90 C5 2B, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T00:33:42,564Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:DA E5 01 91 6A 60 40 1C 92 4E 59 FF A9 75 37 09, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 00:33:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 00:33:59 WARN SparkSQLOperationManager: Operation OperationHandle [2e345116-c68a-41ef-b4f4-d6243131f21c] is timed-out and will be closed\n",
       "26/02/04 00:33:59 WARN SparkSQLOperationManager: Operation OperationHandle [9b9b251e-5868-4161-8c1d-60b8aaf4a39d] is timed-out and will be closed\n",
       "2026-02-04T00:34:12,564Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "2026-02-04T00:34:12,565Z WARN KyuubiOperationManager: Operation OperationHandle [2e345116-c68a-41ef-b4f4-d6243131f21c] is timed-out and will be closed\n",
       "2026-02-04T00:34:12,565Z WARN KyuubiOperationManager: Operation OperationHandle [9b9b251e-5868-4161-8c1d-60b8aaf4a39d] is timed-out and will be closed\n",
       "2026-02-04T00:34:12,566Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:2E 34 51 16 C6 8A 41 EF B4 F4 D6 24 31 31 F2 1C, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [2e345116-c68a-41ef-b4f4-d6243131f21c]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:34:12,568Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:9B 9B 25 1E 58 68 41 61 8C 1D 60 B8 AA F4 A3 9D, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [9b9b251e-5868-4161-8c1d-60b8aaf4a39d]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:34:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [2e345116-c68a-41ef-b4f4-d6243131f21c]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:34:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [9b9b251e-5868-4161-8c1d-60b8aaf4a39d]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:34:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T00:34:42,568Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 00:34:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T00:35:12,569Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 00:35:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T00:35:42,569Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "2026-02-04T00:35:47,767Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T00:35:47,767Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T00:35:47,768Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T00:35:47,768Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@55e3625a\n",
       "2026-02-04T00:35:47,769Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T00:35:47,769Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T00:35:47,769Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T00:35:47,789Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:35:47,792Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-04T00:35:47,792Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T00:35:47,793Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:50930, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-04T00:35:47,797Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b56fa1, negotiated timeout = 120000\n",
       "2026-02-04T00:35:47,797Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T00:35:47,799Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:35:47,802Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:35:47,802Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:35:47,802Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T00:35:47,906Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b56fa1\n",
       "2026-02-04T00:35:47,906Z INFO ZooKeeper: Session: 0x30263a585b56fa1 closed\n",
       "2026-02-04T00:35:47,910Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:48,184Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.216.117\n",
       "2026-02-04T00:35:48,185Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T00:35:48,185Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T00:35:48,187Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/6460b423-a4df-4e86-94d6-637dfa633ce9\n",
       "2026-02-04T00:35:48,192Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]/kernel-v3a0e16093c68d45f18c0600f70906b34ce1f45f18 is opened, current opening sessions 4\n",
       "2026-02-04T00:35:48,192Z INFO LaunchEngine: Processing anonymous's query[6460b423-a4df-4e86-94d6-637dfa633ce9]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:35:48,193Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:48,193Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T00:35:48,197Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@1f24973e\n",
       "2026-02-04T00:35:48,198Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T00:35:48,198Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T00:35:48,200Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T00:35:48,203Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-04T00:35:48,205Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T00:35:48,207Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:50934, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-04T00:35:48,211Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b56fa5, negotiated timeout = 120000\n",
       "2026-02-04T00:35:48,211Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T00:35:48,213Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:35:48,214Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:35:48,216Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:35:48,240Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]]\n",
       "2026-02-04T00:35:48,240Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T00:35:48,350Z INFO ZooKeeper: Session: 0x30263a585b56fa5 closed\n",
       "2026-02-04T00:35:48,350Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b56fa5\n",
       "2026-02-04T00:35:48,350Z INFO LaunchEngine: Processing anonymous's query[6460b423-a4df-4e86-94d6-637dfa633ce9]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.157 seconds\n",
       "2026-02-04T00:35:48,437Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204003547-6tpeohyq]: associated with Kyuubi SessionHandle: [43483002-0b3f-4759-bb37-140d8e96e1e9]\n",
       "2026-02-04T00:35:48,438Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/56cea120-5209-4557-b06d-937f5f6e5c11\n",
       "2026-02-04T00:35:48,438Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T00:35:48,438Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-04T00:35:48,444Z INFO ExecuteStatement: Processing anonymous's query[56cea120-5209-4557-b06d-937f5f6e5c11]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:35:48,445Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/43483002-0b3f-4759-bb37-140d8e96e1e9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204003547-6tpeohyq\n",
       "2026-02-04T00:35:48,450Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/56cea120-5209-4557-b06d-937f5f6e5c11/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:48 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 00:35:48 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 00:35:48 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]\n",
       "26/02/04 00:35:48 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]/kernel-v3a0e16093c68d45f18c0600f70906b34ce1f45f18 is opened, current opening sessions 7\n",
       "26/02/04 00:35:48 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 00:35:48 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 00:35:48 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [3cd755ba-6022-4629-a0ac-46fa6358e1dd]\n",
       "26/02/04 00:35:48 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [3cd755ba-6022-4629-a0ac-46fa6358e1dd]/43483002-0b3f-4759-bb37-140d8e96e1e9_aliveness_probe is opened, current opening sessions 8\n",
       "26/02/04 00:35:48 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204003547-6tpeohyq)]: associated with spark-sql operation session: [43483002-0b3f-4759-bb37-140d8e96e1e9]\n",
       "26/02/04 00:35:48 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-87611108-70f8-45fe-8154-fe54f2aa07c1/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:8:7:6:1:1:1:0:8:-:7:0:f:8:-:4:5:f:e:-:8:1:5:4:-:f:e:5:4:f:2:a:a:0:7:c:1\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-87611108-70f8-45fe-8154-fe54f2aa07c1/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=43483002-0b3f-4759-bb37-140d8e96e1e9\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/04 00:35:48 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/56cea120-5209-4557-b06d-937f5f6e5c11\n",
       "2026-02-04T00:35:49,000Z INFO ExecuteStatement: Query[56cea120-5209-4557-b06d-937f5f6e5c11] in FINISHED_STATE\n",
       "2026-02-04T00:35:49,000Z INFO ExecuteStatement: Processing anonymous's query[56cea120-5209-4557-b06d-937f5f6e5c11]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.555 seconds\n",
       "2026-02-04T00:35:49,456Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/56cea120-5209-4557-b06d-937f5f6e5c11/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:49,466Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/56cea120-5209-4557-b06d-937f5f6e5c11/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:48 INFO ExecutePython: Processing anonymous's query[56cea120-5209-4557-b06d-937f5f6e5c11]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:35:48 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:35:48 INFO ExecutePython: Processing anonymous's query[56cea120-5209-4557-b06d-937f5f6e5c11]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.009 seconds\n",
       "26/02/04 00:35:48 INFO DAGScheduler: Asked to cancel job group 56cea120-5209-4557-b06d-937f5f6e5c11\n",
       "26/02/04 00:35:49 INFO DAGScheduler: Asked to cancel job group 56cea120-5209-4557-b06d-937f5f6e5c11\n",
       "2026-02-04T00:35:49,612Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/c48fad9e-7d1b-47c6-ac4e-280eb44ae265\n",
       "2026-02-04T00:35:49,615Z INFO ExecuteStatement: Processing anonymous's query[c48fad9e-7d1b-47c6-ac4e-280eb44ae265]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:35:49,616Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/43483002-0b3f-4759-bb37-140d8e96e1e9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:49,621Z INFO ExecuteStatement: Query[c48fad9e-7d1b-47c6-ac4e-280eb44ae265] in FINISHED_STATE\n",
       "2026-02-04T00:35:49,621Z INFO ExecuteStatement: Processing anonymous's query[c48fad9e-7d1b-47c6-ac4e-280eb44ae265]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.006 seconds\n",
       "2026-02-04T00:35:50,162Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/87952cff-d5b0-4d39-9d43-dcf4e8b7e842\n",
       "2026-02-04T00:35:50,165Z INFO ExecuteStatement: Processing anonymous's query[87952cff-d5b0-4d39-9d43-dcf4e8b7e842]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:35:50,165Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/43483002-0b3f-4759-bb37-140d8e96e1e9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:50,209Z INFO ExecuteStatement: Query[87952cff-d5b0-4d39-9d43-dcf4e8b7e842] in FINISHED_STATE\n",
       "2026-02-04T00:35:50,209Z INFO ExecuteStatement: Processing anonymous's query[87952cff-d5b0-4d39-9d43-dcf4e8b7e842]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.044 seconds\n",
       "26/02/04 00:35:49 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/c48fad9e-7d1b-47c6-ac4e-280eb44ae265\n",
       "26/02/04 00:35:49 INFO ExecutePython: Processing anonymous's query[c48fad9e-7d1b-47c6-ac4e-280eb44ae265]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:35:49 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:35:49 INFO ExecutePython: Processing anonymous's query[c48fad9e-7d1b-47c6-ac4e-280eb44ae265]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:35:49 INFO DAGScheduler: Asked to cancel job group c48fad9e-7d1b-47c6-ac4e-280eb44ae265\n",
       "26/02/04 00:35:50 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/87952cff-d5b0-4d39-9d43-dcf4e8b7e842\n",
       "26/02/04 00:35:50 INFO ExecutePython: Processing anonymous's query[87952cff-d5b0-4d39-9d43-dcf4e8b7e842]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:35:50 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:35:50 INFO ExecutePython: Processing anonymous's query[87952cff-d5b0-4d39-9d43-dcf4e8b7e842]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.04 seconds\n",
       "26/02/04 00:35:50 INFO DAGScheduler: Asked to cancel job group 87952cff-d5b0-4d39-9d43-dcf4e8b7e842\n",
       "2026-02-04T00:35:50,694Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/87952cff-d5b0-4d39-9d43-dcf4e8b7e842/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:50,842Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/87952cff-d5b0-4d39-9d43-dcf4e8b7e842/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:50 INFO DAGScheduler: Asked to cancel job group 87952cff-d5b0-4d39-9d43-dcf4e8b7e842\n",
       "2026-02-04T00:35:52,996Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/24d7dcb5-159b-4390-9355-6174c6a2688d\n",
       "2026-02-04T00:35:52,998Z INFO ExecuteStatement: Processing anonymous's query[24d7dcb5-159b-4390-9355-6174c6a2688d]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:35:52,999Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/43483002-0b3f-4759-bb37-140d8e96e1e9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:53,004Z INFO ExecuteStatement: Query[24d7dcb5-159b-4390-9355-6174c6a2688d] in FINISHED_STATE\n",
       "2026-02-04T00:35:53,004Z INFO ExecuteStatement: Processing anonymous's query[24d7dcb5-159b-4390-9355-6174c6a2688d]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 00:35:52 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/24d7dcb5-159b-4390-9355-6174c6a2688d\n",
       "26/02/04 00:35:53 INFO ExecutePython: Processing anonymous's query[24d7dcb5-159b-4390-9355-6174c6a2688d]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:35:53 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:35:53 INFO ExecutePython: Processing anonymous's query[24d7dcb5-159b-4390-9355-6174c6a2688d]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:35:53 INFO DAGScheduler: Asked to cancel job group 24d7dcb5-159b-4390-9355-6174c6a2688d\n",
       "2026-02-04T00:35:53,531Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/41d80aa1-1316-4105-892d-c9d6af756578\n",
       "2026-02-04T00:35:53,533Z INFO ExecuteStatement: Processing anonymous's query[41d80aa1-1316-4105-892d-c9d6af756578]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:35:53,534Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/43483002-0b3f-4759-bb37-140d8e96e1e9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:35:54,039Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/41d80aa1-1316-4105-892d-c9d6af756578/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:53 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/43483002-0b3f-4759-bb37-140d8e96e1e9/41d80aa1-1316-4105-892d-c9d6af756578\n",
       "26/02/04 00:35:53 INFO ExecutePython: Processing anonymous's query[41d80aa1-1316-4105-892d-c9d6af756578]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:35:53 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:35:53 INFO GpuOverrides: Plan conversion to the GPU took 0.22 ms\n",
       "26/02/04 00:35:53 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "26/02/04 00:35:53 INFO GpuOverrides: Plan conversion to the GPU took 0.27 ms\n",
       "26/02/04 00:35:53 INFO GpuOverrides: GPU plan transition optimization took 0.12 ms\n",
       "26/02/04 00:35:54 INFO InMemoryFileIndex: It took 34 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:35:54 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Got job 9 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Final stage: ResultStage 11 (sql at <unknown>:0)\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[64] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 00:35:54 INFO SQLOperationListener: Query [41d80aa1-1316-4105-892d-c9d6af756578]: Job 9 started with 1 stages, 1 active jobs running\n",
       "26/02/04 00:35:54 INFO SQLOperationListener: Query [41d80aa1-1316-4105-892d-c9d6af756578]: Stage 11.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 00:35:54 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:35:54 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:35:54 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:35:54 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[64] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:35:54 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:35:54 INFO FairSchedulableBuilder: Added task set TaskSet_11.0 tasks to pool \n",
       "26/02/04 00:35:54 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:35:54 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 100.67.38.155:35047 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:35:54 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 123 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:35:54 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:35:54 INFO DAGScheduler: ResultStage 11 (sql at <unknown>:0) finished in 0.143 s\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:35:54 INFO SQLOperationListener: Finished stage: Stage(11, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 143 msec\n",
       "26/02/04 00:35:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
       "26/02/04 00:35:54 INFO DAGScheduler: Job 9 finished: sql at <unknown>:0, took 0.147176 s\n",
       "26/02/04 00:35:54 INFO StatsReportListener: task runtime:(count: 1, mean: 123.000000, stdev: 0.000000, max: 123.000000, min: 123.000000)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\n",
       "26/02/04 00:35:54 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:35:54 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 00:35:54 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:35:54 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 00:35:54 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 79.674797, stdev: 0.000000, max: 79.674797, min: 79.674797)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\t80 %\n",
       "26/02/04 00:35:54 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 00:35:54 INFO StatsReportListener: other time pct: (count: 1, mean: 20.325203, stdev: 0.000000, max: 20.325203, min: 20.325203)\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:35:54 INFO StatsReportListener: \t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\t20 %\n",
       "26/02/04 00:35:54 INFO SparkSQLEngineListener: Job end. Job 9 state is JobSucceeded\n",
       "26/02/04 00:35:54 INFO SQLOperationListener: Query [41d80aa1-1316-4105-892d-c9d6af756578]: Job 9 succeeded, 0 active jobs running\n",
       "26/02/04 00:35:54 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 00:35:54 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "2026-02-04T00:35:55,203Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/41d80aa1-1316-4105-892d-c9d6af756578/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:54 INFO GpuOverrides: Plan conversion to the GPU took 0.32 ms\n",
       "26/02/04 00:35:54 INFO GpuOverrides: GPU plan transition optimization took 0.12 ms\n",
       "26/02/04 00:35:54 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#377 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#378 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#379 could run on GPU\n",
       "\n",
       "26/02/04 00:35:54 INFO GpuOverrides: Plan conversion to the GPU took 0.68 ms\n",
       "26/02/04 00:35:54 INFO GpuOverrides: GPU plan transition optimization took 0.20 ms\n",
       "26/02/04 00:35:54 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:35:54 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 00:35:54 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:35:54 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 00:35:54 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:35:55 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:35:55 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 00:35:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 00:35:55 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 00:35:55 INFO GpuOverrides: Plan conversion to the GPU took 12.06 ms\n",
       "26/02/04 00:35:55 INFO GpuOverrides: GPU plan transition optimization took 4.61 ms\n",
       "26/02/04 00:35:55 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 00:35:55 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 00:35:55 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:35:55 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:35:55 INFO SparkContext: Created broadcast 14 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 00:35:55 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 00:35:55 INFO DAGScheduler: Got job 10 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 00:35:55 INFO DAGScheduler: Final stage: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 00:35:55 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:35:55 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:35:55 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[73] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 00:35:55 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:35:55 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 00:35:55 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:35:55 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:35:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[73] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:35:55 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:35:55 INFO FairSchedulableBuilder: Added task set TaskSet_12.0 tasks to pool \n",
       "26/02/04 00:35:55 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 10142 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:35:55 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 100.67.38.155:35047 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "2026-02-04T00:35:56,343Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/41d80aa1-1316-4105-892d-c9d6af756578/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:55 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 100.67.38.155:35047 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T00:35:57,246Z INFO ExecuteStatement: Query[41d80aa1-1316-4105-892d-c9d6af756578] in FINISHED_STATE\n",
       "2026-02-04T00:35:57,246Z INFO ExecuteStatement: Processing anonymous's query[41d80aa1-1316-4105-892d-c9d6af756578]: RUNNING_STATE -> FINISHED_STATE, time taken: 3.712 seconds\n",
       "2026-02-04T00:35:57,494Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/41d80aa1-1316-4105-892d-c9d6af756578/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:57 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 2007 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:35:57 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:35:57 INFO DAGScheduler: ResultStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 2.065 s\n",
       "26/02/04 00:35:57 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:35:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
       "26/02/04 00:35:57 INFO SparkSQLEngineListener: Job end. Job 10 state is JobSucceeded\n",
       "26/02/04 00:35:57 INFO DAGScheduler: Job 10 finished: showString at NativeMethodAccessorImpl.java:0, took 2.068791 s\n",
       "26/02/04 00:35:57 INFO CodeGenerator: Code generated in 19.625104 ms\n",
       "26/02/04 00:35:57 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 00:35:57 INFO ExecutePython: |node_id     |scontrol_state                     |reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 00:35:57 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72001-T02|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:32+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72001-T09|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:37+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72001-T10|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:43+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72001-T12|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:28+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72001-T16|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:49+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72003-T14|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:43+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72003-T18|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:58:47+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72004-T05|[\"FUTURE\", \"RESERVED\"]             |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-01T00:33:21+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72007-T01|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:06+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: |nvl72007-T02|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:19+00:00|\n",
       "26/02/04 00:35:57 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 00:35:57 INFO ExecutePython: Processing anonymous's query[41d80aa1-1316-4105-892d-c9d6af756578]: RUNNING_STATE -> FINISHED_STATE, time taken: 3.707 seconds\n",
       "26/02/04 00:35:57 INFO DAGScheduler: Asked to cancel job group 41d80aa1-1316-4105-892d-c9d6af756578\n",
       "2026-02-04T00:35:57,643Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/41d80aa1-1316-4105-892d-c9d6af756578/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:35:57 INFO DAGScheduler: Asked to cancel job group 41d80aa1-1316-4105-892d-c9d6af756578\n",
       "26/02/04 00:35:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:36:12,569Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:36:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:36:42,570Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:36:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "26/02/04 00:36:59 WARN SparkSQLOperationManager: Operation OperationHandle [24d7dcb5-159b-4390-9355-6174c6a2688d] is timed-out and will be closed\n",
       "26/02/04 00:36:59 WARN SparkSQLOperationManager: Operation OperationHandle [56cea120-5209-4557-b06d-937f5f6e5c11] is timed-out and will be closed\n",
       "26/02/04 00:36:59 WARN SparkSQLOperationManager: Operation OperationHandle [c48fad9e-7d1b-47c6-ac4e-280eb44ae265] is timed-out and will be closed\n",
       "26/02/04 00:36:59 WARN SparkSQLOperationManager: Operation OperationHandle [41d80aa1-1316-4105-892d-c9d6af756578] is timed-out and will be closed\n",
       "26/02/04 00:36:59 WARN SparkSQLOperationManager: Operation OperationHandle [87952cff-d5b0-4d39-9d43-dcf4e8b7e842] is timed-out and will be closed\n",
       "2026-02-04T00:37:12,570Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:37:12,570Z WARN KyuubiOperationManager: Operation OperationHandle [24d7dcb5-159b-4390-9355-6174c6a2688d] is timed-out and will be closed\n",
       "2026-02-04T00:37:12,570Z WARN KyuubiOperationManager: Operation OperationHandle [56cea120-5209-4557-b06d-937f5f6e5c11] is timed-out and will be closed\n",
       "2026-02-04T00:37:12,570Z WARN KyuubiOperationManager: Operation OperationHandle [c48fad9e-7d1b-47c6-ac4e-280eb44ae265] is timed-out and will be closed\n",
       "2026-02-04T00:37:12,570Z WARN KyuubiOperationManager: Operation OperationHandle [41d80aa1-1316-4105-892d-c9d6af756578] is timed-out and will be closed\n",
       "2026-02-04T00:37:12,570Z WARN KyuubiOperationManager: Operation OperationHandle [6460b423-a4df-4e86-94d6-637dfa633ce9] is timed-out and will be closed\n",
       "2026-02-04T00:37:12,570Z WARN KyuubiOperationManager: Operation OperationHandle [87952cff-d5b0-4d39-9d43-dcf4e8b7e842] is timed-out and will be closed\n",
       "2026-02-04T00:37:12,572Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:24 D7 DC B5 15 9B 43 90 93 55 61 74 C6 A2 68 8D, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [24d7dcb5-159b-4390-9355-6174c6a2688d]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:37:12,575Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:56 CE A1 20 52 09 45 57 B0 6D 93 7F 5F 6E 5C 11, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [56cea120-5209-4557-b06d-937f5f6e5c11]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:37:12,576Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:C4 8F AD 9E 7D 1B 47 C6 AC 4E 28 0E B4 4A E2 65, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [c48fad9e-7d1b-47c6-ac4e-280eb44ae265]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:37:12,577Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:41 D8 0A A1 13 16 41 05 89 2D C9 D6 AF 75 65 78, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [41d80aa1-1316-4105-892d-c9d6af756578]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:37:12,578Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:87 95 2C FF D5 B0 4D 39 9D 43 DC F4 E8 B7 E8 42, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [87952cff-d5b0-4d39-9d43-dcf4e8b7e842]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:37:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [24d7dcb5-159b-4390-9355-6174c6a2688d]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:37:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [56cea120-5209-4557-b06d-937f5f6e5c11]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:37:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [c48fad9e-7d1b-47c6-ac4e-280eb44ae265]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:37:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [41d80aa1-1316-4105-892d-c9d6af756578]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:37:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [87952cff-d5b0-4d39-9d43-dcf4e8b7e842]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:37:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:37:42,579Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:37:56 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.38.155:36262\n",
       "26/02/04 00:37:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:38:12,579Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:38:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:38:42,579Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:38:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:39:12,580Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:39:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:39:42,580Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:39:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:40:12,580Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:40:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:40:42,628Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "3612.596: [GC (System.gc()) [PSYoungGen: 473091K->7398K(1060352K)] 545483K->79799K(4902912K), 0.0125471 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] \n",
       "3612.609: [Full GC (System.gc()) [PSYoungGen: 7398K->0K(1060352K)] [ParOldGen: 72400K->78461K(3842560K)] 79799K->78461K(4902912K), [Metaspace: 204157K->204157K(1245184K)], 0.7345073 secs] [Times: user=1.50 sys=0.00, real=0.74 secs] \n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 100.67.38.155:35047 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 100.67.38.155:35047 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 100.67.38.155:35047 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:40:58 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 100.67.38.155:35047 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:40:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:41:12,629Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:41:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:41:37,887Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T00:41:37,887Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T00:41:37,888Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T00:41:37,888Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@4c083003\n",
       "2026-02-04T00:41:37,888Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T00:41:37,888Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T00:41:37,889Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T00:41:37,890Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:41:37,892Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-04T00:41:37,892Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T00:41:37,893Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:48946, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-04T00:41:37,897Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b57bfd, negotiated timeout = 120000\n",
       "2026-02-04T00:41:37,897Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T00:41:37,899Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:41:37,901Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:41:37,901Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T00:41:37,902Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:41:38,005Z INFO ZooKeeper: Session: 0x30263a585b57bfd closed\n",
       "2026-02-04T00:41:38,005Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b57bfd\n",
       "2026-02-04T00:41:38,009Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:38,335Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.97.22\n",
       "2026-02-04T00:41:38,336Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T00:41:38,336Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T00:41:38,337Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/f9817f32-bc83-4349-9a9f-682241792ae1\n",
       "2026-02-04T00:41:38,337Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]/kernel-v3375be03908a89033b8c315d7cb6369ed1297e01a is opened, current opening sessions 5\n",
       "2026-02-04T00:41:38,337Z INFO LaunchEngine: Processing anonymous's query[f9817f32-bc83-4349-9a9f-682241792ae1]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:41:38,338Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:38,338Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T00:41:38,338Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@17f58fa3\n",
       "2026-02-04T00:41:38,339Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T00:41:38,339Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T00:41:38,339Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T00:41:38,340Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-04T00:41:38,340Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T00:41:38,341Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:51102, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-04T00:41:38,345Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a4907884e, negotiated timeout = 120000\n",
       "2026-02-04T00:41:38,345Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T00:41:38,346Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:41:38,347Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T00:41:38,347Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T00:41:38,362Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]]\n",
       "2026-02-04T00:41:38,362Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T00:41:38,467Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a4907884e\n",
       "2026-02-04T00:41:38,467Z INFO ZooKeeper: Session: 0x100f75a4907884e closed\n",
       "2026-02-04T00:41:38,468Z INFO LaunchEngine: Processing anonymous's query[f9817f32-bc83-4349-9a9f-682241792ae1]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.13 seconds\n",
       "26/02/04 00:41:38 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 00:41:38 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 00:41:38 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]\n",
       "26/02/04 00:41:38 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]/kernel-v3375be03908a89033b8c315d7cb6369ed1297e01a is opened, current opening sessions 9\n",
       "26/02/04 00:41:38 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 00:41:38 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 00:41:38 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [981840bc-e115-4299-9f8b-45ec602a4556]\n",
       "26/02/04 00:41:38 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [981840bc-e115-4299-9f8b-45ec602a4556]/945dc03c-2aaf-4860-b1cd-a3bcb75de58f_aliveness_probe is opened, current opening sessions 10\n",
       "2026-02-04T00:41:38,628Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204004137-gnbthn3f]: associated with Kyuubi SessionHandle: [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]\n",
       "2026-02-04T00:41:38,629Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/61d88d45-7745-4601-8d9b-4d02fe88b050\n",
       "2026-02-04T00:41:38,629Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T00:41:38,629Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-04T00:41:38,635Z INFO ExecuteStatement: Processing anonymous's query[61d88d45-7745-4601-8d9b-4d02fe88b050]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:41:38,636Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204004137-gnbthn3f\n",
       "2026-02-04T00:41:38,639Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/61d88d45-7745-4601-8d9b-4d02fe88b050/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:39,071Z INFO ExecuteStatement: Query[61d88d45-7745-4601-8d9b-4d02fe88b050] in FINISHED_STATE\n",
       "2026-02-04T00:41:39,072Z INFO ExecuteStatement: Processing anonymous's query[61d88d45-7745-4601-8d9b-4d02fe88b050]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.436 seconds\n",
       "26/02/04 00:41:38 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204004137-gnbthn3f)]: associated with spark-sql operation session: [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]\n",
       "26/02/04 00:41:38 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-61b23148-cc34-46af-91e2-aa53919e6d34/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:6:1:b:2:3:1:4:8:-:c:c:3:4:-:4:6:a:f:-:9:1:e:2:-:a:a:5:3:9:1:9:e:6:d:3:4\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-61b23148-cc34-46af-91e2-aa53919e6d34/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=945dc03c-2aaf-4860-b1cd-a3bcb75de58f\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/04 00:41:38 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/61d88d45-7745-4601-8d9b-4d02fe88b050\n",
       "26/02/04 00:41:39 INFO ExecutePython: Processing anonymous's query[61d88d45-7745-4601-8d9b-4d02fe88b050]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:41:39 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:41:39 INFO ExecutePython: Processing anonymous's query[61d88d45-7745-4601-8d9b-4d02fe88b050]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.009 seconds\n",
       "26/02/04 00:41:39 INFO DAGScheduler: Asked to cancel job group 61d88d45-7745-4601-8d9b-4d02fe88b050\n",
       "2026-02-04T00:41:39,643Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/61d88d45-7745-4601-8d9b-4d02fe88b050/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:39,652Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/61d88d45-7745-4601-8d9b-4d02fe88b050/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:39,799Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/2dc73b4f-1096-49e1-80b0-ada110250fb2\n",
       "2026-02-04T00:41:39,802Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:39,803Z INFO ExecuteStatement: Processing anonymous's query[2dc73b4f-1096-49e1-80b0-ada110250fb2]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:41:39,809Z INFO ExecuteStatement: Query[2dc73b4f-1096-49e1-80b0-ada110250fb2] in FINISHED_STATE\n",
       "2026-02-04T00:41:39,809Z INFO ExecuteStatement: Processing anonymous's query[2dc73b4f-1096-49e1-80b0-ada110250fb2]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.006 seconds\n",
       "2026-02-04T00:41:40,349Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/134ec97a-066f-4282-9976-82564047353f\n",
       "2026-02-04T00:41:40,351Z INFO ExecuteStatement: Processing anonymous's query[134ec97a-066f-4282-9976-82564047353f]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:41:40,351Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:40,396Z INFO ExecuteStatement: Query[134ec97a-066f-4282-9976-82564047353f] in FINISHED_STATE\n",
       "2026-02-04T00:41:40,396Z INFO ExecuteStatement: Processing anonymous's query[134ec97a-066f-4282-9976-82564047353f]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.045 seconds\n",
       "26/02/04 00:41:39 INFO DAGScheduler: Asked to cancel job group 61d88d45-7745-4601-8d9b-4d02fe88b050\n",
       "26/02/04 00:41:39 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/2dc73b4f-1096-49e1-80b0-ada110250fb2\n",
       "26/02/04 00:41:39 INFO ExecutePython: Processing anonymous's query[2dc73b4f-1096-49e1-80b0-ada110250fb2]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:41:39 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:41:39 INFO ExecutePython: Processing anonymous's query[2dc73b4f-1096-49e1-80b0-ada110250fb2]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:41:39 INFO DAGScheduler: Asked to cancel job group 2dc73b4f-1096-49e1-80b0-ada110250fb2\n",
       "26/02/04 00:41:40 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/134ec97a-066f-4282-9976-82564047353f\n",
       "26/02/04 00:41:40 INFO ExecutePython: Processing anonymous's query[134ec97a-066f-4282-9976-82564047353f]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:41:40 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:41:40 INFO ExecutePython: Processing anonymous's query[134ec97a-066f-4282-9976-82564047353f]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.04 seconds\n",
       "26/02/04 00:41:40 INFO DAGScheduler: Asked to cancel job group 134ec97a-066f-4282-9976-82564047353f\n",
       "2026-02-04T00:41:40,865Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/134ec97a-066f-4282-9976-82564047353f/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:41,014Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/134ec97a-066f-4282-9976-82564047353f/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:41:41 INFO DAGScheduler: Asked to cancel job group 134ec97a-066f-4282-9976-82564047353f\n",
       "2026-02-04T00:41:42,629Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T00:41:43,156Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/eb0bf24d-4bed-4de4-af8c-015b9b53a520\n",
       "2026-02-04T00:41:43,158Z INFO ExecuteStatement: Processing anonymous's query[eb0bf24d-4bed-4de4-af8c-015b9b53a520]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:41:43,159Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:43,162Z INFO ExecuteStatement: Query[eb0bf24d-4bed-4de4-af8c-015b9b53a520] in FINISHED_STATE\n",
       "2026-02-04T00:41:43,162Z INFO ExecuteStatement: Processing anonymous's query[eb0bf24d-4bed-4de4-af8c-015b9b53a520]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "26/02/04 00:41:43 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/eb0bf24d-4bed-4de4-af8c-015b9b53a520\n",
       "26/02/04 00:41:43 INFO ExecutePython: Processing anonymous's query[eb0bf24d-4bed-4de4-af8c-015b9b53a520]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:41:43 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:41:43 INFO ExecutePython: Processing anonymous's query[eb0bf24d-4bed-4de4-af8c-015b9b53a520]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 00:41:43 INFO DAGScheduler: Asked to cancel job group eb0bf24d-4bed-4de4-af8c-015b9b53a520\n",
       "2026-02-04T00:41:43,792Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/0ffc59f8-f707-471f-96c4-7654afe509af\n",
       "2026-02-04T00:41:43,795Z INFO ExecuteStatement: Processing anonymous's query[0ffc59f8-f707-471f-96c4-7654afe509af]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T00:41:43,795Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T00:41:44,295Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0ffc59f8-f707-471f-96c4-7654afe509af/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:41:43 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/945dc03c-2aaf-4860-b1cd-a3bcb75de58f/0ffc59f8-f707-471f-96c4-7654afe509af\n",
       "26/02/04 00:41:43 INFO ExecutePython: Processing anonymous's query[0ffc59f8-f707-471f-96c4-7654afe509af]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 00:41:43 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 00:41:43 INFO GpuOverrides: Plan conversion to the GPU took 0.26 ms\n",
       "26/02/04 00:41:43 INFO GpuOverrides: GPU plan transition optimization took 0.14 ms\n",
       "26/02/04 00:41:43 INFO GpuOverrides: Plan conversion to the GPU took 0.42 ms\n",
       "26/02/04 00:41:43 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 00:41:43 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:41:44 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Got job 11 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Final stage: ResultStage 13 (sql at <unknown>:0)\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[75] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 00:41:44 INFO SQLOperationListener: Query [0ffc59f8-f707-471f-96c4-7654afe509af]: Job 11 started with 1 stages, 1 active jobs running\n",
       "26/02/04 00:41:44 INFO SQLOperationListener: Query [0ffc59f8-f707-471f-96c4-7654afe509af]: Stage 13.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 00:41:44 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:41:44 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:41:44 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:41:44 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[75] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:41:44 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:41:44 INFO FairSchedulableBuilder: Added task set TaskSet_13.0 tasks to pool \n",
       "26/02/04 00:41:44 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:41:44 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 100.67.38.155:35047 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:41:44 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 123 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:41:44 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:41:44 INFO DAGScheduler: ResultStage 13 (sql at <unknown>:0) finished in 0.138 s\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:41:44 INFO SQLOperationListener: Finished stage: Stage(13, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 138 msec\n",
       "26/02/04 00:41:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
       "26/02/04 00:41:44 INFO DAGScheduler: Job 11 finished: sql at <unknown>:0, took 0.142790 s\n",
       "26/02/04 00:41:44 INFO StatsReportListener: task runtime:(count: 1, mean: 123.000000, stdev: 0.000000, max: 123.000000, min: 123.000000)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\t123.0 ms\n",
       "26/02/04 00:41:44 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:41:44 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 00:41:44 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 00:41:44 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 00:41:44 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 82.926829, stdev: 0.000000, max: 82.926829, min: 82.926829)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\n",
       "26/02/04 00:41:44 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 00:41:44 INFO StatsReportListener: other time pct: (count: 1, mean: 17.073171, stdev: 0.000000, max: 17.073171, min: 17.073171)\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 00:41:44 INFO StatsReportListener: \t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\n",
       "26/02/04 00:41:44 INFO SparkSQLEngineListener: Job end. Job 11 state is JobSucceeded\n",
       "26/02/04 00:41:44 INFO SQLOperationListener: Query [0ffc59f8-f707-471f-96c4-7654afe509af]: Job 11 succeeded, 0 active jobs running\n",
       "26/02/04 00:41:44 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`oci_hsg_maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 00:41:44 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`oci_hsg_maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "2026-02-04T00:41:45,295Z INFO ExecuteStatement: Query[0ffc59f8-f707-471f-96c4-7654afe509af] in FINISHED_STATE\n",
       "2026-02-04T00:41:45,295Z INFO ExecuteStatement: Processing anonymous's query[0ffc59f8-f707-471f-96c4-7654afe509af]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.5 seconds\n",
       "2026-02-04T00:41:45,436Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0ffc59f8-f707-471f-96c4-7654afe509af/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:41:44 INFO GpuOverrides: Plan conversion to the GPU took 0.28 ms\n",
       "26/02/04 00:41:44 INFO GpuOverrides: GPU plan transition optimization took 0.16 ms\n",
       "26/02/04 00:41:44 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#454 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#455 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#456 could run on GPU\n",
       "\n",
       "26/02/04 00:41:44 INFO GpuOverrides: Plan conversion to the GPU took 0.52 ms\n",
       "26/02/04 00:41:44 INFO GpuOverrides: GPU plan transition optimization took 0.17 ms\n",
       "26/02/04 00:41:44 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:41:44 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 00:41:44 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:41:44 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 00:41:44 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 00:41:45 INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.\n",
       "26/02/04 00:41:45 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 00:41:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 00:41:45 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 00:41:45 INFO GpuOverrides: Plan conversion to the GPU took 2.48 ms\n",
       "26/02/04 00:41:45 INFO GpuOverrides: GPU plan transition optimization took 0.70 ms\n",
       "26/02/04 00:41:45 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 00:41:45 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 00:41:45 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 00:41:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:41:45 INFO SparkContext: Created broadcast 17 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 00:41:45 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Got job 12 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Final stage: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[84] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 00:41:45 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 00:41:45 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 00:41:45 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 00:41:45 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[84] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 00:41:45 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
       "26/02/04 00:41:45 INFO FairSchedulableBuilder: Added task set TaskSet_14.0 tasks to pool \n",
       "26/02/04 00:41:45 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (100.67.38.155, executor 2, partition 0, PROCESS_LOCAL, 10142 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 00:41:45 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 100.67.38.155:35047 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:41:45 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 100.67.38.155:35047 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 00:41:45 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 175 ms on 100.67.38.155 (executor 2) (1/1)\n",
       "26/02/04 00:41:45 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
       "26/02/04 00:41:45 INFO DAGScheduler: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0) finished in 0.182 s\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 00:41:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
       "26/02/04 00:41:45 INFO SparkSQLEngineListener: Job end. Job 12 state is JobSucceeded\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Job 12 finished: showString at NativeMethodAccessorImpl.java:0, took 0.184771 s\n",
       "26/02/04 00:41:45 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 00:41:45 INFO ExecutePython: |node_id     |scontrol_state                     |reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 00:41:45 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72001-T02|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:32+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72001-T09|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:37+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72001-T10|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:43+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72001-T12|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:28+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72001-T16|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:49+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72003-T14|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:43+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72003-T18|[\"ALLOCATED\"]                      |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:58:47+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72004-T05|[\"FUTURE\", \"RESERVED\"]             |      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-01T00:33:21+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72007-T01|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:06+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: |nvl72007-T02|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T00:35:08.613052+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:19+00:00|\n",
       "26/02/04 00:41:45 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 00:41:45 INFO ExecutePython: Processing anonymous's query[0ffc59f8-f707-471f-96c4-7654afe509af]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.496 seconds\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Asked to cancel job group 0ffc59f8-f707-471f-96c4-7654afe509af\n",
       "2026-02-04T00:41:45,593Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0ffc59f8-f707-471f-96c4-7654afe509af/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 00:41:45 INFO DAGScheduler: Asked to cancel job group 0ffc59f8-f707-471f-96c4-7654afe509af\n",
       "26/02/04 00:41:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T00:42:12,629Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 00:42:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T00:42:42,630Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T00:42:42,630Z WARN KyuubiOperationManager: Operation OperationHandle [134ec97a-066f-4282-9976-82564047353f] is timed-out and will be closed\n",
       "2026-02-04T00:42:42,630Z WARN KyuubiOperationManager: Operation OperationHandle [2dc73b4f-1096-49e1-80b0-ada110250fb2] is timed-out and will be closed\n",
       "2026-02-04T00:42:42,630Z WARN KyuubiOperationManager: Operation OperationHandle [61d88d45-7745-4601-8d9b-4d02fe88b050] is timed-out and will be closed\n",
       "2026-02-04T00:42:42,630Z WARN KyuubiOperationManager: Operation OperationHandle [f9817f32-bc83-4349-9a9f-682241792ae1] is timed-out and will be closed\n",
       "2026-02-04T00:42:42,632Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:13 4E C9 7A 06 6F 42 82 99 76 82 56 40 47 35 3F, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T00:42:42,634Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:2D C7 3B 4F 10 96 49 E1 80 B0 AD A1 10 25 0F B2, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T00:42:42,635Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:61 D8 8D 45 77 45 46 01 8D 9B 4D 02 FE 88 B0 50, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T00:42:42,635Z INFO KyuubiSessionManager: Closing session cfc8b837-b0f7-40ee-b388-391d8316c4dc that has been idle for more than 3600000 ms\n",
       "2026-02-04T00:42:42,636Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]/hongy-default-20260203202802-startup is closed, current opening sessions 4\n",
       "26/02/04 00:42:42 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]\n",
       "26/02/04 00:42:42 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]\n",
       "26/02/04 00:42:42 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]/hongy-default-20260203202802-startup is closed, current opening sessions 9\n",
       "26/02/04 00:42:42 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [cfc8b837-b0f7-40ee-b388-391d8316c4dc]\n",
       "26/02/04 00:42:42 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [842f5dda-d065-4528-b30f-336a8fc57fab]\n",
       "26/02/04 00:42:42 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [842f5dda-d065-4528-b30f-336a8fc57fab]\n",
       "26/02/04 00:42:42 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [842f5dda-d065-4528-b30f-336a8fc57fab]/cfc8b837-b0f7-40ee-b388-391d8316c4dc_aliveness_probe is closed, current opening sessions 8\n",
       "26/02/04 00:42:42 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [842f5dda-d065-4528-b30f-336a8fc57fab]\n",
       "26/02/04 00:42:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "26/02/04 00:42:59 WARN SparkSQLOperationManager: Operation OperationHandle [0ffc59f8-f707-471f-96c4-7654afe509af] is timed-out and will be closed\n",
       "26/02/04 00:42:59 WARN SparkSQLOperationManager: Operation OperationHandle [eb0bf24d-4bed-4de4-af8c-015b9b53a520] is timed-out and will be closed\n",
       "2026-02-04T00:43:12,675Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T00:43:12,676Z WARN KyuubiOperationManager: Operation OperationHandle [0ffc59f8-f707-471f-96c4-7654afe509af] is timed-out and will be closed\n",
       "2026-02-04T00:43:12,676Z WARN KyuubiOperationManager: Operation OperationHandle [eb0bf24d-4bed-4de4-af8c-015b9b53a520] is timed-out and will be closed\n",
       "2026-02-04T00:43:12,677Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:0F FC 59 F8 F7 07 47 1F 96 C4 76 54 AF E5 09 AF, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [0ffc59f8-f707-471f-96c4-7654afe509af]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T00:43:12,679Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:EB 0B F2 4D 4B ED 4D E4 AF 8C 01 5B 9B 53 A5 20, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [eb0bf24d-4bed-4de4-af8c-015b9b53a520]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:43:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [0ffc59f8-f707-471f-96c4-7654afe509af]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:43:12 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [eb0bf24d-4bed-4de4-af8c-015b9b53a520]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 00:43:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:43:42,679Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:43:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:44:12,679Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:44:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:44:42,680Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:44:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:45:12,680Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:45:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:45:42,680Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:45:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:46:12,681Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:46:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:46:42,681Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:46:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:47:12,681Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:47:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:47:42,682Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:47:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:48:12,682Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:48:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:48:42,682Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:48:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:49:12,683Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:49:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:49:42,683Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:49:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:50:12,683Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:50:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:50:42,683Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:50:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:51:12,684Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:51:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:51:42,684Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:51:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:52:12,684Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:52:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:52:42,685Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:52:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:53:12,685Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:53:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:53:42,685Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:53:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:54:12,686Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:54:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:54:42,686Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:54:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:55:12,686Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:55:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:55:42,686Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:55:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:56:12,687Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:56:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:56:42,687Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:56:45 INFO KubernetesClusterSchedulerBackend: Requesting to kill executor(s) 2\n",
       "26/02/04 00:56:45 INFO KubernetesClusterSchedulerBackend: Actual list of executor(s) to be killed is 2\n",
       "26/02/04 00:56:45 INFO TaskSchedulerImpl: Executor 2 on 100.67.38.155 killed by driver.\n",
       "26/02/04 00:56:45 INFO DAGScheduler: Executor lost: 2 (epoch 3)\n",
       "26/02/04 00:56:45 INFO ExecutorAllocationManager: Executors 2 removed due to idle timeout.\n",
       "26/02/04 00:56:45 INFO ExecutorMonitor: Executor 2 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 2, unexpectedly exited: 0).\n",
       "26/02/04 00:56:45 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.\n",
       "26/02/04 00:56:45 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 100.67.38.155, 35047, None)\n",
       "26/02/04 00:56:45 INFO BlockManagerMaster: Removed 2 successfully in removeExecutor\n",
       "26/02/04 00:56:45 INFO DAGScheduler: Shuffle files lost for executor: 2 (epoch 3)\n",
       "26/02/04 00:56:46 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.38.155:53518\n",
       "26/02/04 00:56:48 INFO BlockManagerMaster: Removal of executor 2 requested\n",
       "26/02/04 00:56:48 INFO BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.\n",
       "26/02/04 00:56:48 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: Asked to remove non-existent executor 2\n",
       "26/02/04 00:56:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:57:12,687Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:57:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:57:42,688Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:57:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:58:12,688Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:58:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:58:42,688Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:58:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:59:12,689Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:59:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T00:59:42,689Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 00:59:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:00:12,689Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:00:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:00:42,689Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "4812.383: [GC (Allocation Failure) [PSYoungGen: 1005568K->4837K(1000448K)] 1084029K->83306K(4843008K), 0.0128252 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \n",
       "26/02/04 01:00:57 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:00:57 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:00:57 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:00:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:01:12,690Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:01:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:01:42,690Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:01:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:02:12,690Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:02:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:02:42,690Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:02:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:03:12,691Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:03:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:03:42,691Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:03:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:04:10,220Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T01:04:10,220Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T01:04:10,221Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T01:04:10,221Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@794f081e\n",
       "2026-02-04T01:04:10,222Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T01:04:10,222Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T01:04:10,222Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T01:04:10,222Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T01:04:10,226Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-04T01:04:10,226Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T01:04:10,226Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:48690, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-04T01:04:10,231Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f4a951, negotiated timeout = 120000\n",
       "2026-02-04T01:04:10,231Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T01:04:10,232Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:04:10,233Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T01:04:10,233Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T01:04:10,235Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:04:10,338Z INFO ZooKeeper: Session: 0x200d89556f4a951 closed\n",
       "2026-02-04T01:04:10,338Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f4a951\n",
       "2026-02-04T01:04:10,342Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:10,588Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.97.22\n",
       "2026-02-04T01:04:10,588Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T01:04:10,588Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T01:04:10,589Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/c3ede94c-2b96-4564-9b92-e60120983338\n",
       "2026-02-04T01:04:10,589Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [4076596d-1c27-407c-af0b-d372cb2a90f3]/kernel-v32488f7af5f1152cdb63f07653652f8a8d0a617da is opened, current opening sessions 5\n",
       "2026-02-04T01:04:10,590Z INFO LaunchEngine: Processing anonymous's query[c3ede94c-2b96-4564-9b92-e60120983338]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:04:10,590Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:10,591Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T01:04:10,591Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@6a4bf822\n",
       "2026-02-04T01:04:10,591Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T01:04:10,591Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T01:04:10,592Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-04T01:04:10,592Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T01:04:10,592Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T01:04:10,593Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:48702, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-04T01:04:10,598Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f4a956, negotiated timeout = 120000\n",
       "2026-02-04T01:04:10,598Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T01:04:10,599Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:04:10,599Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:04:10,600Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T01:04:10,615Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [4076596d-1c27-407c-af0b-d372cb2a90f3] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [4076596d-1c27-407c-af0b-d372cb2a90f3]]\n",
       "2026-02-04T01:04:10,615Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "26/02/04 01:04:10 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 01:04:10 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 01:04:10 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [4076596d-1c27-407c-af0b-d372cb2a90f3]\n",
       "26/02/04 01:04:10 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [4076596d-1c27-407c-af0b-d372cb2a90f3]/kernel-v32488f7af5f1152cdb63f07653652f8a8d0a617da is opened, current opening sessions 9\n",
       "26/02/04 01:04:10 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 01:04:10 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 01:04:10 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [8aee055a-1db1-4dec-8a6a-40800f687bdb]\n",
       "26/02/04 01:04:10 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [8aee055a-1db1-4dec-8a6a-40800f687bdb]/4076596d-1c27-407c-af0b-d372cb2a90f3_aliveness_probe is opened, current opening sessions 10\n",
       "2026-02-04T01:04:10,721Z INFO ZooKeeper: Session: 0x200d89556f4a956 closed\n",
       "2026-02-04T01:04:10,721Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f4a956\n",
       "2026-02-04T01:04:10,721Z INFO LaunchEngine: Processing anonymous's query[c3ede94c-2b96-4564-9b92-e60120983338]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.131 seconds\n",
       "2026-02-04T01:04:10,805Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204010409-f97pg5mv]: associated with Kyuubi SessionHandle: [4076596d-1c27-407c-af0b-d372cb2a90f3]\n",
       "2026-02-04T01:04:10,806Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/e9404517-68b6-4cd2-8fb7-d08a3027aea2\n",
       "2026-02-04T01:04:10,806Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [4076596d-1c27-407c-af0b-d372cb2a90f3] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T01:04:10,806Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [4076596d-1c27-407c-af0b-d372cb2a90f3] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-04T01:04:10,812Z INFO ExecuteStatement: Processing anonymous's query[e9404517-68b6-4cd2-8fb7-d08a3027aea2]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:04:10,813Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204010409-f97pg5mv\n",
       "2026-02-04T01:04:10,817Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/e9404517-68b6-4cd2-8fb7-d08a3027aea2/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:11,275Z INFO ExecuteStatement: Query[e9404517-68b6-4cd2-8fb7-d08a3027aea2] in FINISHED_STATE\n",
       "2026-02-04T01:04:11,275Z INFO ExecuteStatement: Processing anonymous's query[e9404517-68b6-4cd2-8fb7-d08a3027aea2]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.462 seconds\n",
       "26/02/04 01:04:10 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204010409-f97pg5mv)]: associated with spark-sql operation session: [4076596d-1c27-407c-af0b-d372cb2a90f3]\n",
       "26/02/04 01:04:10 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-24d28e92-e6b3-4e31-9a96-f6cf7e22f952/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:2:4:d:2:8:e:9:2:-:e:6:b:3:-:4:e:3:1:-:9:a:9:6:-:f:6:c:f:7:e:2:2:f:9:5:2\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-24d28e92-e6b3-4e31-9a96-f6cf7e22f952/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=4076596d-1c27-407c-af0b-d372cb2a90f3\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/04 01:04:10 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/e9404517-68b6-4cd2-8fb7-d08a3027aea2\n",
       "26/02/04 01:04:11 INFO ExecutePython: Processing anonymous's query[e9404517-68b6-4cd2-8fb7-d08a3027aea2]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:04:11 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:04:11 INFO ExecutePython: Processing anonymous's query[e9404517-68b6-4cd2-8fb7-d08a3027aea2]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.013 seconds\n",
       "26/02/04 01:04:11 INFO DAGScheduler: Asked to cancel job group e9404517-68b6-4cd2-8fb7-d08a3027aea2\n",
       "2026-02-04T01:04:11,821Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/e9404517-68b6-4cd2-8fb7-d08a3027aea2/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:11,829Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/e9404517-68b6-4cd2-8fb7-d08a3027aea2/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:12,055Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/e3bc3229-0322-454f-82f3-9ffb0cb62ab0\n",
       "2026-02-04T01:04:12,058Z INFO ExecuteStatement: Processing anonymous's query[e3bc3229-0322-454f-82f3-9ffb0cb62ab0]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:04:12,058Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:12,064Z INFO ExecuteStatement: Query[e3bc3229-0322-454f-82f3-9ffb0cb62ab0] in FINISHED_STATE\n",
       "2026-02-04T01:04:12,064Z INFO ExecuteStatement: Processing anonymous's query[e3bc3229-0322-454f-82f3-9ffb0cb62ab0]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.006 seconds\n",
       "26/02/04 01:04:11 INFO DAGScheduler: Asked to cancel job group e9404517-68b6-4cd2-8fb7-d08a3027aea2\n",
       "26/02/04 01:04:12 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/e3bc3229-0322-454f-82f3-9ffb0cb62ab0\n",
       "26/02/04 01:04:12 INFO ExecutePython: Processing anonymous's query[e3bc3229-0322-454f-82f3-9ffb0cb62ab0]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:04:12 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:04:12 INFO ExecutePython: Processing anonymous's query[e3bc3229-0322-454f-82f3-9ffb0cb62ab0]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:04:12 INFO DAGScheduler: Asked to cancel job group e3bc3229-0322-454f-82f3-9ffb0cb62ab0\n",
       "2026-02-04T01:04:12,691Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T01:04:12,954Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/336e8100-338a-4c77-8a37-33726882f542\n",
       "2026-02-04T01:04:12,956Z INFO ExecuteStatement: Processing anonymous's query[336e8100-338a-4c77-8a37-33726882f542]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:04:12,957Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:13,005Z INFO ExecuteStatement: Query[336e8100-338a-4c77-8a37-33726882f542] in FINISHED_STATE\n",
       "2026-02-04T01:04:13,005Z INFO ExecuteStatement: Processing anonymous's query[336e8100-338a-4c77-8a37-33726882f542]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.049 seconds\n",
       "2026-02-04T01:04:13,662Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/336e8100-338a-4c77-8a37-33726882f542/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:12 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/336e8100-338a-4c77-8a37-33726882f542\n",
       "26/02/04 01:04:12 INFO ExecutePython: Processing anonymous's query[336e8100-338a-4c77-8a37-33726882f542]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:04:12 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:04:13 INFO ExecutePython: Processing anonymous's query[336e8100-338a-4c77-8a37-33726882f542]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.042 seconds\n",
       "26/02/04 01:04:13 INFO DAGScheduler: Asked to cancel job group 336e8100-338a-4c77-8a37-33726882f542\n",
       "2026-02-04T01:04:13,823Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/336e8100-338a-4c77-8a37-33726882f542/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:13 INFO DAGScheduler: Asked to cancel job group 336e8100-338a-4c77-8a37-33726882f542\n",
       "2026-02-04T01:04:15,979Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb\n",
       "2026-02-04T01:04:15,981Z INFO ExecuteStatement: Processing anonymous's query[56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:04:15,982Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:15,986Z INFO ExecuteStatement: Query[56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb] in FINISHED_STATE\n",
       "2026-02-04T01:04:15,986Z INFO ExecuteStatement: Processing anonymous's query[56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T01:04:16,467Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/df6fb7bc-6438-4de6-9618-e4bca455b018\n",
       "2026-02-04T01:04:16,469Z INFO ExecuteStatement: Processing anonymous's query[df6fb7bc-6438-4de6-9618-e4bca455b018]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:04:16,469Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:15 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb\n",
       "26/02/04 01:04:15 INFO ExecutePython: Processing anonymous's query[56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:04:15 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:04:15 INFO ExecutePython: Processing anonymous's query[56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.0 seconds\n",
       "26/02/04 01:04:15 INFO DAGScheduler: Asked to cancel job group 56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb\n",
       "26/02/04 01:04:16 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/df6fb7bc-6438-4de6-9618-e4bca455b018\n",
       "26/02/04 01:04:16 INFO ExecutePython: Processing anonymous's query[df6fb7bc-6438-4de6-9618-e4bca455b018]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:04:16 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:04:16 INFO GpuOverrides: Plan conversion to the GPU took 0.33 ms\n",
       "26/02/04 01:04:16 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 01:04:16 INFO GpuOverrides: Plan conversion to the GPU took 0.33 ms\n",
       "26/02/04 01:04:16 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 01:04:16 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:04:16 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 01:04:16 INFO DAGScheduler: Got job 13 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 01:04:16 INFO DAGScheduler: Final stage: ResultStage 15 (sql at <unknown>:0)\n",
       "26/02/04 01:04:16 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:04:16 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:04:16 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[86] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 01:04:16 INFO SQLOperationListener: Query [df6fb7bc-6438-4de6-9618-e4bca455b018]: Job 13 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:04:16 INFO SQLOperationListener: Query [df6fb7bc-6438-4de6-9618-e4bca455b018]: Stage 15.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:04:16 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:04:16 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:04:16 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:04:16 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:04:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[86] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:04:16 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:04:16 INFO FairSchedulableBuilder: Added task set TaskSet_15.0 tasks to pool \n",
       "2026-02-04T01:04:17,091Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:18,278Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:19,431Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:20,624Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:21,471Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "2026-02-04T01:04:21,760Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:22,916Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:24,072Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:25,214Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:26,472Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "2026-02-04T01:04:26,506Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:27,644Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:26 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
       "26/02/04 01:04:26 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
       "26/02/04 01:04:26 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : log4j2.properties,nvoauth.conf,metrics.properties\n",
       "26/02/04 01:04:26 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
       "26/02/04 01:04:26 INFO SparkExecutorFsGroupFeatureStep: SparkExecutorFsGroupFeatureStep configure security context fsGroup\n",
       "26/02/04 01:04:26 INFO ExecutorPodsAllocator: Trying to create PersistentVolumeClaim cluster-20260203202803-yawkv5ak-exec-3-pvc-0 with StorageClass gp3\n",
       "2026-02-04T01:04:28,810Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:04:29,955Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:31,110Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:31,473Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "2026-02-04T01:04:32,287Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:33,441Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:34,588Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:35,731Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:36,474Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "2026-02-04T01:04:36,867Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:38,012Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:39,156Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:40,298Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:41,450Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:41,475Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "2026-02-04T01:04:42,607Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:42,692Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T01:04:43,754Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:44,923Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:45 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.4.62:55532\n",
       "2026-02-04T01:04:46,065Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:46,477Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "26/02/04 01:04:46 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (100.67.4.62:55534) with ID 3,  ResourceProfileId 0\n",
       "26/02/04 01:04:46 INFO ExecutorMonitor: New executor 3 has registered (new total is 1)\n",
       "26/02/04 01:04:46 INFO BlockManagerMasterEndpoint: Registering block manager 100.67.4.62:41749 with 9.0 GiB RAM, BlockManagerId(3, 100.67.4.62, 41749, None)\n",
       "2026-02-04T01:04:47,202Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:48,350Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:49,492Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:50,629Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:51,478Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "2026-02-04T01:04:51,780Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:52,927Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:54,063Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:55,212Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:55 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (100.67.4.62, executor 3, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:04:55 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 100.67.4.62:41749 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:04:56,364Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:56,479Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "2026-02-04T01:04:57,511Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:04:58,669Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:59 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 3928 ms on 100.67.4.62 (executor 3) (1/1)\n",
       "26/02/04 01:04:59 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:04:59 INFO DAGScheduler: ResultStage 15 (sql at <unknown>:0) finished in 42.713 s\n",
       "26/02/04 01:04:59 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:04:59 INFO SQLOperationListener: Finished stage: Stage(15, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 42713 msec\n",
       "26/02/04 01:04:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
       "26/02/04 01:04:59 INFO DAGScheduler: Job 13 finished: sql at <unknown>:0, took 42.716456 s\n",
       "26/02/04 01:04:59 INFO StatsReportListener: task runtime:(count: 1, mean: 3928.000000, stdev: 0.000000, max: 3928.000000, min: 3928.000000)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t3.9 s\t3.9 s\t3.9 s\t3.9 s\t3.9 s\t3.9 s\t3.9 s\t3.9 s\t3.9 s\n",
       "26/02/04 01:04:59 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:04:59 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:04:59 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:04:59 INFO StatsReportListener: task result size:(count: 1, mean: 1975.000000, stdev: 0.000000, max: 1975.000000, min: 1975.000000)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\n",
       "26/02/04 01:04:59 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 83.503055, stdev: 0.000000, max: 83.503055, min: 83.503055)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\n",
       "26/02/04 01:04:59 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:04:59 INFO StatsReportListener: other time pct: (count: 1, mean: 16.496945, stdev: 0.000000, max: 16.496945, min: 16.496945)\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:04:59 INFO StatsReportListener: \t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\n",
       "26/02/04 01:04:59 INFO SparkSQLEngineListener: Job end. Job 13 state is JobSucceeded\n",
       "26/02/04 01:04:59 INFO SQLOperationListener: Query [df6fb7bc-6438-4de6-9618-e4bca455b018]: Job 13 succeeded, 0 active jobs running\n",
       "26/02/04 01:04:59 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`gcp_east4_maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 01:04:59 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`gcp_east4_maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "26/02/04 01:04:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:04:59,825Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:04:59 INFO GpuOverrides: Plan conversion to the GPU took 0.28 ms\n",
       "26/02/04 01:04:59 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "26/02/04 01:04:59 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#531 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#532 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#533 could run on GPU\n",
       "\n",
       "26/02/04 01:04:59 INFO GpuOverrides: Plan conversion to the GPU took 0.44 ms\n",
       "26/02/04 01:04:59 INFO GpuOverrides: GPU plan transition optimization took 0.16 ms\n",
       "26/02/04 01:05:00 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:05:00 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 01:05:00 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:05:00 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 01:05:00 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:05:00 INFO InMemoryFileIndex: It took 28 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:05:00 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:05:00 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:05:00 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 01:05:00 INFO GpuOverrides: Plan conversion to the GPU took 2.61 ms\n",
       "26/02/04 01:05:00 INFO GpuOverrides: GPU plan transition optimization took 0.78 ms\n",
       "26/02/04 01:05:00 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:05:00 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:05:00 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:05:00 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:05:00 INFO SparkContext: Created broadcast 20 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:05:00 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:05:00 INFO DAGScheduler: Got job 14 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:05:00 INFO DAGScheduler: Final stage: ResultStage 16 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:05:00 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:05:00 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:05:00 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[95] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:05:00 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:05:00 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:05:00 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:05:00 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:05:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[95] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:05:00 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:05:00 INFO FairSchedulableBuilder: Added task set TaskSet_16.0 tasks to pool \n",
       "26/02/04 01:05:00 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (100.67.4.62, executor 3, partition 0, PROCESS_LOCAL, 10142 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:05:00 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 100.67.4.62:41749 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:05:00,969Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:05:01,480Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in RUNNING_STATE\n",
       "26/02/04 01:05:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 100.67.4.62:41749 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:05:02,115Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:05:02,451Z INFO ExecuteStatement: Query[df6fb7bc-6438-4de6-9618-e4bca455b018] in FINISHED_STATE\n",
       "2026-02-04T01:05:02,451Z INFO ExecuteStatement: Processing anonymous's query[df6fb7bc-6438-4de6-9618-e4bca455b018]: RUNNING_STATE -> FINISHED_STATE, time taken: 45.982 seconds\n",
       "26/02/04 01:05:02 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 2194 ms on 100.67.4.62 (executor 3) (1/1)\n",
       "26/02/04 01:05:02 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:05:02 INFO DAGScheduler: ResultStage 16 (showString at NativeMethodAccessorImpl.java:0) finished in 2.202 s\n",
       "26/02/04 01:05:02 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:05:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
       "26/02/04 01:05:02 INFO SparkSQLEngineListener: Job end. Job 14 state is JobSucceeded\n",
       "26/02/04 01:05:02 INFO DAGScheduler: Job 14 finished: showString at NativeMethodAccessorImpl.java:0, took 2.205395 s\n",
       "26/02/04 01:05:02 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:05:02 INFO ExecutePython: |node_id   |scontrol_state|reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 01:05:02 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-001|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-002|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-003|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-004|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-005|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-006|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-007|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-008|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-009|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: |cpu-dm-010|[\"IDLE\"]      |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:05:02 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:05:02 INFO ExecutePython: Processing anonymous's query[df6fb7bc-6438-4de6-9618-e4bca455b018]: RUNNING_STATE -> FINISHED_STATE, time taken: 45.979 seconds\n",
       "26/02/04 01:05:02 INFO DAGScheduler: Asked to cancel job group df6fb7bc-6438-4de6-9618-e4bca455b018\n",
       "2026-02-04T01:05:03,255Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:05:03,405Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/df6fb7bc-6438-4de6-9618-e4bca455b018/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:05:03 INFO DAGScheduler: Asked to cancel job group df6fb7bc-6438-4de6-9618-e4bca455b018\n",
       "2026-02-04T01:05:12,692Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T01:05:12,692Z WARN KyuubiOperationManager: Operation OperationHandle [c3ede94c-2b96-4564-9b92-e60120983338] is timed-out and will be closed\n",
       "2026-02-04T01:05:12,692Z WARN KyuubiOperationManager: Operation OperationHandle [e3bc3229-0322-454f-82f3-9ffb0cb62ab0] is timed-out and will be closed\n",
       "2026-02-04T01:05:12,692Z WARN KyuubiOperationManager: Operation OperationHandle [e9404517-68b6-4cd2-8fb7-d08a3027aea2] is timed-out and will be closed\n",
       "2026-02-04T01:05:12,695Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:E3 BC 32 29 03 22 45 4F 82 F3 9F FB 0C B6 2A B0, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T01:05:12,696Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:E9 40 45 17 68 B6 4C D2 8F B7 D0 8A 30 27 AE A2, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:05:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "26/02/04 01:05:29 WARN SparkSQLOperationManager: Operation OperationHandle [336e8100-338a-4c77-8a37-33726882f542] is timed-out and will be closed\n",
       "26/02/04 01:05:29 WARN SparkSQLOperationManager: Operation OperationHandle [56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb] is timed-out and will be closed\n",
       "2026-02-04T01:05:42,697Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T01:05:42,697Z WARN KyuubiOperationManager: Operation OperationHandle [336e8100-338a-4c77-8a37-33726882f542] is timed-out and will be closed\n",
       "2026-02-04T01:05:42,697Z WARN KyuubiOperationManager: Operation OperationHandle [56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb] is timed-out and will be closed\n",
       "2026-02-04T01:05:42,698Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:33 6E 81 00 33 8A 4C 77 8A 37 33 72 68 82 F5 42, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [336e8100-338a-4c77-8a37-33726882f542]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T01:05:42,700Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:56 76 3E 01 1B 3E 46 AD 8E 46 C8 AA A5 AC 9B BB, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:05:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [336e8100-338a-4c77-8a37-33726882f542]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:05:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [56763e01-1b3e-46ad-8e46-c8aaa5ac9bbb]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:05:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:06:12,700Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T01:06:12,700Z WARN KyuubiOperationManager: Operation OperationHandle [df6fb7bc-6438-4de6-9618-e4bca455b018] is timed-out and will be closed\n",
       "2026-02-04T01:06:12,701Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:DF 6F B7 BC 64 38 4D E6 96 18 E4 BC A4 55 B0 18, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:06:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:06:42,701Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:06:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "26/02/04 01:07:01 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.4.62:55540\n",
       "2026-02-04T01:07:09,657Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T01:07:09,657Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T01:07:09,658Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T01:07:09,658Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@38ff24a6\n",
       "2026-02-04T01:07:09,658Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T01:07:09,658Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T01:07:09,659Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T01:07:09,659Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T01:07:09,661Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-04T01:07:09,662Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T01:07:09,663Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:51672, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-04T01:07:09,667Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b5b2d3, negotiated timeout = 120000\n",
       "2026-02-04T01:07:09,667Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T01:07:09,669Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:07:09,671Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T01:07:09,671Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T01:07:09,671Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:07:09,775Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b5b2d3\n",
       "2026-02-04T01:07:09,775Z INFO ZooKeeper: Session: 0x30263a585b5b2d3 closed\n",
       "2026-02-04T01:07:09,779Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:10,026Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.216.117\n",
       "2026-02-04T01:07:10,026Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T01:07:10,026Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T01:07:10,027Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/d530b833-07ee-4e0e-ac9d-b1d7748cb8f2\n",
       "2026-02-04T01:07:10,027Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]/kernel-v3b8e3c6138a1cf45790ec5e0b52755323126f16f3 is opened, current opening sessions 6\n",
       "2026-02-04T01:07:10,028Z INFO LaunchEngine: Processing anonymous's query[d530b833-07ee-4e0e-ac9d-b1d7748cb8f2]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:07:10,028Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:10,028Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T01:07:10,029Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@271a8d57\n",
       "2026-02-04T01:07:10,029Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T01:07:10,029Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T01:07:10,029Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T01:07:10,030Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-04T01:07:10,030Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T01:07:10,031Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:47632, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-04T01:07:10,036Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f4aea8, negotiated timeout = 120000\n",
       "2026-02-04T01:07:10,036Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T01:07:10,037Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:07:10,037Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T01:07:10,038Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T01:07:10,050Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]]\n",
       "2026-02-04T01:07:10,050Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T01:07:10,154Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f4aea8\n",
       "2026-02-04T01:07:10,154Z INFO ZooKeeper: Session: 0x200d89556f4aea8 closed\n",
       "2026-02-04T01:07:10,155Z INFO LaunchEngine: Processing anonymous's query[d530b833-07ee-4e0e-ac9d-b1d7748cb8f2]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.127 seconds\n",
       "2026-02-04T01:07:10,272Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204010709-3uje0m9i]: associated with Kyuubi SessionHandle: [5d19cc21-fea8-4341-b33b-b5ae4053cabe]\n",
       "2026-02-04T01:07:10,272Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1\n",
       "2026-02-04T01:07:10,272Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T01:07:10,272Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-04T01:07:10,277Z INFO ExecuteStatement: Processing anonymous's query[d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:07:10,278Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/5d19cc21-fea8-4341-b33b-b5ae4053cabe/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204010709-3uje0m9i\n",
       "2026-02-04T01:07:10,282Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:07:10 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 01:07:10 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 01:07:10 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]\n",
       "26/02/04 01:07:10 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]/kernel-v3b8e3c6138a1cf45790ec5e0b52755323126f16f3 is opened, current opening sessions 11\n",
       "26/02/04 01:07:10 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 01:07:10 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 01:07:10 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [750c7b4d-5cf2-46d1-944d-b235af1c7485]\n",
       "26/02/04 01:07:10 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [750c7b4d-5cf2-46d1-944d-b235af1c7485]/5d19cc21-fea8-4341-b33b-b5ae4053cabe_aliveness_probe is opened, current opening sessions 12\n",
       "26/02/04 01:07:10 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204010709-3uje0m9i)]: associated with spark-sql operation session: [5d19cc21-fea8-4341-b33b-b5ae4053cabe]\n",
       "26/02/04 01:07:10 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-a696761f-7b6e-411a-8c67-dfccbe4aef1a/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:a:6:9:6:7:6:1:f:-:7:b:6:e:-:4:1:1:a:-:8:c:6:7:-:d:f:c:c:b:e:4:a:e:f:1:a\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-a696761f-7b6e-411a-8c67-dfccbe4aef1a/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=5d19cc21-fea8-4341-b33b-b5ae4053cabe\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/04 01:07:10 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1\n",
       "2026-02-04T01:07:10,780Z INFO ExecuteStatement: Query[d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1] in FINISHED_STATE\n",
       "2026-02-04T01:07:10,780Z INFO ExecuteStatement: Processing anonymous's query[d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.503 seconds\n",
       "2026-02-04T01:07:11,287Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:11,296Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:11,442Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/0d733a64-caca-40e0-8898-695ff74fe142\n",
       "2026-02-04T01:07:11,444Z INFO ExecuteStatement: Processing anonymous's query[0d733a64-caca-40e0-8898-695ff74fe142]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:07:11,444Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/5d19cc21-fea8-4341-b33b-b5ae4053cabe/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:11,448Z INFO ExecuteStatement: Query[0d733a64-caca-40e0-8898-695ff74fe142] in FINISHED_STATE\n",
       "2026-02-04T01:07:11,448Z INFO ExecuteStatement: Processing anonymous's query[0d733a64-caca-40e0-8898-695ff74fe142]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "26/02/04 01:07:10 INFO ExecutePython: Processing anonymous's query[d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:07:10 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:07:10 INFO ExecutePython: Processing anonymous's query[d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.006 seconds\n",
       "26/02/04 01:07:10 INFO DAGScheduler: Asked to cancel job group d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1\n",
       "26/02/04 01:07:11 INFO DAGScheduler: Asked to cancel job group d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1\n",
       "26/02/04 01:07:11 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/0d733a64-caca-40e0-8898-695ff74fe142\n",
       "26/02/04 01:07:11 INFO ExecutePython: Processing anonymous's query[0d733a64-caca-40e0-8898-695ff74fe142]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:07:11 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:07:11 INFO ExecutePython: Processing anonymous's query[0d733a64-caca-40e0-8898-695ff74fe142]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.0 seconds\n",
       "26/02/04 01:07:11 INFO DAGScheduler: Asked to cancel job group 0d733a64-caca-40e0-8898-695ff74fe142\n",
       "2026-02-04T01:07:11,990Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/63f1e4d7-9ace-4f6f-af61-d3086a6c5573\n",
       "2026-02-04T01:07:11,992Z INFO ExecuteStatement: Processing anonymous's query[63f1e4d7-9ace-4f6f-af61-d3086a6c5573]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:07:11,993Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/5d19cc21-fea8-4341-b33b-b5ae4053cabe/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:12,035Z INFO ExecuteStatement: Query[63f1e4d7-9ace-4f6f-af61-d3086a6c5573] in FINISHED_STATE\n",
       "2026-02-04T01:07:12,035Z INFO ExecuteStatement: Processing anonymous's query[63f1e4d7-9ace-4f6f-af61-d3086a6c5573]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.043 seconds\n",
       "2026-02-04T01:07:12,488Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f1e4d7-9ace-4f6f-af61-d3086a6c5573/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:12,636Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f1e4d7-9ace-4f6f-af61-d3086a6c5573/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:07:11 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/63f1e4d7-9ace-4f6f-af61-d3086a6c5573\n",
       "26/02/04 01:07:11 INFO ExecutePython: Processing anonymous's query[63f1e4d7-9ace-4f6f-af61-d3086a6c5573]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:07:11 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:07:12 INFO ExecutePython: Processing anonymous's query[63f1e4d7-9ace-4f6f-af61-d3086a6c5573]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.039 seconds\n",
       "26/02/04 01:07:12 INFO DAGScheduler: Asked to cancel job group 63f1e4d7-9ace-4f6f-af61-d3086a6c5573\n",
       "26/02/04 01:07:12 INFO DAGScheduler: Asked to cancel job group 63f1e4d7-9ace-4f6f-af61-d3086a6c5573\n",
       "2026-02-04T01:07:12,702Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:07:14,784Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/a0043ed0-1cc7-4952-a9b6-5549df369f6e\n",
       "2026-02-04T01:07:14,786Z INFO ExecuteStatement: Processing anonymous's query[a0043ed0-1cc7-4952-a9b6-5549df369f6e]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:07:14,786Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/5d19cc21-fea8-4341-b33b-b5ae4053cabe/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:14,790Z INFO ExecuteStatement: Query[a0043ed0-1cc7-4952-a9b6-5549df369f6e] in FINISHED_STATE\n",
       "2026-02-04T01:07:14,791Z INFO ExecuteStatement: Processing anonymous's query[a0043ed0-1cc7-4952-a9b6-5549df369f6e]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "2026-02-04T01:07:15,296Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/63efabda-80ce-4953-aa5f-f8d91dcf7bba\n",
       "2026-02-04T01:07:15,298Z INFO ExecuteStatement: Processing anonymous's query[63efabda-80ce-4953-aa5f-f8d91dcf7bba]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:07:15,299Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/5d19cc21-fea8-4341-b33b-b5ae4053cabe/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:07:14 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/a0043ed0-1cc7-4952-a9b6-5549df369f6e\n",
       "26/02/04 01:07:14 INFO ExecutePython: Processing anonymous's query[a0043ed0-1cc7-4952-a9b6-5549df369f6e]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:07:14 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:07:14 INFO ExecutePython: Processing anonymous's query[a0043ed0-1cc7-4952-a9b6-5549df369f6e]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:07:14 INFO DAGScheduler: Asked to cancel job group a0043ed0-1cc7-4952-a9b6-5549df369f6e\n",
       "26/02/04 01:07:15 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/5d19cc21-fea8-4341-b33b-b5ae4053cabe/63efabda-80ce-4953-aa5f-f8d91dcf7bba\n",
       "26/02/04 01:07:15 INFO ExecutePython: Processing anonymous's query[63efabda-80ce-4953-aa5f-f8d91dcf7bba]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:07:15 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:07:15 INFO GpuOverrides: Plan conversion to the GPU took 0.18 ms\n",
       "26/02/04 01:07:15 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 01:07:15 INFO GpuOverrides: Plan conversion to the GPU took 0.27 ms\n",
       "26/02/04 01:07:15 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "2026-02-04T01:07:15,787Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63efabda-80ce-4953-aa5f-f8d91dcf7bba/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:07:15 INFO InMemoryFileIndex: It took 33 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:07:15 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Got job 15 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Final stage: ResultStage 17 (sql at <unknown>:0)\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[97] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 01:07:15 INFO SQLOperationListener: Query [63efabda-80ce-4953-aa5f-f8d91dcf7bba]: Job 15 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:07:15 INFO SQLOperationListener: Query [63efabda-80ce-4953-aa5f-f8d91dcf7bba]: Stage 17.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:07:15 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:07:15 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:07:15 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:07:15 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[97] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:07:15 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:07:15 INFO FairSchedulableBuilder: Added task set TaskSet_17.0 tasks to pool \n",
       "26/02/04 01:07:15 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (100.67.4.62, executor 3, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:07:15 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 100.67.4.62:41749 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:07:15 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 169 ms on 100.67.4.62 (executor 3) (1/1)\n",
       "26/02/04 01:07:15 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:07:15 INFO DAGScheduler: ResultStage 17 (sql at <unknown>:0) finished in 0.184 s\n",
       "26/02/04 01:07:15 INFO SQLOperationListener: Finished stage: Stage(17, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 184 msec\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:07:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
       "26/02/04 01:07:15 INFO DAGScheduler: Job 15 finished: sql at <unknown>:0, took 0.187805 s\n",
       "26/02/04 01:07:15 INFO StatsReportListener: task runtime:(count: 1, mean: 169.000000, stdev: 0.000000, max: 169.000000, min: 169.000000)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t169.0 ms\t169.0 ms\t169.0 ms\t169.0 ms\t169.0 ms\t169.0 ms\t169.0 ms\t169.0 ms\t169.0 ms\n",
       "26/02/04 01:07:15 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:07:15 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:07:15 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:07:15 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 01:07:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 85.798817, stdev: 0.000000, max: 85.798817, min: 85.798817)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t86 %\t86 %\t86 %\t86 %\t86 %\t86 %\t86 %\t86 %\t86 %\n",
       "26/02/04 01:07:15 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:07:15 INFO StatsReportListener: other time pct: (count: 1, mean: 14.201183, stdev: 0.000000, max: 14.201183, min: 14.201183)\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:07:15 INFO StatsReportListener: \t14 %\t14 %\t14 %\t14 %\t14 %\t14 %\t14 %\t14 %\t14 %\n",
       "26/02/04 01:07:15 INFO SparkSQLEngineListener: Job end. Job 15 state is JobSucceeded\n",
       "26/02/04 01:07:15 INFO SQLOperationListener: Query [63efabda-80ce-4953-aa5f-f8d91dcf7bba]: Job 15 succeeded, 0 active jobs running\n",
       "26/02/04 01:07:16 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`gcp_east4_maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 01:07:16 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`gcp_east4_maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "26/02/04 01:07:16 INFO GpuOverrides: Plan conversion to the GPU took 0.30 ms\n",
       "26/02/04 01:07:16 INFO GpuOverrides: GPU plan transition optimization took 0.12 ms\n",
       "26/02/04 01:07:16 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#608 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#609 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#610 could run on GPU\n",
       "\n",
       "26/02/04 01:07:16 INFO GpuOverrides: Plan conversion to the GPU took 0.58 ms\n",
       "26/02/04 01:07:16 INFO GpuOverrides: GPU plan transition optimization took 0.16 ms\n",
       "26/02/04 01:07:16 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:07:16 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 01:07:16 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:07:16 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 01:07:16 INFO ExecutePython: +-----------------+---------+-------+\n",
       "2026-02-04T01:07:16,938Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63efabda-80ce-4953-aa5f-f8d91dcf7bba/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:17,059Z INFO ExecuteStatement: Query[63efabda-80ce-4953-aa5f-f8d91dcf7bba] in FINISHED_STATE\n",
       "2026-02-04T01:07:17,059Z INFO ExecuteStatement: Processing anonymous's query[63efabda-80ce-4953-aa5f-f8d91dcf7bba]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.76 seconds\n",
       "26/02/04 01:07:16 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:07:16 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:07:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:07:16 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 01:07:16 INFO GpuOverrides: Plan conversion to the GPU took 1.75 ms\n",
       "26/02/04 01:07:16 INFO GpuOverrides: GPU plan transition optimization took 0.70 ms\n",
       "26/02/04 01:07:16 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:07:16 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:07:16 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:07:16 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:07:16 INFO SparkContext: Created broadcast 23 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:07:16 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:07:16 INFO DAGScheduler: Got job 16 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:07:16 INFO DAGScheduler: Final stage: ResultStage 18 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:07:16 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:07:16 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:07:16 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[106] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:07:16 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:07:16 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:07:16 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:07:16 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:07:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[106] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:07:16 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:07:16 INFO FairSchedulableBuilder: Added task set TaskSet_18.0 tasks to pool \n",
       "26/02/04 01:07:16 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (100.67.4.62, executor 3, partition 0, PROCESS_LOCAL, 10142 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:07:16 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 100.67.4.62:41749 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:07:16 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 100.67.4.62:41749 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:07:17 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 206 ms on 100.67.4.62 (executor 3) (1/1)\n",
       "26/02/04 01:07:17 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:07:17 INFO DAGScheduler: ResultStage 18 (showString at NativeMethodAccessorImpl.java:0) finished in 0.212 s\n",
       "26/02/04 01:07:17 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:07:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
       "26/02/04 01:07:17 INFO SparkSQLEngineListener: Job end. Job 16 state is JobSucceeded\n",
       "26/02/04 01:07:17 INFO DAGScheduler: Job 16 finished: showString at NativeMethodAccessorImpl.java:0, took 0.214952 s\n",
       "26/02/04 01:07:17 INFO ExecutePython: +-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:07:17 INFO ExecutePython: |node_id      |scontrol_state                        |reason                            |updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 01:07:17 INFO ExecutePython: +-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-001   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-002   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-003   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-004   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-005   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-006   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-007   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-008   |[\"MIXED\"]                             |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-009   |[\"IDLE\"]                              |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:00:45+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-010   |[\"IDLE\"]                              |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-dm-011   |[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T07:56:12+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-large-004|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T08:10:44+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |cpu-large-005|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-02T23:00:14+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |nvl72D004-T01|[\"DOWN\", \"RESERVED\", \"NOT_RESPONDING\"]|[A] DCA Automation; Not responding|2026-02-04T01:05:08.861535+00:00|false|2026-02-02T18:36:58+00:00|gcp-iad-cs-001-v1|2026-02-02T17:58:39+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |nvl72D005-T07|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:10:58+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |nvl72D005-T08|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:11:00+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |nvl72D005-T09|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:11:00+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |nvl72D006-T17|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T08:51:28+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |nvl72D007-T02|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T20:20:21+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: |nvl72D010-T05|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:05:08.861535+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T19:15:20+00:00|\n",
       "26/02/04 01:07:17 INFO ExecutePython: +-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:07:17 INFO ExecutePython: only showing top 20 rows\n",
       "26/02/04 01:07:17 INFO ExecutePython: Processing anonymous's query[63efabda-80ce-4953-aa5f-f8d91dcf7bba]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.756 seconds\n",
       "26/02/04 01:07:17 INFO DAGScheduler: Asked to cancel job group 63efabda-80ce-4953-aa5f-f8d91dcf7bba\n",
       "2026-02-04T01:07:18,086Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63efabda-80ce-4953-aa5f-f8d91dcf7bba/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:07:18,238Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63efabda-80ce-4953-aa5f-f8d91dcf7bba/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:07:18 INFO DAGScheduler: Asked to cancel job group 63efabda-80ce-4953-aa5f-f8d91dcf7bba\n",
       "26/02/04 01:07:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:07:42,702Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:07:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:08:12,702Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:08:12,703Z WARN KyuubiOperationManager: Operation OperationHandle [d530b833-07ee-4e0e-ac9d-b1d7748cb8f2] is timed-out and will be closed\n",
       "2026-02-04T01:08:12,703Z WARN KyuubiOperationManager: Operation OperationHandle [0d733a64-caca-40e0-8898-695ff74fe142] is timed-out and will be closed\n",
       "2026-02-04T01:08:12,703Z WARN KyuubiOperationManager: Operation OperationHandle [d8fd7cb5-b49b-4d81-8a16-4da0b87ab7c1] is timed-out and will be closed\n",
       "2026-02-04T01:08:12,703Z WARN KyuubiOperationManager: Operation OperationHandle [63f1e4d7-9ace-4f6f-af61-d3086a6c5573] is timed-out and will be closed\n",
       "2026-02-04T01:08:12,705Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:0D 73 3A 64 CA CA 40 E0 88 98 69 5F F7 4F E1 42, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T01:08:12,707Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:D8 FD 7C B5 B4 9B 4D 81 8A 16 4D A0 B8 7A B7 C1, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T01:08:12,708Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:63 F1 E4 D7 9A CE 4F 6F AF 61 D3 08 6A 6C 55 73, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:08:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "26/02/04 01:08:29 WARN SparkSQLOperationManager: Operation OperationHandle [a0043ed0-1cc7-4952-a9b6-5549df369f6e] is timed-out and will be closed\n",
       "26/02/04 01:08:29 WARN SparkSQLOperationManager: Operation OperationHandle [63efabda-80ce-4953-aa5f-f8d91dcf7bba] is timed-out and will be closed\n",
       "2026-02-04T01:08:42,708Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:08:42,708Z WARN KyuubiOperationManager: Operation OperationHandle [a0043ed0-1cc7-4952-a9b6-5549df369f6e] is timed-out and will be closed\n",
       "2026-02-04T01:08:42,709Z WARN KyuubiOperationManager: Operation OperationHandle [63efabda-80ce-4953-aa5f-f8d91dcf7bba] is timed-out and will be closed\n",
       "2026-02-04T01:08:42,710Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:A0 04 3E D0 1C C7 49 52 A9 B6 55 49 DF 36 9F 6E, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [a0043ed0-1cc7-4952-a9b6-5549df369f6e]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T01:08:42,711Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:63 EF AB DA 80 CE 49 53 AA 5F F8 D9 1D CF 7B BA, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [63efabda-80ce-4953-aa5f-f8d91dcf7bba]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:08:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [a0043ed0-1cc7-4952-a9b6-5549df369f6e]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:08:42 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [63efabda-80ce-4953-aa5f-f8d91dcf7bba]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:08:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:09:12,712Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:09:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:09:42,712Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:09:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:10:12,713Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:10:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:10:42,728Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "5412.596: [GC (System.gc()) [PSYoungGen: 632508K->8738K(1049088K)] 710977K->87215K(4891648K), 0.0130487 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] \n",
       "5412.609: [Full GC (System.gc()) [PSYoungGen: 8738K->0K(1049088K)] [ParOldGen: 78477K->82880K(3842560K)] 87215K->82880K(4891648K), [Metaspace: 205708K->205708K(1247232K)], 0.4381636 secs] [Times: user=0.91 sys=0.01, real=0.44 secs] \n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 100.67.4.62:41749 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 100.67.4.62:41749 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 100.67.4.62:41749 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 100.67.4.62:41749 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 100.67.4.62:41749 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:10:58 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 100.67.4.62:41749 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:10:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:11:12,729Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:11:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:11:42,729Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:11:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:12:12,729Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:12:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:12:42,729Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:12:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:13:12,730Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:13:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:13:42,730Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:13:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:14:12,730Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:14:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:14:42,730Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:14:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:15:12,731Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:15:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:15:42,731Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:15:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:16:12,731Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:16:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:16:42,731Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:16:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:17:12,732Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:17:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:17:42,732Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:17:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:18:12,732Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:18:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:18:42,732Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:18:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:19:12,733Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:19:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:19:42,733Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:19:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:20:12,733Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:20:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:20:42,733Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:20:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:21:12,734Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:21:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:21:42,734Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:21:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:22:12,734Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "26/02/04 01:22:17 INFO KubernetesClusterSchedulerBackend: Requesting to kill executor(s) 3\n",
       "26/02/04 01:22:17 INFO KubernetesClusterSchedulerBackend: Actual list of executor(s) to be killed is 3\n",
       "26/02/04 01:22:17 INFO ExecutorAllocationManager: Executors 3 removed due to idle timeout.\n",
       "26/02/04 01:22:17 INFO TaskSchedulerImpl: Executor 3 on 100.67.4.62 killed by driver.\n",
       "26/02/04 01:22:17 INFO DAGScheduler: Executor lost: 3 (epoch 4)\n",
       "26/02/04 01:22:17 INFO ExecutorMonitor: Executor 3 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 3, unexpectedly exited: 0).\n",
       "26/02/04 01:22:17 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.\n",
       "26/02/04 01:22:17 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, 100.67.4.62, 41749, None)\n",
       "26/02/04 01:22:17 INFO BlockManagerMaster: Removed 3 successfully in removeExecutor\n",
       "26/02/04 01:22:17 INFO DAGScheduler: Shuffle files lost for executor: 3 (epoch 4)\n",
       "26/02/04 01:22:18 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.4.62:55534\n",
       "26/02/04 01:22:19 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.\n",
       "26/02/04 01:22:19 INFO BlockManagerMaster: Removal of executor 3 requested\n",
       "26/02/04 01:22:19 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: Asked to remove non-existent executor 3\n",
       "26/02/04 01:22:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 12\n",
       "2026-02-04T01:22:42,735Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:22:42,735Z INFO KyuubiSessionManager: Closing session f7df5191-9229-40df-86fa-ee5b5502e2f5 that has been idle for more than 3600000 ms\n",
       "2026-02-04T01:22:42,735Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]/kernel-v38feb1b0b9f19ee2a784f50d0f9f8f896d87d022f is closed, current opening sessions 5\n",
       "26/02/04 01:22:42 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]\n",
       "26/02/04 01:22:42 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]\n",
       "26/02/04 01:22:42 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]/kernel-v38feb1b0b9f19ee2a784f50d0f9f8f896d87d022f is closed, current opening sessions 11\n",
       "26/02/04 01:22:42 INFO ExecutePython: send SIGCONT to 413\n",
       "26/02/04 01:22:42 ERROR ExecutePython: python worker exit\n",
       "26/02/04 01:22:42 WARN ExecutePython: Process has been interrupted\n",
       "26/02/04 01:22:42 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [f7df5191-9229-40df-86fa-ee5b5502e2f5]\n",
       "26/02/04 01:22:42 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [5fe59e9d-06b9-4a4d-9106-05d917d255d8]\n",
       "26/02/04 01:22:42 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [5fe59e9d-06b9-4a4d-9106-05d917d255d8]\n",
       "26/02/04 01:22:42 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [5fe59e9d-06b9-4a4d-9106-05d917d255d8]/f7df5191-9229-40df-86fa-ee5b5502e2f5_aliveness_probe is closed, current opening sessions 10\n",
       "26/02/04 01:22:42 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [5fe59e9d-06b9-4a4d-9106-05d917d255d8]\n",
       "26/02/04 01:22:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:23:12,749Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:23:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:23:42,750Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:23:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:24:12,750Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:24:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:24:42,750Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:24:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:25:12,750Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:25:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:25:42,751Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:25:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:26:12,751Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:26:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:26:42,751Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:26:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:27:12,751Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:27:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:27:42,752Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:27:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:28:12,752Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:28:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:28:42,752Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:28:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:29:12,752Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:29:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:29:42,753Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:29:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:30:12,753Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:30:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:30:42,753Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:30:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:31:12,753Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:31:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:31:42,754Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:31:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:32:12,754Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:32:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:32:42,754Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 01:32:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:33:12,754Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "6763.482: [GC (Allocation Failure) [PSYoungGen: 995328K->1232K(986624K)] 1078208K->84113K(4829184K), 0.0090215 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \n",
       "26/02/04 01:33:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:33:42,755Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T01:33:56,320Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/b01604f4-fb53-4548-9eac-1516c0c8ee40\n",
       "2026-02-04T01:33:56,322Z INFO ExecuteStatement: Processing anonymous's query[b01604f4-fb53-4548-9eac-1516c0c8ee40]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:33:56,322Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:33:56,326Z INFO ExecuteStatement: Query[b01604f4-fb53-4548-9eac-1516c0c8ee40] in FINISHED_STATE\n",
       "2026-02-04T01:33:56,327Z INFO ExecuteStatement: Processing anonymous's query[b01604f4-fb53-4548-9eac-1516c0c8ee40]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T01:33:56,838Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/15a87898-2998-4380-86c2-0ca00eb50a5f\n",
       "2026-02-04T01:33:56,840Z INFO ExecuteStatement: Processing anonymous's query[15a87898-2998-4380-86c2-0ca00eb50a5f]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:33:56,840Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:33:56 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/b01604f4-fb53-4548-9eac-1516c0c8ee40\n",
       "26/02/04 01:33:56 INFO ExecutePython: Processing anonymous's query[b01604f4-fb53-4548-9eac-1516c0c8ee40]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:33:56 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:33:56 INFO ExecutePython: Processing anonymous's query[b01604f4-fb53-4548-9eac-1516c0c8ee40]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:33:56 INFO DAGScheduler: Asked to cancel job group b01604f4-fb53-4548-9eac-1516c0c8ee40\n",
       "26/02/04 01:33:56 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/15a87898-2998-4380-86c2-0ca00eb50a5f\n",
       "26/02/04 01:33:56 INFO ExecutePython: Processing anonymous's query[15a87898-2998-4380-86c2-0ca00eb50a5f]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:33:56 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "2026-02-04T01:33:56,867Z INFO ExecuteStatement: Query[15a87898-2998-4380-86c2-0ca00eb50a5f] in ERROR_STATE\n",
       "2026-02-04T01:33:56,867Z INFO ExecuteStatement: Processing anonymous's query[15a87898-2998-4380-86c2-0ca00eb50a5f]: RUNNING_STATE -> ERROR_STATE, time taken: 0.027 seconds\n",
       "2026-02-04T01:33:57,331Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/15a87898-2998-4380-86c2-0ca00eb50a5f/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:33:57,478Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/15a87898-2998-4380-86c2-0ca00eb50a5f/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:33:56 INFO ExecutePython: Processing anonymous's query[15a87898-2998-4380-86c2-0ca00eb50a5f]: RUNNING_STATE -> ERROR_STATE, time taken: 0.023 seconds\n",
       "26/02/04 01:33:56 INFO DAGScheduler: Asked to cancel job group 15a87898-2998-4380-86c2-0ca00eb50a5f\n",
       "26/02/04 01:33:57 INFO DAGScheduler: Asked to cancel job group 15a87898-2998-4380-86c2-0ca00eb50a5f\n",
       "26/02/04 01:33:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T01:34:12,755Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T01:34:12,755Z INFO KyuubiSessionManager: Closing session eb9b5e08-fcce-4e25-bf46-bccbda8aed8b that has been idle for more than 3600000 ms\n",
       "2026-02-04T01:34:12,755Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]/kernel-v382dc89fda55e489b27f77a22164229a625d88e58 is closed, current opening sessions 4\n",
       "26/02/04 01:34:12 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]\n",
       "26/02/04 01:34:12 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]\n",
       "26/02/04 01:34:12 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]/kernel-v382dc89fda55e489b27f77a22164229a625d88e58 is closed, current opening sessions 9\n",
       "26/02/04 01:34:12 INFO ExecutePython: send SIGCONT to 2593\n",
       "26/02/04 01:34:12 WARN ExecutePython: Process has been interrupted\n",
       "26/02/04 01:34:12 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [eb9b5e08-fcce-4e25-bf46-bccbda8aed8b]\n",
       "26/02/04 01:34:12 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [c0907992-9162-4a68-84d0-6dd58873e251]\n",
       "26/02/04 01:34:12 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [c0907992-9162-4a68-84d0-6dd58873e251]\n",
       "26/02/04 01:34:12 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [c0907992-9162-4a68-84d0-6dd58873e251]/eb9b5e08-fcce-4e25-bf46-bccbda8aed8b_aliveness_probe is closed, current opening sessions 8\n",
       "26/02/04 01:34:12 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [c0907992-9162-4a68-84d0-6dd58873e251]\n",
       "26/02/04 01:34:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:34:30,842Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/960e2ffd-d642-448e-b315-d7b3b2b750a8\n",
       "2026-02-04T01:34:30,844Z INFO ExecuteStatement: Processing anonymous's query[960e2ffd-d642-448e-b315-d7b3b2b750a8]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:34:30,844Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:30,848Z INFO ExecuteStatement: Query[960e2ffd-d642-448e-b315-d7b3b2b750a8] in FINISHED_STATE\n",
       "2026-02-04T01:34:30,848Z INFO ExecuteStatement: Processing anonymous's query[960e2ffd-d642-448e-b315-d7b3b2b750a8]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "26/02/04 01:34:30 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/960e2ffd-d642-448e-b315-d7b3b2b750a8\n",
       "26/02/04 01:34:30 INFO ExecutePython: Processing anonymous's query[960e2ffd-d642-448e-b315-d7b3b2b750a8]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:34:30 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:34:30 INFO ExecutePython: Processing anonymous's query[960e2ffd-d642-448e-b315-d7b3b2b750a8]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:34:30 INFO DAGScheduler: Asked to cancel job group 960e2ffd-d642-448e-b315-d7b3b2b750a8\n",
       "2026-02-04T01:34:31,335Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/0bfc1654-afaa-4014-b5ba-d5d5477c2984\n",
       "2026-02-04T01:34:31,337Z INFO ExecuteStatement: Processing anonymous's query[0bfc1654-afaa-4014-b5ba-d5d5477c2984]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:34:31,338Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:31,829Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:34:31 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/0bfc1654-afaa-4014-b5ba-d5d5477c2984\n",
       "26/02/04 01:34:31 INFO ExecutePython: Processing anonymous's query[0bfc1654-afaa-4014-b5ba-d5d5477c2984]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:34:31 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:34:31 INFO GpuOverrides: Plan conversion to the GPU took 0.25 ms\n",
       "26/02/04 01:34:31 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 01:34:31 INFO GpuOverrides: Plan conversion to the GPU took 0.28 ms\n",
       "26/02/04 01:34:31 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 01:34:31 INFO InMemoryFileIndex: It took 35 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:34:31 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 01:34:31 INFO DAGScheduler: Got job 17 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 01:34:31 INFO DAGScheduler: Final stage: ResultStage 19 (sql at <unknown>:0)\n",
       "26/02/04 01:34:31 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:34:31 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:34:31 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[108] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 01:34:31 INFO SQLOperationListener: Query [0bfc1654-afaa-4014-b5ba-d5d5477c2984]: Job 17 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:34:31 INFO SQLOperationListener: Query [0bfc1654-afaa-4014-b5ba-d5d5477c2984]: Stage 19.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:34:31 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:34:31 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:34:31 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:34:31 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[108] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:34:31 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:34:31 INFO FairSchedulableBuilder: Added task set TaskSet_19.0 tasks to pool \n",
       "2026-02-04T01:34:32,974Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:34,195Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:35,339Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:36,339Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:34:36,485Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:37,625Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:38,780Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:39,927Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:41,071Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:41,340Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "26/02/04 01:34:41 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1 for resource profile id: 0)\n",
       "26/02/04 01:34:41 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1, known: 0, sharedSlotFromPendingPods: 2147483647.\n",
       "26/02/04 01:34:41 INFO KubernetesClientUtils: Spark configuration files loaded from Some(/opt/spark/conf) : log4j2.properties,nvoauth.conf,metrics.properties\n",
       "26/02/04 01:34:41 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
       "26/02/04 01:34:41 INFO SparkExecutorFsGroupFeatureStep: SparkExecutorFsGroupFeatureStep configure security context fsGroup\n",
       "26/02/04 01:34:41 INFO ExecutorPodsAllocator: Trying to create PersistentVolumeClaim cluster-20260203202803-yawkv5ak-exec-4-pvc-0 with StorageClass gp3\n",
       "2026-02-04T01:34:42,215Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:42,765Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:34:43,360Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:44,532Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:45,677Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:46,341Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:34:46,830Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:47,983Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:49,127Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:50,273Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:51,342Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:34:51,427Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:52,589Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:53,734Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:54,888Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:56,040Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:56,343Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:34:57,187Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:58,341Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:34:59,484Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:34:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "26/02/04 01:34:59 WARN SparkSQLOperationManager: Operation OperationHandle [15a87898-2998-4380-86c2-0ca00eb50a5f] is timed-out and will be closed\n",
       "26/02/04 01:34:59 WARN SparkSQLOperationManager: Operation OperationHandle [b01604f4-fb53-4548-9eac-1516c0c8ee40] is timed-out and will be closed\n",
       "2026-02-04T01:35:00,627Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:00 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.4.240:45718\n",
       "26/02/04 01:35:00 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (100.67.4.240:45730) with ID 4,  ResourceProfileId 0\n",
       "26/02/04 01:35:00 INFO ExecutorMonitor: New executor 4 has registered (new total is 1)\n",
       "26/02/04 01:35:00 INFO BlockManagerMasterEndpoint: Registering block manager 100.67.4.240:46241 with 9.0 GiB RAM, BlockManagerId(4, 100.67.4.240, 46241, None)\n",
       "2026-02-04T01:35:01,344Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:35:01,766Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:02,918Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:04,064Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:05,210Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:06,345Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:35:06,355Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:07,509Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:08,654Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:09,801Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:09 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:35:10 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 100.67.4.240:46241 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:35:10,950Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:11,346Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:35:12,102Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:12,765Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:35:12,765Z WARN KyuubiOperationManager: Operation OperationHandle [15a87898-2998-4380-86c2-0ca00eb50a5f] is timed-out and will be closed\n",
       "2026-02-04T01:35:12,765Z WARN KyuubiOperationManager: Operation OperationHandle [b01604f4-fb53-4548-9eac-1516c0c8ee40] is timed-out and will be closed\n",
       "2026-02-04T01:35:13,254Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:13 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 3787 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:35:13 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:35:13 INFO DAGScheduler: ResultStage 19 (sql at <unknown>:0) finished in 42.099 s\n",
       "26/02/04 01:35:13 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:35:13 INFO SQLOperationListener: Finished stage: Stage(19, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 42099 msec\n",
       "26/02/04 01:35:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
       "26/02/04 01:35:13 INFO DAGScheduler: Job 17 finished: sql at <unknown>:0, took 42.102295 s\n",
       "26/02/04 01:35:13 INFO StatsReportListener: task runtime:(count: 1, mean: 3787.000000, stdev: 0.000000, max: 3787.000000, min: 3787.000000)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t3.8 s\t3.8 s\t3.8 s\t3.8 s\t3.8 s\t3.8 s\t3.8 s\t3.8 s\t3.8 s\n",
       "26/02/04 01:35:13 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:35:13 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:35:13 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:35:13 INFO StatsReportListener: task result size:(count: 1, mean: 1975.000000, stdev: 0.000000, max: 1975.000000, min: 1975.000000)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\t1975.0 B\n",
       "26/02/04 01:35:13 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 83.469765, stdev: 0.000000, max: 83.469765, min: 83.469765)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\t83 %\n",
       "26/02/04 01:35:13 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:35:13 INFO StatsReportListener: other time pct: (count: 1, mean: 16.530235, stdev: 0.000000, max: 16.530235, min: 16.530235)\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:13 INFO StatsReportListener: \t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\t17 %\n",
       "26/02/04 01:35:13 INFO SparkSQLEngineListener: Job end. Job 17 state is JobSucceeded\n",
       "26/02/04 01:35:13 INFO SQLOperationListener: Query [0bfc1654-afaa-4014-b5ba-d5d5477c2984]: Job 17 succeeded, 0 active jobs running\n",
       "26/02/04 01:35:13 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`kratos\\`.\\`gcp_east4_maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 01:35:13 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`kratos\\`.\\`gcp_east4_maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "2026-02-04T01:35:14,399Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:14 WARN S3AFileSystem: Cannot create directory marker at s3a://kratos-spark-hive/datalake/kratos.db: java.nio.file.AccessDeniedException: datalake/kratos.db: PUT 0-byte object  on datalake/kratos.db: software.amazon.awssdk.services.s3.model.S3Exception: User: arn:aws:sts::900732750576:assumed-role/xp-dcartm-team-role/aws-sdk-java-1770162079103 is not authorized to perform: s3:PutObject on resource: \"arn:aws:s3:::kratos-spark-hive/datalake/kratos.db/\" because no identity-based policy allows the s3:PutObject action (Service: S3, Status Code: 403, Request ID: 47Z3W0CW3P8Y0HQ2, Extended Request ID: rMe1J7ZFLfGOqd2ax+Xoaoad+10gSUeJjWIZiVOCBSQMJvNQOzeOhKdYP1QkHcPa2WsXu1uB7Hqgnjkx/4q3i4YGtN4cDMpZ):AccessDenied\n",
       "26/02/04 01:35:14 INFO GpuOverrides: Plan conversion to the GPU took 0.26 ms\n",
       "26/02/04 01:35:14 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 01:35:14 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#685 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#686 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#687 could run on GPU\n",
       "\n",
       "26/02/04 01:35:14 INFO GpuOverrides: Plan conversion to the GPU took 0.59 ms\n",
       "26/02/04 01:35:14 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "26/02/04 01:35:14 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:35:14 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 01:35:14 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:35:14 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 01:35:14 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:35:14 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:35:14 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:35:14 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 01:35:14 INFO GpuOverrides: Plan conversion to the GPU took 1.48 ms\n",
       "26/02/04 01:35:14 INFO GpuOverrides: GPU plan transition optimization took 0.65 ms\n",
       "26/02/04 01:35:14 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:35:14 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:14 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:14 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:35:14 INFO SparkContext: Created broadcast 26 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:35:14 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:35:14 INFO DAGScheduler: Got job 18 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:35:14 INFO DAGScheduler: Final stage: ResultStage 20 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:35:14 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:35:14 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:35:14 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[117] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:35:14 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:14 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:14 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:35:14 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:35:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[117] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:35:14 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:35:14 INFO FairSchedulableBuilder: Added task set TaskSet_20.0 tasks to pool \n",
       "26/02/04 01:35:14 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10142 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:35:14 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:35:15,541Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:15 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:35:16,348Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:15 A8 78 98 29 98 43 80 86 C2 0C A0 0E B5 0A 5F, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [15a87898-2998-4380-86c2-0ca00eb50a5f]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T01:35:16,348Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in RUNNING_STATE\n",
       "2026-02-04T01:35:16,510Z INFO ExecuteStatement: Query[0bfc1654-afaa-4014-b5ba-d5d5477c2984] in FINISHED_STATE\n",
       "2026-02-04T01:35:16,510Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:B0 16 04 F4 FB 53 45 48 9E AC 15 16 C0 C8 EE 40, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [b01604f4-fb53-4548-9eac-1516c0c8ee40]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T01:35:16,510Z INFO ExecuteStatement: Processing anonymous's query[0bfc1654-afaa-4014-b5ba-d5d5477c2984]: RUNNING_STATE -> FINISHED_STATE, time taken: 45.172 seconds\n",
       "2026-02-04T01:35:16,687Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:16,840Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/0bfc1654-afaa-4014-b5ba-d5d5477c2984/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:16 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [15a87898-2998-4380-86c2-0ca00eb50a5f]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:35:16 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 2045 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:35:16 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:35:16 INFO DAGScheduler: ResultStage 20 (showString at NativeMethodAccessorImpl.java:0) finished in 2.049 s\n",
       "26/02/04 01:35:16 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:35:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
       "26/02/04 01:35:16 INFO SparkSQLEngineListener: Job end. Job 18 state is JobSucceeded\n",
       "26/02/04 01:35:16 INFO DAGScheduler: Job 18 finished: showString at NativeMethodAccessorImpl.java:0, took 2.052561 s\n",
       "26/02/04 01:35:16 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:35:16 INFO ExecutePython: |node_id   |scontrol_state|reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 01:35:16 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-001|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-002|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-003|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-004|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-005|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-006|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-007|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-008|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-009|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: |cpu-dm-010|[\"IDLE\"]      |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:16 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:35:16 INFO ExecutePython: Processing anonymous's query[0bfc1654-afaa-4014-b5ba-d5d5477c2984]: RUNNING_STATE -> FINISHED_STATE, time taken: 45.17 seconds\n",
       "26/02/04 01:35:16 INFO DAGScheduler: Asked to cancel job group 0bfc1654-afaa-4014-b5ba-d5d5477c2984\n",
       "26/02/04 01:35:16 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [b01604f4-fb53-4548-9eac-1516c0c8ee40]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:35:16 INFO DAGScheduler: Asked to cancel job group 0bfc1654-afaa-4014-b5ba-d5d5477c2984\n",
       "26/02/04 01:35:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:35:44,685Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/723473b3-3a61-4cf4-8e52-9eee429dab49\n",
       "2026-02-04T01:35:44,688Z INFO ExecuteStatement: Processing anonymous's query[723473b3-3a61-4cf4-8e52-9eee429dab49]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:35:44,688Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:44,692Z INFO ExecuteStatement: Query[723473b3-3a61-4cf4-8e52-9eee429dab49] in FINISHED_STATE\n",
       "2026-02-04T01:35:44,693Z INFO ExecuteStatement: Processing anonymous's query[723473b3-3a61-4cf4-8e52-9eee429dab49]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 01:35:44 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/723473b3-3a61-4cf4-8e52-9eee429dab49\n",
       "26/02/04 01:35:44 INFO ExecutePython: Processing anonymous's query[723473b3-3a61-4cf4-8e52-9eee429dab49]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:35:44 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:35:44 INFO ExecutePython: Processing anonymous's query[723473b3-3a61-4cf4-8e52-9eee429dab49]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:35:44 INFO DAGScheduler: Asked to cancel job group 723473b3-3a61-4cf4-8e52-9eee429dab49\n",
       "2026-02-04T01:35:45,228Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/9006e3ec-71b3-4eb6-84e1-ec8d844e1d11\n",
       "2026-02-04T01:35:45,230Z INFO ExecuteStatement: Processing anonymous's query[9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:35:45,230Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:45,769Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/9006e3ec-71b3-4eb6-84e1-ec8d844e1d11/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:45 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/9006e3ec-71b3-4eb6-84e1-ec8d844e1d11\n",
       "26/02/04 01:35:45 INFO ExecutePython: Processing anonymous's query[9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:35:45 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:35:45 INFO GpuOverrides: Plan conversion to the GPU took 0.18 ms\n",
       "26/02/04 01:35:45 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "26/02/04 01:35:45 INFO GpuOverrides: Plan conversion to the GPU took 0.25 ms\n",
       "26/02/04 01:35:45 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 01:35:45 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:35:45 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Got job 19 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Final stage: ResultStage 21 (sql at <unknown>:0)\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[119] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 01:35:45 INFO SQLOperationListener: Query [9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]: Job 19 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:35:45 INFO SQLOperationListener: Query [9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]: Stage 21.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:35:45 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:45 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:45 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:35:45 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[119] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:35:45 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:35:45 INFO FairSchedulableBuilder: Added task set TaskSet_21.0 tasks to pool \n",
       "26/02/04 01:35:45 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:35:45 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 100.67.4.240:46241 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:35:46,510Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:35:46,511Z WARN KyuubiOperationManager: Operation OperationHandle [960e2ffd-d642-448e-b315-d7b3b2b750a8] is timed-out and will be closed\n",
       "26/02/04 01:35:45 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 139 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:35:45 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:35:45 INFO DAGScheduler: ResultStage 21 (sql at <unknown>:0) finished in 0.153 s\n",
       "26/02/04 01:35:45 INFO SQLOperationListener: Finished stage: Stage(21, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 153 msec\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:35:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
       "26/02/04 01:35:45 INFO DAGScheduler: Job 19 finished: sql at <unknown>:0, took 0.157881 s\n",
       "26/02/04 01:35:45 INFO StatsReportListener: task runtime:(count: 1, mean: 139.000000, stdev: 0.000000, max: 139.000000, min: 139.000000)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t139.0 ms\t139.0 ms\t139.0 ms\t139.0 ms\t139.0 ms\t139.0 ms\t139.0 ms\t139.0 ms\t139.0 ms\n",
       "26/02/04 01:35:45 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:35:45 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:35:45 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:35:45 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 01:35:45 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 79.136691, stdev: 0.000000, max: 79.136691, min: 79.136691)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t79 %\t79 %\t79 %\t79 %\t79 %\t79 %\t79 %\t79 %\t79 %\n",
       "26/02/04 01:35:45 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:35:45 INFO StatsReportListener: other time pct: (count: 1, mean: 20.863309, stdev: 0.000000, max: 20.863309, min: 20.863309)\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:35:45 INFO StatsReportListener: \t21 %\t21 %\t21 %\t21 %\t21 %\t21 %\t21 %\t21 %\t21 %\n",
       "26/02/04 01:35:45 INFO SparkSQLEngineListener: Job end. Job 19 state is JobSucceeded\n",
       "26/02/04 01:35:45 INFO SQLOperationListener: Query [9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]: Job 19 succeeded, 0 active jobs running\n",
       "26/02/04 01:35:45 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`kratos\\`.\\`gcp_east4_maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 01:35:45 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`kratos\\`.\\`gcp_east4_maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "26/02/04 01:35:46 WARN S3AFileSystem: Cannot create directory marker at s3a://kratos-spark-hive/datalake/kratos.db: java.nio.file.AccessDeniedException: datalake/kratos.db: PUT 0-byte object  on datalake/kratos.db: software.amazon.awssdk.services.s3.model.S3Exception: User: arn:aws:sts::900732750576:assumed-role/xp-dcartm-team-role/aws-sdk-java-1770162079103 is not authorized to perform: s3:PutObject on resource: \"arn:aws:s3:::kratos-spark-hive/datalake/kratos.db/\" because no identity-based policy allows the s3:PutObject action (Service: S3, Status Code: 403, Request ID: ZJ7BP59M774G264D, Extended Request ID: AFtmS9ZK0LwEz3wsUd59Tp6RiEJ49Does6RlcUMw2M6LKG6AmYyiMUfheassdA366ymunRLJJXEy8G4GgEDPFA/K0IKjKjf+7qr6yaoACoI=):AccessDenied\n",
       "26/02/04 01:35:46 INFO GpuOverrides: Plan conversion to the GPU took 0.47 ms\n",
       "26/02/04 01:35:46 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 01:35:46 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#753 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#754 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#755 could run on GPU\n",
       "\n",
       "26/02/04 01:35:46 INFO GpuOverrides: Plan conversion to the GPU took 0.48 ms\n",
       "26/02/04 01:35:46 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "26/02/04 01:35:46 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:35:46 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 01:35:46 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:35:46 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 01:35:46 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 01:35:46 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:35:46 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:35:46 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:35:46 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 01:35:46 INFO GpuOverrides: Plan conversion to the GPU took 1.28 ms\n",
       "26/02/04 01:35:46 INFO GpuOverrides: GPU plan transition optimization took 0.99 ms\n",
       "26/02/04 01:35:46 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:35:46 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:46 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:46 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:35:46 INFO SparkContext: Created broadcast 29 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:35:46 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:35:46 INFO DAGScheduler: Got job 20 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:35:46 INFO DAGScheduler: Final stage: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:35:46 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:35:46 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:35:46 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[128] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:35:46 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:46 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:35:46 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:35:46 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:35:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[128] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:35:46 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:35:46 INFO FairSchedulableBuilder: Added task set TaskSet_22.0 tasks to pool \n",
       "26/02/04 01:35:46 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10142 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:35:46 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:35:46,912Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/9006e3ec-71b3-4eb6-84e1-ec8d844e1d11/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:47,030Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:96 0E 2F FD D6 42 44 8E B3 15 D7 B3 B2 B7 50 A8, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T01:35:47,031Z INFO ExecuteStatement: Query[9006e3ec-71b3-4eb6-84e1-ec8d844e1d11] in FINISHED_STATE\n",
       "2026-02-04T01:35:47,031Z INFO ExecuteStatement: Processing anonymous's query[9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.801 seconds\n",
       "26/02/04 01:35:46 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:35:47 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 197 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:35:47 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:35:47 INFO DAGScheduler: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0) finished in 0.203 s\n",
       "26/02/04 01:35:47 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:35:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
       "26/02/04 01:35:47 INFO SparkSQLEngineListener: Job end. Job 20 state is JobSucceeded\n",
       "26/02/04 01:35:47 INFO DAGScheduler: Job 20 finished: showString at NativeMethodAccessorImpl.java:0, took 0.206277 s\n",
       "26/02/04 01:35:47 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:35:47 INFO ExecutePython: |node_id   |scontrol_state|reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 01:35:47 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-001|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-002|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-003|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-004|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-005|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-006|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-007|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-008|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-009|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: |cpu-dm-010|[\"IDLE\"]      |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:35:47 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:35:47 INFO ExecutePython: Processing anonymous's query[9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.796 seconds\n",
       "26/02/04 01:35:47 INFO DAGScheduler: Asked to cancel job group 9006e3ec-71b3-4eb6-84e1-ec8d844e1d11\n",
       "2026-02-04T01:35:48,089Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/9006e3ec-71b3-4eb6-84e1-ec8d844e1d11/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:35:48,287Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/9006e3ec-71b3-4eb6-84e1-ec8d844e1d11/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:35:48 INFO DAGScheduler: Asked to cancel job group 9006e3ec-71b3-4eb6-84e1-ec8d844e1d11\n",
       "26/02/04 01:35:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:36:17,031Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:36:17,031Z WARN KyuubiOperationManager: Operation OperationHandle [0bfc1654-afaa-4014-b5ba-d5d5477c2984] is timed-out and will be closed\n",
       "2026-02-04T01:36:17,032Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:0B FC 16 54 AF AA 40 14 B5 BA D5 D5 47 7C 29 84, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:36:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T01:36:47,032Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:36:47,032Z WARN KyuubiOperationManager: Operation OperationHandle [723473b3-3a61-4cf4-8e52-9eee429dab49] is timed-out and will be closed\n",
       "2026-02-04T01:36:47,034Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:72 34 73 B3 3A 61 4C F4 8E 52 9E EE 42 9D AB 49, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:36:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "26/02/04 01:36:59 WARN SparkSQLOperationManager: Operation OperationHandle [9006e3ec-71b3-4eb6-84e1-ec8d844e1d11] is timed-out and will be closed\n",
       "26/02/04 01:37:16 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.4.240:45746\n",
       "2026-02-04T01:37:17,034Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:37:17,034Z WARN KyuubiOperationManager: Operation OperationHandle [9006e3ec-71b3-4eb6-84e1-ec8d844e1d11] is timed-out and will be closed\n",
       "2026-02-04T01:37:17,036Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:90 06 E3 EC 71 B3 4E B6 84 E1 EC 8D 84 4E 1D 11, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T01:37:17,036Z INFO KyuubiSessionManager: Closing session 43483002-0b3f-4759-bb37-140d8e96e1e9 that has been idle for more than 3600000 ms\n",
       "2026-02-04T01:37:17,036Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]/kernel-v3a0e16093c68d45f18c0600f70906b34ce1f45f18 is closed, current opening sessions 3\n",
       "26/02/04 01:37:17 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [9006e3ec-71b3-4eb6-84e1-ec8d844e1d11]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:37:17 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]\n",
       "26/02/04 01:37:17 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]\n",
       "26/02/04 01:37:17 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]/kernel-v3a0e16093c68d45f18c0600f70906b34ce1f45f18 is closed, current opening sessions 7\n",
       "26/02/04 01:37:17 INFO ExecutePython: send SIGCONT to 2852\n",
       "26/02/04 01:37:17 WARN ExecutePython: Process has been interrupted\n",
       "26/02/04 01:37:17 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [43483002-0b3f-4759-bb37-140d8e96e1e9]\n",
       "26/02/04 01:37:17 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [3cd755ba-6022-4629-a0ac-46fa6358e1dd]\n",
       "26/02/04 01:37:17 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [3cd755ba-6022-4629-a0ac-46fa6358e1dd]\n",
       "26/02/04 01:37:17 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [3cd755ba-6022-4629-a0ac-46fa6358e1dd]/43483002-0b3f-4759-bb37-140d8e96e1e9_aliveness_probe is closed, current opening sessions 6\n",
       "26/02/04 01:37:17 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [3cd755ba-6022-4629-a0ac-46fa6358e1dd]\n",
       "26/02/04 01:37:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:37:47,051Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:37:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:38:17,051Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:38:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:38:47,051Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:38:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:39:17,051Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:39:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:39:47,052Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:39:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:40:08,824Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/42fc3d49-baed-49e9-a216-e9e00547ee92\n",
       "2026-02-04T01:40:08,826Z INFO ExecuteStatement: Processing anonymous's query[42fc3d49-baed-49e9-a216-e9e00547ee92]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:40:08,827Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:40:08,831Z INFO ExecuteStatement: Query[42fc3d49-baed-49e9-a216-e9e00547ee92] in FINISHED_STATE\n",
       "2026-02-04T01:40:08,831Z INFO ExecuteStatement: Processing anonymous's query[42fc3d49-baed-49e9-a216-e9e00547ee92]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "26/02/04 01:40:08 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/42fc3d49-baed-49e9-a216-e9e00547ee92\n",
       "26/02/04 01:40:08 INFO ExecutePython: Processing anonymous's query[42fc3d49-baed-49e9-a216-e9e00547ee92]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:40:08 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:40:08 INFO ExecutePython: Processing anonymous's query[42fc3d49-baed-49e9-a216-e9e00547ee92]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:40:08 INFO DAGScheduler: Asked to cancel job group 42fc3d49-baed-49e9-a216-e9e00547ee92\n",
       "2026-02-04T01:40:09,883Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/e072a230-1afd-4e9b-8a1b-4e8ca0c8f190\n",
       "2026-02-04T01:40:09,885Z INFO ExecuteStatement: Processing anonymous's query[e072a230-1afd-4e9b-8a1b-4e8ca0c8f190]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:40:09,886Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:40:10,770Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/e072a230-1afd-4e9b-8a1b-4e8ca0c8f190/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:40:09 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/e072a230-1afd-4e9b-8a1b-4e8ca0c8f190\n",
       "26/02/04 01:40:09 INFO ExecutePython: Processing anonymous's query[e072a230-1afd-4e9b-8a1b-4e8ca0c8f190]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:40:09 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:40:09 INFO GpuOverrides: Plan conversion to the GPU took 0.18 ms\n",
       "26/02/04 01:40:09 INFO GpuOverrides: GPU plan transition optimization took 0.14 ms\n",
       "26/02/04 01:40:10 INFO InMemoryFileIndex: It took 40 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:40:10 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Got job 21 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Final stage: ResultStage 23 (parquet at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[130] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:40:10 INFO SQLOperationListener: Query [e072a230-1afd-4e9b-8a1b-4e8ca0c8f190]: Job 21 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:40:10 INFO SQLOperationListener: Query [e072a230-1afd-4e9b-8a1b-4e8ca0c8f190]: Stage 23.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:40:10 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:40:10 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:40:10 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:10 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[130] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:40:10 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:40:10 INFO FairSchedulableBuilder: Added task set TaskSet_23.0 tasks to pool \n",
       "26/02/04 01:40:10 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:40:10 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 100.67.4.240:46241 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:10 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 126 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:40:10 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:40:10 INFO DAGScheduler: ResultStage 23 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.140 s\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:40:10 INFO SQLOperationListener: Finished stage: Stage(23, 0); Name: 'parquet at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 140 msec\n",
       "26/02/04 01:40:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
       "26/02/04 01:40:10 INFO DAGScheduler: Job 21 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.145930 s\n",
       "26/02/04 01:40:10 INFO StatsReportListener: task runtime:(count: 1, mean: 126.000000, stdev: 0.000000, max: 126.000000, min: 126.000000)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t126.0 ms\t126.0 ms\t126.0 ms\t126.0 ms\t126.0 ms\t126.0 ms\t126.0 ms\t126.0 ms\t126.0 ms\n",
       "26/02/04 01:40:10 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:40:10 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:40:10 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:40:10 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 01:40:10 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 81.746032, stdev: 0.000000, max: 81.746032, min: 81.746032)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\n",
       "26/02/04 01:40:10 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:40:10 INFO StatsReportListener: other time pct: (count: 1, mean: 18.253968, stdev: 0.000000, max: 18.253968, min: 18.253968)\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:40:10 INFO StatsReportListener: \t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\n",
       "26/02/04 01:40:10 INFO SparkSQLEngineListener: Job end. Job 21 state is JobSucceeded\n",
       "26/02/04 01:40:10 INFO SQLOperationListener: Query [e072a230-1afd-4e9b-8a1b-4e8ca0c8f190]: Job 21 succeeded, 0 active jobs running\n",
       "26/02/04 01:40:10 INFO GpuOverrides: Plan conversion to the GPU took 38.17 ms\n",
       "26/02/04 01:40:10 INFO GpuOverrides: GPU plan transition optimization took 0.20 ms\n",
       "26/02/04 01:40:10 INFO GpuOverrides: Plan conversion to the GPU took 4.42 ms\n",
       "26/02/04 01:40:10 INFO GpuOverrides: GPU plan transition optimization took 0.19 ms\n",
       "26/02/04 01:40:10 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3a\n",
       "26/02/04 01:40:11 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
       "26/02/04 01:40:11 INFO DummySnapshot: [tableId=a5a1ee04-5988-4372-8326-d1b23d4716db] Created snapshot DummySnapshot(path=s3a://kratos-spark-hive/datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log, version=-1, metadata=Metadata(43c05a9d-7342-4d78-a5b0-a9153610c1bd,null,null,Format(parquet,Map()),null,List(),Map(),Some(1770169211108)), logSegment=LogSegment(s3a://kratos-spark-hive/datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,-1), checksumOpt=None)\n",
       "26/02/04 01:40:11 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
       "26/02/04 01:40:11 INFO DummySnapshot: [tableId=43c05a9d-7342-4d78-a5b0-a9153610c1bd] Created snapshot DummySnapshot(path=s3a://kratos-spark-hive/datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log, version=-1, metadata=Metadata(71f702e5-1e6d-4cc2-9ea8-ffcd44ce72cd,null,null,Format(parquet,Map()),null,List(),Map(),Some(1770169211362)), logSegment=LogSegment(s3a://kratos-spark-hive/datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,-1), checksumOpt=None)\n",
       "26/02/04 01:40:11 INFO GpuOptimisticTransaction: [tableId=71f702e5,txnId=5114e5f7] Updated metadata from - to Metadata(47fc9d07-31fe-439b-8238-105be18c4e16,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},ArrayBuffer(),Map(),Some(1770169211457))\n",
       "26/02/04 01:40:11 ERROR Utils: Aborting task\n",
       "org.apache.spark.sql.delta.DeltaIOException: [DELTA_CANNOT_CREATE_LOG_PATH] Cannot create s3a://kratos-spark-hive/datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log\n",
       "\tat org.apache.spark.sql.delta.DeltaErrorsBase.cannotCreateLogPathException(DeltaErrors.scala:1647)\n",
       "\tat org.apache.spark.sql.delta.DeltaErrorsBase.cannotCreateLogPathException\\$(DeltaErrors.scala:1643)\n",
       "\tat org.apache.spark.sql.delta.DeltaErrors\\$.cannotCreateLogPathException(DeltaErrors.scala:3598)\n",
       "\tat org.apache.spark.sql.delta.DeltaLog.createDirIfNotExists\\$1(DeltaLog.scala:511)\n",
       "\tat org.apache.spark.sql.delta.DeltaLog.createLogDirectoriesIfNotExists(DeltaLog.scala:514)\n",
       "\tat org.apache.spark.sql.delta.rapids.GpuWriteIntoDelta.writeAndReturnCommitData(GpuWriteIntoDelta.scala:205)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.doDeltaWrite\\$1(GpuCreateDeltaTableCommand.scala:315)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.handleCreateTableAsSelect(GpuCreateDeltaTableCommand.scala:342)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.\\$anonfun\\$handleCommit\\$1(GpuCreateDeltaTableCommand.scala:205)\n",
       "\tat org.apache.spark.sql.delta.OptimisticTransaction\\$.withActive(OptimisticTransaction.scala:209)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.handleCommit(GpuCreateDeltaTableCommand.scala:192)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.\\$anonfun\\$run\\$4(GpuCreateDeltaTableCommand.scala:157)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile\\$(DeltaLogging.scala:169)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.recordFrameProfile(GpuCreateDeltaTableCommand.scala:72)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.\\$anonfun\\$recordDeltaOperationInternal\\$1(DeltaLogging.scala:139)\n",
       "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)\n",
       "\tat com.databricks.spark.util.DatabricksLogging.recordOperation\\$(DatabricksLogging.scala:117)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.recordOperation(GpuCreateDeltaTableCommand.scala:72)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:138)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:128)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation\\$(DeltaLogging.scala:118)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.recordDeltaOperation(GpuCreateDeltaTableCommand.scala:72)\n",
       "\tat org.apache.spark.sql.delta.rapids.delta33x.GpuCreateDeltaTableCommand.run(GpuCreateDeltaTableCommand.scala:156)\n",
       "\tat com.nvidia.spark.rapids.delta.delta33x.GpuDeltaCatalog.\\$anonfun\\$createDeltaTable\\$1(GpuDeltaCatalog.scala:261)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile\\$(DeltaLogging.scala:169)\n",
       "\tat com.nvidia.spark.rapids.delta.delta33x.GpuDeltaCatalog.recordFrameProfile(GpuDeltaCatalog.scala:52)\n",
       "\tat com.nvidia.spark.rapids.delta.delta33x.GpuDeltaCatalog.createDeltaTable(GpuDeltaCatalog.scala:134)\n",
       "\tat com.nvidia.spark.rapids.delta.delta33x.GpuDeltaCatalog\\$GpuStagedDeltaTableV2.\\$anonfun\\$commitStagedChanges\\$1(GpuDeltaCatalog.scala:439)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n",
       "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile\\$(DeltaLogging.scala:169)\n",
       "\tat com.nvidia.spark.rapids.delta.delta33x.GpuDeltaCatalog.recordFrameProfile(GpuDeltaCatalog.scala:52)\n",
       "\tat com.nvidia.spark.rapids.delta.delta33x.GpuDeltaCatalog\\$GpuStagedDeltaTableV2.commitStagedChanges(GpuDeltaCatalog.scala:425)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.\\$anonfun\\$writeToTable\\$1(WriteToDataSourceV2Exec.scala:585)\n",
       "\tat org.apache.spark.util.Utils\\$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable\\$(WriteToDataSourceV2Exec.scala:572)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.rapids.GpuAtomicReplaceTableAsSelectExec.writeToTable(GpuAtomicReplaceTableAsSelectExec.scala:54)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.rapids.GpuAtomicReplaceTableAsSelectExec.run(GpuAtomicReplaceTableAsSelectExec.scala:89)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result\\$lzycompute(V2CommandExec.scala:43)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:645)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:579)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.nio.file.AccessDeniedException: datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log/_commits: PUT 0-byte object  on datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log/_commits: software.amazon.awssdk.services.s3.model.S3Exception: User: arn:aws:sts::900732750576:assumed-role/xp-dcartm-team-role/aws-sdk-java-1770162079103 is not authorized to perform: s3:PutObject on resource: \"arn:aws:s3:::kratos-spark-hive/datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log/_commits/\" because no identity-based policy allows the s3:PutObject action (Service: S3, Status Code: 403, Request ID: M4KEWJNY0Y4TNB8J, Extended Request ID: KAReGuVE6Zi1Uyo9VsvTUSjo0qAYD/h2j++ZbqPy/+1z4eUYOCZaViTCMH3Hr+SZGaqj84QeGzM=):AccessDenied\n",
       "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:267)\n",
       "\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:124)\n",
       "\tat org.apache.hadoop.fs.s3a.Invoker.lambda\\$retry\\$4(Invoker.java:376)\n",
       "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n",
       "\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:372)\n",
       "\tat org.apache.hadoop.fs.s3a.Invoker.retry(Invoker.java:347)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createEmptyObject(S3AFileSystem.java:4826)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.createFakeDirectory(S3AFileSystem.java:4807)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.access\\$2900(S3AFileSystem.java:290)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem\\$MkdirOperationCallbacksImpl.createFakeDirectory(S3AFileSystem.java:3870)\n",
       "\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:164)\n",
       "\tat org.apache.hadoop.fs.s3a.impl.MkdirOperation.execute(MkdirOperation.java:59)\n",
       "\tat org.apache.hadoop.fs.s3a.impl.ExecutingStoreOperation.apply(ExecutingStoreOperation.java:76)\n",
       "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n",
       "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda\\$trackDurationOfOperation\\$5(IOStatisticsBinding.java:528)\n",
       "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2865)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2884)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:3835)\n",
       "\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2496)\n",
       "\tat org.apache.spark.sql.delta.DeltaLog.liftedTree1\\$1(DeltaLog.scala:494)\n",
       "\tat org.apache.spark.sql.delta.DeltaLog.createDirIfNotExists\\$1(DeltaLog.scala:491)\n",
       "\t... 75 more\n",
       "Caused by: software.amazon.awssdk.services.s3.model.S3Exception: User: arn:aws:sts::900732750576:assumed-role/xp-dcartm-team-role/aws-sdk-java-1770162079103 is not authorized to perform: s3:PutObject on resource: \"arn:aws:s3:::kratos-spark-hive/datalake/kratos.db/gcp_east4_maestro_slurm_nodes/_delta_log/_commits/\" because no identity-based policy allows the s3:PutObject action (Service: S3, Status Code: 403, Request ID: M4KEWJNY0Y4TNB8J, Extended Request ID: KAReGuVE6Zi1Uyo9VsvTUSjo0qAYD/h2j++ZbqPy/+1z4eUYOCZaViTCMH3Hr+SZGaqj84QeGzM=)\n",
       "\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\n",
       "\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\n",
       "\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\n",
       "\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\n",
       "\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler\\$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:93)\n",
       "\tat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda\\$successTransformationResponseHandler\\$7(BaseClientHandler.java:279)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder\\$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder\\$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n",
       "\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n",
       "\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder\\$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder\\$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n",
       "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n",
       "\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient\\$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)\n",
       "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n",
       "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n",
       "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda\\$execute\\$1(BaseSyncClientHandler.java:80)\n",
       "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n",
       "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\n",
       "\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\n",
       "\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\n",
       "\tat software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:10191)\n",
       "\tat software.amazon.awssdk.services.s3.DelegatingS3Client.lambda\\$putObject\\$83(DelegatingS3Client.java:8207)\n",
       "\tat software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\n",
       "\tat software.amazon.awssdk.services.s3.DelegatingS3Client.putObject(DelegatingS3Client.java:8207)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda\\$putObjectDirect\\$15(S3AFileSystem.java:3348)\n",
       "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDurationOfSupplier(IOStatisticsBinding.java:651)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.putObjectDirect(S3AFileSystem.java:3346)\n",
       "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda\\$createEmptyObject\\$31(S3AFileSystem.java:4827)\n",
       "\tat org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:122)\n",
       "\t... 95 more\n",
       "2026-02-04T01:40:11,926Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/e072a230-1afd-4e9b-8a1b-4e8ca0c8f190/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:40:12,199Z INFO ExecuteStatement: Query[e072a230-1afd-4e9b-8a1b-4e8ca0c8f190] in ERROR_STATE\n",
       "2026-02-04T01:40:12,199Z INFO ExecuteStatement: Processing anonymous's query[e072a230-1afd-4e9b-8a1b-4e8ca0c8f190]: RUNNING_STATE -> ERROR_STATE, time taken: 2.314 seconds\n",
       "26/02/04 01:40:12 INFO ExecutePython: Processing anonymous's query[e072a230-1afd-4e9b-8a1b-4e8ca0c8f190]: RUNNING_STATE -> ERROR_STATE, time taken: 2.31 seconds\n",
       "26/02/04 01:40:12 INFO DAGScheduler: Asked to cancel job group e072a230-1afd-4e9b-8a1b-4e8ca0c8f190\n",
       "2026-02-04T01:40:13,086Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/e072a230-1afd-4e9b-8a1b-4e8ca0c8f190/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:40:13,272Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/e072a230-1afd-4e9b-8a1b-4e8ca0c8f190/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:40:13 INFO DAGScheduler: Asked to cancel job group e072a230-1afd-4e9b-8a1b-4e8ca0c8f190\n",
       "2026-02-04T01:40:17,052Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:40:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:40:47,052Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "7212.596: [GC (System.gc()) [PSYoungGen: 615294K->15040K(1034240K)] 698174K->97929K(4876800K), 0.0206920 secs] [Times: user=0.05 sys=0.00, real=0.02 secs] \n",
       "7212.617: [Full GC (System.gc()) [PSYoungGen: 15040K->0K(1034240K)] [ParOldGen: 82888K->89772K(3842560K)] 97929K->89772K(4876800K), [Metaspace: 214367K->214348K(1255424K)], 0.6137087 secs] [Times: user=1.11 sys=0.03, real=0.61 secs] \n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 100.67.4.240:46241 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 100.67.4.240:46241 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 100.67.4.240:46241 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 100.67.4.240:46241 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 100.67.4.240:46241 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:40:58 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:40:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:41:17,052Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "2026-02-04T01:41:17,053Z WARN KyuubiOperationManager: Operation OperationHandle [42fc3d49-baed-49e9-a216-e9e00547ee92] is timed-out and will be closed\n",
       "2026-02-04T01:41:17,053Z WARN KyuubiOperationManager: Operation OperationHandle [e072a230-1afd-4e9b-8a1b-4e8ca0c8f190] is timed-out and will be closed\n",
       "2026-02-04T01:41:17,054Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:42 FC 3D 49 BA ED 49 E9 A2 16 E9 E0 05 47 EE 92, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T01:41:17,055Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:E0 72 A2 30 1A FD 4E 9B 8A 1B 4E 8C A0 C8 F1 90, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:41:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:41:47,055Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:41:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:42:17,055Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:42:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:42:47,056Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "26/02/04 01:42:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T01:43:17,056Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "2026-02-04T01:43:17,056Z INFO KyuubiSessionManager: Closing session 945dc03c-2aaf-4860-b1cd-a3bcb75de58f that has been idle for more than 3600000 ms\n",
       "2026-02-04T01:43:17,056Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]/kernel-v3375be03908a89033b8c315d7cb6369ed1297e01a is closed, current opening sessions 2\n",
       "26/02/04 01:43:17 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]\n",
       "26/02/04 01:43:17 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]\n",
       "26/02/04 01:43:17 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]/kernel-v3375be03908a89033b8c315d7cb6369ed1297e01a is closed, current opening sessions 5\n",
       "26/02/04 01:43:17 INFO ExecutePython: send SIGCONT to 3386\n",
       "26/02/04 01:43:17 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [945dc03c-2aaf-4860-b1cd-a3bcb75de58f]\n",
       "26/02/04 01:43:17 WARN ExecutePython: Process has been interrupted\n",
       "26/02/04 01:43:17 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [981840bc-e115-4299-9f8b-45ec602a4556]\n",
       "26/02/04 01:43:17 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [981840bc-e115-4299-9f8b-45ec602a4556]\n",
       "26/02/04 01:43:17 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [981840bc-e115-4299-9f8b-45ec602a4556]/945dc03c-2aaf-4860-b1cd-a3bcb75de58f_aliveness_probe is closed, current opening sessions 4\n",
       "26/02/04 01:43:17 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [981840bc-e115-4299-9f8b-45ec602a4556]\n",
       "26/02/04 01:43:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:43:32,755Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/3adf5d34-b306-4ef7-a540-24d371f60c6e\n",
       "2026-02-04T01:43:32,757Z INFO ExecuteStatement: Processing anonymous's query[3adf5d34-b306-4ef7-a540-24d371f60c6e]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:43:32,758Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:43:32,763Z INFO ExecuteStatement: Query[3adf5d34-b306-4ef7-a540-24d371f60c6e] in FINISHED_STATE\n",
       "2026-02-04T01:43:32,763Z INFO ExecuteStatement: Processing anonymous's query[3adf5d34-b306-4ef7-a540-24d371f60c6e]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 01:43:32 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/3adf5d34-b306-4ef7-a540-24d371f60c6e\n",
       "26/02/04 01:43:32 INFO ExecutePython: Processing anonymous's query[3adf5d34-b306-4ef7-a540-24d371f60c6e]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:43:32 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:43:32 INFO ExecutePython: Processing anonymous's query[3adf5d34-b306-4ef7-a540-24d371f60c6e]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:43:32 INFO DAGScheduler: Asked to cancel job group 3adf5d34-b306-4ef7-a540-24d371f60c6e\n",
       "2026-02-04T01:43:33,331Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/63f14563-5fcd-4d2a-88c5-368dde7daed5\n",
       "2026-02-04T01:43:33,333Z INFO ExecuteStatement: Processing anonymous's query[63f14563-5fcd-4d2a-88c5-368dde7daed5]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:43:33,334Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:43:33,832Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:33 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/63f14563-5fcd-4d2a-88c5-368dde7daed5\n",
       "26/02/04 01:43:33 INFO ExecutePython: Processing anonymous's query[63f14563-5fcd-4d2a-88c5-368dde7daed5]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:43:33 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:43:33 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:43:33 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Got job 22 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Final stage: ResultStage 24 (parquet at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[132] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:43:33 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Job 22 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:43:33 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Stage 24.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:43:33 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:33 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:33 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:33 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[132] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:43:33 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:43:33 INFO FairSchedulableBuilder: Added task set TaskSet_24.0 tasks to pool \n",
       "26/02/04 01:43:33 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:33 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 100.67.4.240:46241 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:33 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 118 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:43:33 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:33 INFO DAGScheduler: ResultStage 24 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.130 s\n",
       "26/02/04 01:43:33 INFO SQLOperationListener: Finished stage: Stage(24, 0); Name: 'parquet at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 130 msec\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:43:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
       "26/02/04 01:43:33 INFO StatsReportListener: task runtime:(count: 1, mean: 118.000000, stdev: 0.000000, max: 118.000000, min: 118.000000)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t118.0 ms\t118.0 ms\t118.0 ms\t118.0 ms\t118.0 ms\t118.0 ms\t118.0 ms\t118.0 ms\t118.0 ms\n",
       "26/02/04 01:43:33 INFO DAGScheduler: Job 22 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.134355 s\n",
       "26/02/04 01:43:33 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:43:33 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:43:33 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:43:33 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 01:43:33 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 80.508475, stdev: 0.000000, max: 80.508475, min: 80.508475)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\t81 %\n",
       "26/02/04 01:43:33 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:43:33 INFO StatsReportListener: other time pct: (count: 1, mean: 19.491525, stdev: 0.000000, max: 19.491525, min: 19.491525)\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:33 INFO StatsReportListener: \t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\t19 %\n",
       "26/02/04 01:43:33 INFO SparkSQLEngineListener: Job end. Job 22 state is JobSucceeded\n",
       "26/02/04 01:43:33 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Job 22 succeeded, 0 active jobs running\n",
       "26/02/04 01:43:33 INFO DelegatingLogStore: LogStore LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore) is used for scheme s3\n",
       "26/02/04 01:43:33 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
       "26/02/04 01:43:33 INFO DummySnapshot: [tableId=0a61905c-02c7-4e77-a684-9bb519a47217] Created snapshot DummySnapshot(path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log, version=-1, metadata=Metadata(629f10bd-dadb-46af-b0c0-40dba0b4748d,null,null,Format(parquet,Map()),null,List(),Map(),Some(1770169413696)), logSegment=LogSegment(s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,-1), checksumOpt=None)\n",
       "26/02/04 01:43:33 INFO GpuOverrides: Plan conversion to the GPU took 65.71 ms\n",
       "26/02/04 01:43:33 INFO GpuOverrides: GPU plan transition optimization took 0.25 ms\n",
       "26/02/04 01:43:33 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
       "26/02/04 01:43:33 INFO DummySnapshot: [tableId=629f10bd-dadb-46af-b0c0-40dba0b4748d] Created snapshot DummySnapshot(path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log, version=-1, metadata=Metadata(080d119b-0ef7-43f9-a052-9ff10635b2d0,null,null,Format(parquet,Map()),null,List(),Map(),Some(1770169413736)), logSegment=LogSegment(s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log,-1,List(),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,-1), checksumOpt=None)\n",
       "26/02/04 01:43:33 INFO GpuOptimisticTransaction: [tableId=080d119b,txnId=8c988fff] Updated metadata from - to Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741))\n",
       "26/02/04 01:43:34 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:43:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:43:34 INFO GpuOverrides: Plan conversion to the GPU took 2.97 ms\n",
       "26/02/04 01:43:34 INFO GpuOverrides: GPU plan transition optimization took 0.42 ms\n",
       "26/02/04 01:43:34 INFO GpuParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n",
       "26/02/04 01:43:34 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:43:34 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO SparkContext: Created broadcast 33 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:43:34 WARN GpuDeltaInvariantCheckerExec: GpuRapidsDeltaWriteExec returned empty metrics in getOpTimeNewMetric\n",
       "26/02/04 01:43:34 WARN GpuOptimizeWriteExchangeExec: GpuRapidsDeltaWriteExec returned empty metrics in getOpTimeNewMetric\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Registering RDD 141 (RDD at GpuExec.scala:58) as input to shuffle 2\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Got map stage job 23 (internalDoExecuteColumnar at GpuExec.scala:341) with 1 output partitions\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (RDD at GpuExec.scala:58)\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:34 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Job 23 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Submitting ShuffleMapStage 25 (GpuOpTimeTrackingRDD[141] at RDD at GpuExec.scala:58), which has no missing parents\n",
       "26/02/04 01:43:34 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Stage 25.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:43:34 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 28.7 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 13.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 100.67.56.160:7079 (size: 13.3 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (GpuOpTimeTrackingRDD[141] at RDD at GpuExec.scala:58) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:43:34 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:43:34 INFO FairSchedulableBuilder: Added task set TaskSet_25.0 tasks to pool \n",
       "26/02/04 01:43:34 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10131 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 100.67.4.240:46241 (size: 13.3 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:34 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:34 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 378 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:43:34 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:34 INFO DAGScheduler: ShuffleMapStage 25 (RDD at GpuExec.scala:58) finished in 0.383 s\n",
       "26/02/04 01:43:34 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/04 01:43:34 INFO DAGScheduler: running: Set()\n",
       "26/02/04 01:43:34 INFO DAGScheduler: waiting: Set()\n",
       "26/02/04 01:43:34 INFO DAGScheduler: failed: Set()\n",
       "26/02/04 01:43:34 INFO SQLOperationListener: Finished stage: Stage(25, 0); Name: 'RDD at GpuExec.scala:58'; Status: succeeded; numTasks: 1; Took: 383 msec\n",
       "26/02/04 01:43:34 INFO StatsReportListener: task runtime:(count: 1, mean: 378.000000, stdev: 0.000000, max: 378.000000, min: 378.000000)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t378.0 ms\t378.0 ms\t378.0 ms\t378.0 ms\t378.0 ms\t378.0 ms\t378.0 ms\t378.0 ms\t378.0 ms\n",
       "26/02/04 01:43:34 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 2306.000000, stdev: 0.000000, max: 2306.000000, min: 2306.000000)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\n",
       "26/02/04 01:43:34 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:43:34 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:43:34 INFO StatsReportListener: task result size:(count: 1, mean: 8261.000000, stdev: 0.000000, max: 8261.000000, min: 8261.000000)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t8.1 KiB\t8.1 KiB\t8.1 KiB\t8.1 KiB\t8.1 KiB\t8.1 KiB\t8.1 KiB\t8.1 KiB\t8.1 KiB\n",
       "26/02/04 01:43:34 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 82.275132, stdev: 0.000000, max: 82.275132, min: 82.275132)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\t82 %\n",
       "26/02/04 01:43:34 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:43:34 INFO StatsReportListener: other time pct: (count: 1, mean: 17.724868, stdev: 0.000000, max: 17.724868, min: 17.724868)\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:34 INFO StatsReportListener: \t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\t18 %\n",
       "26/02/04 01:43:34 INFO SparkSQLEngineListener: Job end. Job 23 state is JobSucceeded\n",
       "26/02/04 01:43:34 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Job 23 succeeded, 0 active jobs running\n",
       "26/02/04 01:43:34 INFO SparkContext: Starting job: run at GpuDeltaDataSource.scala:57\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Got job 24 (run at GpuDeltaDataSource.scala:57) with 1 output partitions\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Final stage: ResultStage 27 (run at GpuDeltaDataSource.scala:57)\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:34 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Job 24 started with 2 stages, 1 active jobs running\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Submitting ResultStage 27 (GpuOpTimeTrackingRDD[145] at RDD at GpuExec.scala:58), which has no missing parents\n",
       "26/02/04 01:43:34 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Stage 27.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:43:34 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 321.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 119.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 100.67.56.160:7079 (size: 119.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:34 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (GpuOpTimeTrackingRDD[145] at RDD at GpuExec.scala:58) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:43:34 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:43:34 INFO FairSchedulableBuilder: Added task set TaskSet_27.0 tasks to pool \n",
       "26/02/04 01:43:34 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 26) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9238 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 100.67.4.240:46241 (size: 119.1 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:43:34,969Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 100.67.4.240:45730\n",
       "2026-02-04T01:43:36,120Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:43:37,268Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:37 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 26) in 2757 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:43:37 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:37 INFO DAGScheduler: ResultStage 27 (run at GpuDeltaDataSource.scala:57) finished in 2.782 s\n",
       "26/02/04 01:43:37 INFO SQLOperationListener: Finished stage: Stage(27, 0); Name: 'run at GpuDeltaDataSource.scala:57'; Status: succeeded; numTasks: 1; Took: 2782 msec\n",
       "26/02/04 01:43:37 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:43:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
       "26/02/04 01:43:37 INFO StatsReportListener: task runtime:(count: 1, mean: 2757.000000, stdev: 0.000000, max: 2757.000000, min: 2757.000000)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t2.8 s\t2.8 s\t2.8 s\t2.8 s\t2.8 s\t2.8 s\t2.8 s\t2.8 s\t2.8 s\n",
       "26/02/04 01:43:37 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:43:37 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:43:37 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:43:37 INFO StatsReportListener: task result size:(count: 1, mean: 7566.000000, stdev: 0.000000, max: 7566.000000, min: 7566.000000)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t7.4 KiB\t7.4 KiB\t7.4 KiB\t7.4 KiB\t7.4 KiB\t7.4 KiB\t7.4 KiB\t7.4 KiB\t7.4 KiB\n",
       "26/02/04 01:43:37 INFO DAGScheduler: Job 24 finished: run at GpuDeltaDataSource.scala:57, took 2.788871 s\n",
       "26/02/04 01:43:37 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 89.590134, stdev: 0.000000, max: 89.590134, min: 89.590134)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\n",
       "26/02/04 01:43:37 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:43:37 INFO StatsReportListener: other time pct: (count: 1, mean: 10.409866, stdev: 0.000000, max: 10.409866, min: 10.409866)\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:43:37 INFO StatsReportListener: \t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\n",
       "26/02/04 01:43:37 INFO SparkSQLEngineListener: Job end. Job 24 state is JobSucceeded\n",
       "26/02/04 01:43:37 INFO SQLOperationListener: Query [63f14563-5fcd-4d2a-88c5-368dde7daed5]: Job 24 succeeded, 0 active jobs running\n",
       "26/02/04 01:43:37 INFO GpuDeltaFileFormatWriter: Write Job d06d9798-5375-4197-a826-2f97d75c7989 committed. Elapsed time: 1 ms.\n",
       "26/02/04 01:43:37 INFO GpuDeltaFileFormatWriter: Finished processing stats for write job d06d9798-5375-4197-a826-2f97d75c7989.\n",
       "2026-02-04T01:43:38,339Z INFO ExecuteStatement: Query[63f14563-5fcd-4d2a-88c5-368dde7daed5] in RUNNING_STATE\n",
       "2026-02-04T01:43:38,411Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:43:39,602Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:39 WARN GpuOverrides: \n",
       "  ! <SerializeFromObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.SerializeFromObjectExec\n",
       "    @Expression <Alias> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).path, true, false, true) AS path#1137 could run on GPU\n",
       "      !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).path, true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "        ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).path cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).partitionValues) AS partitionValues#1138 could run on GPU\n",
       "      ! <ExternalMapToCatalyst> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).partitionValues) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ExternalMapToCatalyst\n",
       "        ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "          ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "            ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "          ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "            ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).partitionValues cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).size AS size#1139L could run on GPU\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).size cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).modificationTime AS modificationTime#1140L could run on GPU\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).modificationTime cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).dataChange AS dataChange#1141 could run on GPU\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).dataChange cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).tags) AS tags#1143 could run on GPU\n",
       "      ! <ExternalMapToCatalyst> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).tags) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ExternalMapToCatalyst\n",
       "        ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "          ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "            ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "          ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "            ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "        ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).tags cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> if (isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector)) null else named_struct(storageType, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true), pathOrInlineDv, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true), offset, unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset), sizeInBytes, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes, cardinality, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality, maxRowIndex, unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex)) AS deletionVector#1144 could run on GPU\n",
       "      @Expression <If> if (isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector)) null else named_struct(storageType, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true), pathOrInlineDv, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true), offset, unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset), sizeInBytes, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes, cardinality, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality, maxRowIndex, unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex)) could run on GPU\n",
       "        !Expression <IsNull> isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported)\n",
       "          ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "              ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "        @Expression <Literal> null could run on GPU\n",
       "        @Expression <CreateNamedStruct> named_struct(storageType, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true), pathOrInlineDv, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true), offset, unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset), sizeInBytes, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes, cardinality, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality, maxRowIndex, unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex)) could run on GPU\n",
       "          @Expression <Literal> storageType could run on GPU\n",
       "          !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "            ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "                ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                    ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                      !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "          @Expression <Literal> pathOrInlineDv could run on GPU\n",
       "          !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "            ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "                ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                    ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                      !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "          @Expression <Literal> offset could run on GPU\n",
       "          ! <UnwrapOption> unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "            ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "                ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                    ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                      !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "          @Expression <Literal> sizeInBytes could run on GPU\n",
       "          ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                  ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "          @Expression <Literal> cardinality could run on GPU\n",
       "          ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                  ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "          @Expression <Literal> maxRowIndex could run on GPU\n",
       "          ! <UnwrapOption> unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "            ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "                ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                    ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                      !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).baseRowId) AS baseRowId#1145L could run on GPU\n",
       "      ! <UnwrapOption> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).baseRowId) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "        ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).baseRowId cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).defaultRowCommitVersion) AS defaultRowCommitVersion#1146L could run on GPU\n",
       "      ! <UnwrapOption> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).defaultRowCommitVersion) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "        ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).defaultRowCommitVersion cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    @Expression <Alias> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, unwrapoption(ObjectType(class java.lang.String), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider), true, false, true) AS clusteringProvider#1147 could run on GPU\n",
       "      !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, unwrapoption(ObjectType(class java.lang.String), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "        ! <UnwrapOption> unwrapoption(ObjectType(class java.lang.String), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "          ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "              ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    ! <MapElementsExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.MapElementsExec\n",
       "      !Expression <AttributeReference> obj#1136 cannot run on GPU because expression AttributeReference obj#1136 produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "      ! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
       "        ! <NewInstance> newInstance(class scala.Tuple1) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
       "          !Expression <If> if (isnull(add#1050)) null else newInstance(class org.apache.spark.sql.delta.actions.AddFile) cannot run on GPU because expression If if (isnull(add#1050)) null else newInstance(class org.apache.spark.sql.delta.actions.AddFile) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); falseValue expression NewInstance newInstance(class org.apache.spark.sql.delta.actions.AddFile) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported); trueValue expression Literal null (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            @Expression <IsNull> isnull(add#1050) could run on GPU\n",
       "              @Expression <AttributeReference> add#1050 could run on GPU\n",
       "            !Expression <Literal> null cannot run on GPU because expression Literal null produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "            ! <NewInstance> newInstance(class org.apache.spark.sql.delta.actions.AddFile) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
       "              ! <Invoke> add#1050.path.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                @Expression <GetStructField> add#1050.path could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <CatalystToExternalMap> catalysttoexternalmap(lambdavariable(CatalystToExternalMap_key, StringType, false, -5), lambdavariable(CatalystToExternalMap_key, StringType, false, -5).toString, lambdavariable(CatalystToExternalMap_value, StringType, true, -6), lambdavariable(CatalystToExternalMap_value, StringType, true, -6).toString, add#1050.partitionValues, interface scala.collection.immutable.Map) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CatalystToExternalMap\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -5) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                ! <Invoke> lambdavariable(CatalystToExternalMap_key, StringType, false, -5).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -5) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -6) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                ! <Invoke> lambdavariable(CatalystToExternalMap_value, StringType, true, -6).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -6) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                @Expression <GetStructField> add#1050.partitionValues could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <AssertNotNull> assertnotnull(add#1050.size) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                @Expression <GetStructField> add#1050.size could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <AssertNotNull> assertnotnull(add#1050.modificationTime) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                @Expression <GetStructField> add#1050.modificationTime could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <AssertNotNull> assertnotnull(add#1050.dataChange) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                @Expression <GetStructField> add#1050.dataChange could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <Invoke> add#1050.stats.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                @Expression <GetStructField> add#1050.stats could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <CatalystToExternalMap> catalysttoexternalmap(lambdavariable(CatalystToExternalMap_key, StringType, false, -7), lambdavariable(CatalystToExternalMap_key, StringType, false, -7).toString, lambdavariable(CatalystToExternalMap_value, StringType, true, -8), lambdavariable(CatalystToExternalMap_value, StringType, true, -8).toString, add#1050.tags, interface scala.collection.immutable.Map) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CatalystToExternalMap\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -7) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                ! <Invoke> lambdavariable(CatalystToExternalMap_key, StringType, false, -7).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -7) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -8) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                ! <Invoke> lambdavariable(CatalystToExternalMap_value, StringType, true, -8).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -8) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "                @Expression <GetStructField> add#1050.tags could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              !Expression <If> if (isnull(add#1050.deletionVector)) null else newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) cannot run on GPU because trueValue expression Literal null (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); falseValue expression NewInstance newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression If if (isnull(add#1050.deletionVector)) null else newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "                @Expression <IsNull> isnull(add#1050.deletionVector) could run on GPU\n",
       "                  @Expression <GetStructField> add#1050.deletionVector could run on GPU\n",
       "                    @Expression <AttributeReference> add#1050 could run on GPU\n",
       "                !Expression <Literal> null cannot run on GPU because expression Literal null produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "                ! <NewInstance> newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
       "                  ! <Invoke> add#1050.deletionVector.storageType.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                    @Expression <GetStructField> add#1050.deletionVector.storageType could run on GPU\n",
       "                      @Expression <GetStructField> add#1050.deletionVector could run on GPU\n",
       "                        @Expression <AttributeReference> add#1050 could run on GPU\n",
       "                  ! <Invoke> add#1050.deletionVector.pathOrInlineDv.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                    @Expression <GetStructField> add#1050.deletionVector.pathOrInlineDv could run on GPU\n",
       "                      @Expression <GetStructField> add#1050.deletionVector could run on GPU\n",
       "                        @Expression <AttributeReference> add#1050 could run on GPU\n",
       "                  ! <WrapOption> wrapoption(add#1050.deletionVector.offset, IntegerType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "                    @Expression <GetStructField> add#1050.deletionVector.offset could run on GPU\n",
       "                      @Expression <GetStructField> add#1050.deletionVector could run on GPU\n",
       "                        @Expression <AttributeReference> add#1050 could run on GPU\n",
       "                  ! <AssertNotNull> assertnotnull(add#1050.deletionVector.sizeInBytes) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    @Expression <GetStructField> add#1050.deletionVector.sizeInBytes could run on GPU\n",
       "                      @Expression <GetStructField> add#1050.deletionVector could run on GPU\n",
       "                        @Expression <AttributeReference> add#1050 could run on GPU\n",
       "                  ! <AssertNotNull> assertnotnull(add#1050.deletionVector.cardinality) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    @Expression <GetStructField> add#1050.deletionVector.cardinality could run on GPU\n",
       "                      @Expression <GetStructField> add#1050.deletionVector could run on GPU\n",
       "                        @Expression <AttributeReference> add#1050 could run on GPU\n",
       "                  ! <WrapOption> wrapoption(add#1050.deletionVector.maxRowIndex, LongType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "                    @Expression <GetStructField> add#1050.deletionVector.maxRowIndex could run on GPU\n",
       "                      @Expression <GetStructField> add#1050.deletionVector could run on GPU\n",
       "                        @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <WrapOption> wrapoption(add#1050.baseRowId, LongType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "                @Expression <GetStructField> add#1050.baseRowId could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <WrapOption> wrapoption(add#1050.defaultRowCommitVersion, LongType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "                @Expression <GetStructField> add#1050.defaultRowCommitVersion could run on GPU\n",
       "                  @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              ! <WrapOption> wrapoption(add#1050.clusteringProvider.toString, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "                ! <Invoke> add#1050.clusteringProvider.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  @Expression <GetStructField> add#1050.clusteringProvider could run on GPU\n",
       "                    @Expression <AttributeReference> add#1050 could run on GPU\n",
       "        !Expression <AttributeReference> obj#1135 cannot run on GPU because expression AttributeReference obj#1135 produces an unsupported type ObjectType(class scala.Tuple1)\n",
       "            ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
       "              @Expression <AttributeReference> txn#1049 could run on GPU\n",
       "              @Expression <AttributeReference> add#1050 could run on GPU\n",
       "              @Expression <AttributeReference> remove#1051 could run on GPU\n",
       "              @Expression <AttributeReference> metaData#1052 could run on GPU\n",
       "              @Expression <AttributeReference> protocol#1053 could run on GPU\n",
       "              @Expression <AttributeReference> cdc#1054 could run on GPU\n",
       "              @Expression <AttributeReference> checkpointMetadata#1055 could run on GPU\n",
       "              @Expression <AttributeReference> sidecar#1056 could run on GPU\n",
       "              @Expression <AttributeReference> domainMetadata#1057 could run on GPU\n",
       "              @Expression <AttributeReference> commitInfo#1058 could run on GPU\n",
       "\n",
       "26/02/04 01:43:39 INFO GpuOverrides: Plan conversion to the GPU took 59.14 ms\n",
       "26/02/04 01:43:39 INFO GpuOverrides: GPU plan transition optimization took 7.96 ms\n",
       "26/02/04 01:43:39 INFO CodeGenerator: Code generated in 334.552113 ms\n",
       "2026-02-04T01:43:40,740Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:39 INFO CodeGenerator: Code generated in 300.955854 ms\n",
       "26/02/04 01:43:40 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:43:40 INFO SparkSQLEngineListener: Job end. Job 25 state is JobSucceeded\n",
       "26/02/04 01:43:40 INFO DAGScheduler: Job 25 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 0.000223 s\n",
       "26/02/04 01:43:40 INFO GpuOptimisticTransaction: [tableId=080d119b,txnId=8c988fff] Attempting to commit version 0 with 4 actions with Serializable isolation level\n",
       "26/02/04 01:43:40 INFO GpuOptimisticTransaction: Incremental commit: starting with snapshot version -1\n",
       "26/02/04 01:43:41 WARN GpuOverrides: \n",
       "! <SerializeFromObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.SerializeFromObjectExec\n",
       "  @Expression <Alias> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).path, true, false, true) AS path#1283 could run on GPU\n",
       "    !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).path, true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).path cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).partitionValues) AS partitionValues#1284 could run on GPU\n",
       "    ! <ExternalMapToCatalyst> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).partitionValues) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ExternalMapToCatalyst\n",
       "      ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "        ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "          ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -1) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "        ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "          ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).partitionValues cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).size AS size#1285L could run on GPU\n",
       "    ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).size cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "      !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "        ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "          !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).modificationTime AS modificationTime#1286L could run on GPU\n",
       "    ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).modificationTime cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "      !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "        ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "          !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).dataChange AS dataChange#1287 could run on GPU\n",
       "    ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).dataChange cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "      !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "        ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "          !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).stats, true, false, true) AS stats#1288 could run on GPU\n",
       "    !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).stats, true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).stats cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).tags) AS tags#1289 could run on GPU\n",
       "    ! <ExternalMapToCatalyst> externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)), true, false, true), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)), true, false, true), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).tags) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ExternalMapToCatalyst\n",
       "      ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "        ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "          ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), true, -3) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "        ! <ValidateExternalType> validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4), StringType, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.ValidateExternalType\n",
       "          ! <LambdaVariable> lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -4) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).tags cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> if (isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector)) null else named_struct(storageType, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true), pathOrInlineDv, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true), offset, unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset), sizeInBytes, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes, cardinality, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality, maxRowIndex, unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex)) AS deletionVector#1290 could run on GPU\n",
       "    @Expression <If> if (isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector)) null else named_struct(storageType, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true), pathOrInlineDv, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true), offset, unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset), sizeInBytes, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes, cardinality, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality, maxRowIndex, unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex)) could run on GPU\n",
       "      !Expression <IsNull> isnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported)\n",
       "        ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "      @Expression <Literal> null could run on GPU\n",
       "      @Expression <CreateNamedStruct> named_struct(storageType, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true), pathOrInlineDv, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true), offset, unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset), sizeInBytes, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes, cardinality, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality, maxRowIndex, unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex)) could run on GPU\n",
       "        @Expression <Literal> storageType could run on GPU\n",
       "        !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType, true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "          ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).storageType cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                  ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "        @Expression <Literal> pathOrInlineDv could run on GPU\n",
       "        !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv, true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "          ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).pathOrInlineDv cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                  ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "        @Expression <Literal> offset could run on GPU\n",
       "        ! <UnwrapOption> unwrapoption(IntegerType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "          ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).offset cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                  ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "        @Expression <Literal> sizeInBytes could run on GPU\n",
       "        ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).sizeInBytes cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "            ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                  !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "        @Expression <Literal> cardinality could run on GPU\n",
       "        ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).cardinality cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "            ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                  !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "        @Expression <Literal> maxRowIndex could run on GPU\n",
       "        ! <UnwrapOption> unwrapoption(LongType, knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "          ! <Invoke> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector).maxRowIndex cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "            !Expression <KnownNotNull> knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) cannot run on GPU because input expression Invoke knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression KnownNotNull knownnotnull(knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).deletionVector cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "                  ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                    !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).baseRowId) AS baseRowId#1291L could run on GPU\n",
       "    ! <UnwrapOption> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).baseRowId) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).baseRowId cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).defaultRowCommitVersion) AS defaultRowCommitVersion#1292L could run on GPU\n",
       "    ! <UnwrapOption> unwrapoption(LongType, knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).defaultRowCommitVersion) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "      ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).defaultRowCommitVersion cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "        !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "            !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  @Expression <Alias> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, unwrapoption(ObjectType(class java.lang.String), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider), true, false, true) AS clusteringProvider#1293 could run on GPU\n",
       "    !Expression <StaticInvoke> staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, unwrapoption(ObjectType(class java.lang.String), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider), true, false, true) cannot run on GPU because StaticInvoke is not supported\n",
       "      ! <UnwrapOption> unwrapoption(ObjectType(class java.lang.String), knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.UnwrapOption\n",
       "        ! <Invoke> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])).clusteringProvider cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "          !Expression <KnownNotNull> knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) cannot run on GPU because expression KnownNotNull knownnotnull(assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true])) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); input expression AssertNotNull assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "            ! <AssertNotNull> assertnotnull(input[0, org.apache.spark.sql.delta.actions.AddFile, true]) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              !Expression <BoundReference> input[0, org.apache.spark.sql.delta.actions.AddFile, true] cannot run on GPU because expression BoundReference input[0, org.apache.spark.sql.delta.actions.AddFile, true] produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "  ! <MapElementsExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.MapElementsExec\n",
       "    !Expression <AttributeReference> obj#1282 cannot run on GPU because expression AttributeReference obj#1282 produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "    ! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
       "      ! <NewInstance> newInstance(class scala.Tuple1) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
       "        !Expression <If> if (isnull(add#1217)) null else newInstance(class org.apache.spark.sql.delta.actions.AddFile) cannot run on GPU because expression If if (isnull(add#1217)) null else newInstance(class org.apache.spark.sql.delta.actions.AddFile) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile); falseValue expression NewInstance newInstance(class org.apache.spark.sql.delta.actions.AddFile) (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported); trueValue expression Literal null (ObjectType(class org.apache.spark.sql.delta.actions.AddFile) is not supported)\n",
       "          @Expression <IsNull> isnull(add#1217) could run on GPU\n",
       "            @Expression <AttributeReference> add#1217 could run on GPU\n",
       "          !Expression <Literal> null cannot run on GPU because expression Literal null produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.AddFile)\n",
       "          ! <NewInstance> newInstance(class org.apache.spark.sql.delta.actions.AddFile) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
       "            ! <Invoke> add#1217.path.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              @Expression <GetStructField> add#1217.path could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <CatalystToExternalMap> catalysttoexternalmap(lambdavariable(CatalystToExternalMap_key, StringType, false, -5), lambdavariable(CatalystToExternalMap_key, StringType, false, -5).toString, lambdavariable(CatalystToExternalMap_value, StringType, true, -6), lambdavariable(CatalystToExternalMap_value, StringType, true, -6).toString, add#1217.partitionValues, interface scala.collection.immutable.Map) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CatalystToExternalMap\n",
       "              ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -5) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              ! <Invoke> lambdavariable(CatalystToExternalMap_key, StringType, false, -5).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -5) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -6) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              ! <Invoke> lambdavariable(CatalystToExternalMap_value, StringType, true, -6).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -6) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              @Expression <GetStructField> add#1217.partitionValues could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <AssertNotNull> assertnotnull(add#1217.size) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              @Expression <GetStructField> add#1217.size could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <AssertNotNull> assertnotnull(add#1217.modificationTime) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              @Expression <GetStructField> add#1217.modificationTime could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <AssertNotNull> assertnotnull(add#1217.dataChange) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "              @Expression <GetStructField> add#1217.dataChange could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <Invoke> add#1217.stats.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "              @Expression <GetStructField> add#1217.stats could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <CatalystToExternalMap> catalysttoexternalmap(lambdavariable(CatalystToExternalMap_key, StringType, false, -7), lambdavariable(CatalystToExternalMap_key, StringType, false, -7).toString, lambdavariable(CatalystToExternalMap_value, StringType, true, -8), lambdavariable(CatalystToExternalMap_value, StringType, true, -8).toString, add#1217.tags, interface scala.collection.immutable.Map) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CatalystToExternalMap\n",
       "              ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -7) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              ! <Invoke> lambdavariable(CatalystToExternalMap_key, StringType, false, -7).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_key, StringType, false, -7) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -8) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              ! <Invoke> lambdavariable(CatalystToExternalMap_value, StringType, true, -8).toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                ! <LambdaVariable> lambdavariable(CatalystToExternalMap_value, StringType, true, -8) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.LambdaVariable\n",
       "              @Expression <GetStructField> add#1217.tags could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            !Expression <If> if (isnull(add#1217.deletionVector)) null else newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) cannot run on GPU because trueValue expression Literal null (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); falseValue expression NewInstance newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) (ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) is not supported); expression If if (isnull(add#1217.deletionVector)) null else newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              @Expression <IsNull> isnull(add#1217.deletionVector) could run on GPU\n",
       "                @Expression <GetStructField> add#1217.deletionVector could run on GPU\n",
       "                  @Expression <AttributeReference> add#1217 could run on GPU\n",
       "              !Expression <Literal> null cannot run on GPU because expression Literal null produces an unsupported type ObjectType(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor)\n",
       "              ! <NewInstance> newInstance(class org.apache.spark.sql.delta.actions.DeletionVectorDescriptor) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
       "                ! <Invoke> add#1217.deletionVector.storageType.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  @Expression <GetStructField> add#1217.deletionVector.storageType could run on GPU\n",
       "                    @Expression <GetStructField> add#1217.deletionVector could run on GPU\n",
       "                      @Expression <AttributeReference> add#1217 could run on GPU\n",
       "                ! <Invoke> add#1217.deletionVector.pathOrInlineDv.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                  @Expression <GetStructField> add#1217.deletionVector.pathOrInlineDv could run on GPU\n",
       "                    @Expression <GetStructField> add#1217.deletionVector could run on GPU\n",
       "                      @Expression <AttributeReference> add#1217 could run on GPU\n",
       "                ! <WrapOption> wrapoption(add#1217.deletionVector.offset, IntegerType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "                  @Expression <GetStructField> add#1217.deletionVector.offset could run on GPU\n",
       "                    @Expression <GetStructField> add#1217.deletionVector could run on GPU\n",
       "                      @Expression <AttributeReference> add#1217 could run on GPU\n",
       "                ! <AssertNotNull> assertnotnull(add#1217.deletionVector.sizeInBytes) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                  @Expression <GetStructField> add#1217.deletionVector.sizeInBytes could run on GPU\n",
       "                    @Expression <GetStructField> add#1217.deletionVector could run on GPU\n",
       "                      @Expression <AttributeReference> add#1217 could run on GPU\n",
       "                ! <AssertNotNull> assertnotnull(add#1217.deletionVector.cardinality) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.AssertNotNull\n",
       "                  @Expression <GetStructField> add#1217.deletionVector.cardinality could run on GPU\n",
       "                    @Expression <GetStructField> add#1217.deletionVector could run on GPU\n",
       "                      @Expression <AttributeReference> add#1217 could run on GPU\n",
       "                ! <WrapOption> wrapoption(add#1217.deletionVector.maxRowIndex, LongType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "                  @Expression <GetStructField> add#1217.deletionVector.maxRowIndex could run on GPU\n",
       "                    @Expression <GetStructField> add#1217.deletionVector could run on GPU\n",
       "                      @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <WrapOption> wrapoption(add#1217.baseRowId, LongType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "              @Expression <GetStructField> add#1217.baseRowId could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <WrapOption> wrapoption(add#1217.defaultRowCommitVersion, LongType) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "              @Expression <GetStructField> add#1217.defaultRowCommitVersion could run on GPU\n",
       "                @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            ! <WrapOption> wrapoption(add#1217.clusteringProvider.toString, ObjectType(class java.lang.String)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.WrapOption\n",
       "              ! <Invoke> add#1217.clusteringProvider.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
       "                @Expression <GetStructField> add#1217.clusteringProvider could run on GPU\n",
       "                  @Expression <AttributeReference> add#1217 could run on GPU\n",
       "      !Expression <AttributeReference> obj#1281 cannot run on GPU because expression AttributeReference obj#1281 produces an unsupported type ObjectType(class scala.Tuple1)\n",
       "          ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
       "            @Expression <AttributeReference> txn#1216 could run on GPU\n",
       "            @Expression <AttributeReference> add#1217 could run on GPU\n",
       "            @Expression <AttributeReference> remove#1218 could run on GPU\n",
       "            @Expression <AttributeReference> metaData#1219 could run on GPU\n",
       "            @Expression <AttributeReference> protocol#1220 could run on GPU\n",
       "            @Expression <AttributeReference> cdc#1221 could run on GPU\n",
       "            @Expression <AttributeReference> checkpointMetadata#1222 could run on GPU\n",
       "            @Expression <AttributeReference> sidecar#1223 could run on GPU\n",
       "            @Expression <AttributeReference> domainMetadata#1224 could run on GPU\n",
       "            @Expression <AttributeReference> commitInfo#1225 could run on GPU\n",
       "\n",
       "26/02/04 01:43:41 INFO GpuOverrides: Plan conversion to the GPU took 9.07 ms\n",
       "26/02/04 01:43:41 INFO GpuOverrides: GPU plan transition optimization took 1.16 ms\n",
       "26/02/04 01:43:41 INFO CodeGenerator: Code generated in 182.068734 ms\n",
       "26/02/04 01:43:41 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:43:41 INFO SparkSQLEngineListener: Job end. Job 26 state is JobSucceeded\n",
       "26/02/04 01:43:41 INFO DAGScheduler: Job 26 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 0.000196 s\n",
       "26/02/04 01:43:41 INFO DeltaLog: Creating a new snapshot v0 for commit version 0\n",
       "26/02/04 01:43:41 INFO DeltaLog: Loading version 0.\n",
       "26/02/04 01:43:41 INFO Snapshot: [tableId=080d119b-0ef7-43f9-a052-9ff10635b2d0] Created snapshot Snapshot(path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log, version=0, metadata=Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)), logSegment=LogSegment(s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log,0,List(S3AFileStatus{path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000000.json; isDirectory=false; length=2216; replication=1; blocksize=67108864; modification_time=1770169421000; access_time=0; owner=spring; group=spring; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=\"1a1dd94afc5dcab27160a6d56fa240d3\" versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,1770169421000), checksumOpt=Some(VersionChecksum(Some(8c988fff-fd94-40b3-bb26-c72ecfa573ea),2175,1,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00000-d135e637-00ed-4ea2-93d2-47dcda920637-c000.snappy.parquet,Map(),2175,1770169417000,false,{\"numRecords\":10,\"minValues\":{\"node_id\":\"cpu-dm-001\",\"scontrol_state\":\"[\\\\\"IDLE\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"maxValues\":{\"node_id\":\"cpu-dm-010\",\"scontrol_state\":\"[\\\\\"MIXED\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"nullCount\":{\"node_id\":0,\"scontrol_state\":0,\"reason\":0,\"updated_at\":0,\"stale\":0,\"reason_changed_at\":0,\"cluster_id\":0,\"last_busy_at\":0}},null,null,None,None,None))))))\n",
       "26/02/04 01:43:41 INFO DeltaLog: Updated snapshot to Snapshot(path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log, version=0, metadata=Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)), logSegment=LogSegment(s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log,0,List(S3AFileStatus{path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000000.json; isDirectory=false; length=2216; replication=1; blocksize=67108864; modification_time=1770169421000; access_time=0; owner=spring; group=spring; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=\"1a1dd94afc5dcab27160a6d56fa240d3\" versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,1770169421000), checksumOpt=Some(VersionChecksum(Some(8c988fff-fd94-40b3-bb26-c72ecfa573ea),2175,1,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00000-d135e637-00ed-4ea2-93d2-47dcda920637-c000.snappy.parquet,Map(),2175,1770169417000,false,{\"numRecords\":10,\"minValues\":{\"node_id\":\"cpu-dm-001\",\"scontrol_state\":\"[\\\\\"IDLE\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"maxValues\":{\"node_id\":\"cpu-dm-010\",\"scontrol_state\":\"[\\\\\"MIXED\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"nullCount\":{\"node_id\":0,\"scontrol_state\":0,\"reason\":0,\"updated_at\":0,\"stale\":0,\"reason_changed_at\":0,\"cluster_id\":0,\"last_busy_at\":0}},null,null,None,None,None))))))\n",
       "26/02/04 01:43:41 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 2216)\n",
       "26/02/04 01:43:41 INFO GpuOptimisticTransaction: [tableId=080d119b,txnId=8c988fff] Committed delta #0 to s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log\n",
       "26/02/04 01:43:41 INFO ChecksumHook: Writing checksum file for table path s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log version 0\n",
       "2026-02-04T01:43:41,902Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:41 WARN CheckpointFileManager: Could not use FileContext API for managing Structured Streaming checkpoint files at s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log. Using FileSystem API instead for managing log files. If the implementation of FileSystem.rename() is not atomic, then the correctness and fault-tolerance ofyour Structured Streaming is not guaranteed.\n",
       "26/02/04 01:43:41 INFO CheckpointFileManager: Writing atomically to s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000000.crc using temp file s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/.00000000000000000000.crc.26e57120-9781-420f-b49d-471d30adef89.tmp\n",
       "26/02/04 01:43:42 INFO CheckpointFileManager: Renamed temp file s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/.00000000000000000000.crc.26e57120-9781-420f-b49d-471d30adef89.tmp to s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000000.crc\n",
       "26/02/04 01:43:42 INFO Snapshot: DELTA: Compute snapshot for version: 0\n",
       "26/02/04 01:43:42 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 472.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:42 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:42 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 100.67.56.160:7079 (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:42 INFO SparkContext: Created broadcast 36 from \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "2026-02-04T01:43:43,048Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:43:43,400Z INFO ExecuteStatement: Query[63f14563-5fcd-4d2a-88c5-368dde7daed5] in RUNNING_STATE\n",
       "26/02/04 01:43:43 INFO DataSourceStrategy: Pruning directories with: \n",
       "26/02/04 01:43:43 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:43:43 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:43:43 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:43 INFO GpuOverrides: Plan conversion to the GPU took 103.63 ms\n",
       "26/02/04 01:43:43 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:43 INFO GpuOverrides: Plan conversion to the GPU took 17.19 ms\n",
       "26/02/04 01:43:43 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:43 INFO GpuOverrides: Plan conversion to the GPU took 0.24 ms\n",
       "26/02/04 01:43:43 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:43 INFO GpuOverrides: Plan conversion to the GPU took 63.78 ms\n",
       "26/02/04 01:43:43 INFO GpuOverrides: GPU plan transition optimization took 0.18 ms\n",
       "26/02/04 01:43:43 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:43 INFO GpuOverrides: Plan conversion to the GPU took 3.19 ms\n",
       "26/02/04 01:43:43 INFO GpuOverrides: GPU plan transition optimization took 0.77 ms\n",
       "26/02/04 01:43:43 INFO CodeGenerator: Code generated in 201.244836 ms\n",
       "26/02/04 01:43:43 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 467.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:43 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:43 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 100.67.56.160:7079 (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:43 INFO SparkContext: Created broadcast 37 from \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:43:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "2026-02-04T01:43:44,184Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:43 INFO DAGScheduler: Registering RDD 192 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) as input to shuffle 3\n",
       "26/02/04 01:43:43 INFO DAGScheduler: Got map stage job 27 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 1 output partitions\n",
       "26/02/04 01:43:43 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:43:43 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:43:43 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:43 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[192] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:43:43 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 106.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:43 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:43 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 100.67.56.160:7079 (size: 32.7 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:43 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[192] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:43:43 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:43:43 INFO FairSchedulableBuilder: Added task set TaskSet_28.0 tasks to pool \n",
       "26/02/04 01:43:43 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 27) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9890 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:43 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 100.67.4.240:46241 (size: 32.7 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:44 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 100.67.4.240:46241 (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:44 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 27) in 832 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:43:44 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:44 INFO DAGScheduler: ShuffleMapStage 28 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.860 s\n",
       "26/02/04 01:43:44 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/04 01:43:44 INFO DAGScheduler: running: Set()\n",
       "26/02/04 01:43:44 INFO DAGScheduler: waiting: Set()\n",
       "26/02/04 01:43:44 INFO DAGScheduler: failed: Set()\n",
       "26/02/04 01:43:44 INFO SparkSQLEngineListener: Job end. Job 27 state is JobSucceeded\n",
       "26/02/04 01:43:44 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:44 INFO GpuOverrides: Plan conversion to the GPU took 13.43 ms\n",
       "26/02/04 01:43:44 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:44 INFO GpuOverrides: Plan conversion to the GPU took 10.48 ms\n",
       "26/02/04 01:43:44 INFO GpuOverrides: GPU plan transition optimization took 2.11 ms\n",
       "2026-02-04T01:43:45,402Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "7380.027: [GC (Allocation Failure) [PSYoungGen: 985088K->29425K(1037312K)] 1074860K->119206K(4879872K), 0.0211096 secs] [Times: user=0.06 sys=0.00, real=0.02 secs] \n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 100.67.56.160:7079 in memory (size: 119.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 100.67.4.240:46241 in memory (size: 119.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 100.67.56.160:7079 in memory (size: 32.7 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 100.67.4.240:46241 in memory (size: 32.7 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 100.67.4.240:46241 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 100.67.56.160:7079 in memory (size: 13.3 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:45 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 100.67.4.240:46241 in memory (size: 13.3 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:43:46,602Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:45 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass\\$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0\\$ is 25072 bytes\n",
       "26/02/04 01:43:45 INFO CodeGenerator: Code generated in 875.9295 ms\n",
       "26/02/04 01:43:46 INFO CodeGenerator: Code generated in 205.113171 ms\n",
       "26/02/04 01:43:46 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:46 INFO GpuOverrides: Plan conversion to the GPU took 20.77 ms\n",
       "2026-02-04T01:43:47,067Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T01:43:47,755Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:46 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:46 INFO GpuOverrides: Plan conversion to the GPU took 76.96 ms\n",
       "26/02/04 01:43:46 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:46 INFO GpuOverrides: Plan conversion to the GPU took 0.21 ms\n",
       "26/02/04 01:43:46 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:46 INFO GpuOverrides: Plan conversion to the GPU took 1.96 ms\n",
       "26/02/04 01:43:46 INFO GpuOverrides: GPU plan transition optimization took 0.18 ms\n",
       "26/02/04 01:43:46 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:46 INFO GpuOverrides: Plan conversion to the GPU took 1.37 ms\n",
       "26/02/04 01:43:46 INFO GpuOverrides: GPU plan transition optimization took 0.52 ms\n",
       "26/02/04 01:43:47 INFO CodeGenerator: Code generated in 79.188235 ms\n",
       "26/02/04 01:43:47 INFO DAGScheduler: Registering RDD 202 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) as input to shuffle 4\n",
       "26/02/04 01:43:47 INFO DAGScheduler: Got map stage job 28 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 50 output partitions\n",
       "26/02/04 01:43:47 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:43:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 29)\n",
       "26/02/04 01:43:47 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:47 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[202] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:43:47 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 615.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:47 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 146.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:47 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 100.67.56.160:7079 (size: 146.0 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:47 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:47 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[202] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
       "26/02/04 01:43:47 INFO TaskSchedulerImpl: Adding task set 30.0 with 50 tasks resource profile 0\n",
       "26/02/04 01:43:47 INFO FairSchedulableBuilder: Added task set TaskSet_30.0 tasks to pool \n",
       "26/02/04 01:43:47 INFO TaskSetManager: Starting task 40.0 in stage 30.0 (TID 28) (100.67.4.240, executor 4, partition 40, NODE_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:47 INFO TaskSetManager: Starting task 42.0 in stage 30.0 (TID 29) (100.67.4.240, executor 4, partition 42, NODE_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:47 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:47 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 31) (100.67.4.240, executor 4, partition 1, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:47 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 100.67.4.240:46241 (size: 146.0 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:47 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 100.67.4.240:45730\n",
       "2026-02-04T01:43:48,402Z INFO ExecuteStatement: Query[63f14563-5fcd-4d2a-88c5-368dde7daed5] in RUNNING_STATE\n",
       "2026-02-04T01:43:48,892Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_1 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_0 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_40 in memory on 100.67.4.240:46241 (size: 1078.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_42 in memory on 100.67.4.240:46241 (size: 1151.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 2.0 in stage 30.0 (TID 32) (100.67.4.240, executor 4, partition 2, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 42.0 in stage 30.0 (TID 29) in 2419 ms on 100.67.4.240 (executor 4) (1/50)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 3.0 in stage 30.0 (TID 33) (100.67.4.240, executor 4, partition 3, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 2420 ms on 100.67.4.240 (executor 4) (2/50)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 4.0 in stage 30.0 (TID 34) (100.67.4.240, executor 4, partition 4, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 40.0 in stage 30.0 (TID 28) in 2422 ms on 100.67.4.240 (executor 4) (3/50)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 5.0 in stage 30.0 (TID 35) (100.67.4.240, executor 4, partition 5, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 31) in 2422 ms on 100.67.4.240 (executor 4) (4/50)\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_2 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_5 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_3 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "2026-02-04T01:43:50,042Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:49 INFO BlockManagerInfo: Added rdd_199_4 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 6.0 in stage 30.0 (TID 36) (100.67.4.240, executor 4, partition 6, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 3.0 in stage 30.0 (TID 33) in 299 ms on 100.67.4.240 (executor 4) (5/50)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 7.0 in stage 30.0 (TID 37) (100.67.4.240, executor 4, partition 7, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 2.0 in stage 30.0 (TID 32) in 302 ms on 100.67.4.240 (executor 4) (6/50)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 8.0 in stage 30.0 (TID 38) (100.67.4.240, executor 4, partition 8, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 5.0 in stage 30.0 (TID 35) in 308 ms on 100.67.4.240 (executor 4) (7/50)\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Starting task 9.0 in stage 30.0 (TID 39) (100.67.4.240, executor 4, partition 9, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:49 INFO TaskSetManager: Finished task 4.0 in stage 30.0 (TID 34) in 325 ms on 100.67.4.240 (executor 4) (8/50)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_8 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_7 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_6 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_9 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 10.0 in stage 30.0 (TID 40) (100.67.4.240, executor 4, partition 10, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 7.0 in stage 30.0 (TID 37) in 295 ms on 100.67.4.240 (executor 4) (9/50)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 11.0 in stage 30.0 (TID 41) (100.67.4.240, executor 4, partition 11, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 8.0 in stage 30.0 (TID 38) in 287 ms on 100.67.4.240 (executor 4) (10/50)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 12.0 in stage 30.0 (TID 42) (100.67.4.240, executor 4, partition 12, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 9.0 in stage 30.0 (TID 39) in 280 ms on 100.67.4.240 (executor 4) (11/50)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 13.0 in stage 30.0 (TID 43) (100.67.4.240, executor 4, partition 13, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 6.0 in stage 30.0 (TID 36) in 308 ms on 100.67.4.240 (executor 4) (12/50)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_10 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_13 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_11 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 14.0 in stage 30.0 (TID 44) (100.67.4.240, executor 4, partition 14, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 10.0 in stage 30.0 (TID 40) in 216 ms on 100.67.4.240 (executor 4) (13/50)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 15.0 in stage 30.0 (TID 45) (100.67.4.240, executor 4, partition 15, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 13.0 in stage 30.0 (TID 43) in 214 ms on 100.67.4.240 (executor 4) (14/50)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_12 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 16.0 in stage 30.0 (TID 46) (100.67.4.240, executor 4, partition 16, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 11.0 in stage 30.0 (TID 41) in 230 ms on 100.67.4.240 (executor 4) (15/50)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 17.0 in stage 30.0 (TID 47) (100.67.4.240, executor 4, partition 17, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 12.0 in stage 30.0 (TID 42) in 311 ms on 100.67.4.240 (executor 4) (16/50)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_15 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_16 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_14 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 18.0 in stage 30.0 (TID 48) (100.67.4.240, executor 4, partition 18, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 15.0 in stage 30.0 (TID 45) in 283 ms on 100.67.4.240 (executor 4) (17/50)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 19.0 in stage 30.0 (TID 49) (100.67.4.240, executor 4, partition 19, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 16.0 in stage 30.0 (TID 46) in 276 ms on 100.67.4.240 (executor 4) (18/50)\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 20.0 in stage 30.0 (TID 50) (100.67.4.240, executor 4, partition 20, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 14.0 in stage 30.0 (TID 44) in 309 ms on 100.67.4.240 (executor 4) (19/50)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_17 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "2026-02-04T01:43:51,235Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Starting task 21.0 in stage 30.0 (TID 51) (100.67.4.240, executor 4, partition 21, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:50 INFO TaskSetManager: Finished task 17.0 in stage 30.0 (TID 47) in 380 ms on 100.67.4.240 (executor 4) (20/50)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_19 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:50 INFO BlockManagerInfo: Added rdd_199_18 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_20 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 22.0 in stage 30.0 (TID 52) (100.67.4.240, executor 4, partition 22, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 19.0 in stage 30.0 (TID 49) in 312 ms on 100.67.4.240 (executor 4) (21/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 23.0 in stage 30.0 (TID 53) (100.67.4.240, executor 4, partition 23, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 20.0 in stage 30.0 (TID 50) in 301 ms on 100.67.4.240 (executor 4) (22/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 24.0 in stage 30.0 (TID 54) (100.67.4.240, executor 4, partition 24, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 18.0 in stage 30.0 (TID 48) in 319 ms on 100.67.4.240 (executor 4) (23/50)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_21 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 25.0 in stage 30.0 (TID 55) (100.67.4.240, executor 4, partition 25, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 21.0 in stage 30.0 (TID 51) in 305 ms on 100.67.4.240 (executor 4) (24/50)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_22 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_24 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_23 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 26.0 in stage 30.0 (TID 56) (100.67.4.240, executor 4, partition 26, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 22.0 in stage 30.0 (TID 52) in 214 ms on 100.67.4.240 (executor 4) (25/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 27.0 in stage 30.0 (TID 57) (100.67.4.240, executor 4, partition 27, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 24.0 in stage 30.0 (TID 54) in 278 ms on 100.67.4.240 (executor 4) (26/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 28.0 in stage 30.0 (TID 58) (100.67.4.240, executor 4, partition 28, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 23.0 in stage 30.0 (TID 53) in 293 ms on 100.67.4.240 (executor 4) (27/50)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_25 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_26 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_27 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_28 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 29.0 in stage 30.0 (TID 59) (100.67.4.240, executor 4, partition 29, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 25.0 in stage 30.0 (TID 55) in 340 ms on 100.67.4.240 (executor 4) (28/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 30.0 in stage 30.0 (TID 60) (100.67.4.240, executor 4, partition 30, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 26.0 in stage 30.0 (TID 56) in 320 ms on 100.67.4.240 (executor 4) (29/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 31.0 in stage 30.0 (TID 61) (100.67.4.240, executor 4, partition 31, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 27.0 in stage 30.0 (TID 57) in 301 ms on 100.67.4.240 (executor 4) (30/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 32.0 in stage 30.0 (TID 62) (100.67.4.240, executor 4, partition 32, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 28.0 in stage 30.0 (TID 58) in 289 ms on 100.67.4.240 (executor 4) (31/50)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_29 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_31 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_32 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_30 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 33.0 in stage 30.0 (TID 63) (100.67.4.240, executor 4, partition 33, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 29.0 in stage 30.0 (TID 59) in 177 ms on 100.67.4.240 (executor 4) (32/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 34.0 in stage 30.0 (TID 64) (100.67.4.240, executor 4, partition 34, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 31.0 in stage 30.0 (TID 61) in 124 ms on 100.67.4.240 (executor 4) (33/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 35.0 in stage 30.0 (TID 65) (100.67.4.240, executor 4, partition 35, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 32.0 in stage 30.0 (TID 62) in 130 ms on 100.67.4.240 (executor 4) (34/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 36.0 in stage 30.0 (TID 66) (100.67.4.240, executor 4, partition 36, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 30.0 in stage 30.0 (TID 60) in 182 ms on 100.67.4.240 (executor 4) (35/50)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_33 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_34 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "2026-02-04T01:43:52,402Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_36 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_35 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 37.0 in stage 30.0 (TID 67) (100.67.4.240, executor 4, partition 37, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 33.0 in stage 30.0 (TID 63) in 119 ms on 100.67.4.240 (executor 4) (36/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 38.0 in stage 30.0 (TID 68) (100.67.4.240, executor 4, partition 38, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 34.0 in stage 30.0 (TID 64) in 118 ms on 100.67.4.240 (executor 4) (37/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 39.0 in stage 30.0 (TID 69) (100.67.4.240, executor 4, partition 39, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 35.0 in stage 30.0 (TID 65) in 118 ms on 100.67.4.240 (executor 4) (38/50)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 41.0 in stage 30.0 (TID 70) (100.67.4.240, executor 4, partition 41, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 36.0 in stage 30.0 (TID 66) in 120 ms on 100.67.4.240 (executor 4) (39/50)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_37 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_38 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO BlockManagerInfo: Added rdd_199_39 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Starting task 43.0 in stage 30.0 (TID 71) (100.67.4.240, executor 4, partition 43, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:51 INFO TaskSetManager: Finished task 37.0 in stage 30.0 (TID 67) in 94 ms on 100.67.4.240 (executor 4) (40/50)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Starting task 44.0 in stage 30.0 (TID 72) (100.67.4.240, executor 4, partition 44, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 38.0 in stage 30.0 (TID 68) in 96 ms on 100.67.4.240 (executor 4) (41/50)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_41 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Starting task 45.0 in stage 30.0 (TID 73) (100.67.4.240, executor 4, partition 45, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 39.0 in stage 30.0 (TID 69) in 152 ms on 100.67.4.240 (executor 4) (42/50)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Starting task 46.0 in stage 30.0 (TID 74) (100.67.4.240, executor 4, partition 46, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 41.0 in stage 30.0 (TID 70) in 156 ms on 100.67.4.240 (executor 4) (43/50)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_43 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_45 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_44 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Starting task 47.0 in stage 30.0 (TID 75) (100.67.4.240, executor 4, partition 47, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 43.0 in stage 30.0 (TID 71) in 179 ms on 100.67.4.240 (executor 4) (44/50)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Starting task 48.0 in stage 30.0 (TID 76) (100.67.4.240, executor 4, partition 48, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 45.0 in stage 30.0 (TID 73) in 112 ms on 100.67.4.240 (executor 4) (45/50)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_46 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Starting task 49.0 in stage 30.0 (TID 77) (100.67.4.240, executor 4, partition 49, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 44.0 in stage 30.0 (TID 72) in 177 ms on 100.67.4.240 (executor 4) (46/50)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 46.0 in stage 30.0 (TID 74) in 118 ms on 100.67.4.240 (executor 4) (47/50)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_48 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_49 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added rdd_199_47 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 48.0 in stage 30.0 (TID 76) in 107 ms on 100.67.4.240 (executor 4) (48/50)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 49.0 in stage 30.0 (TID 77) in 106 ms on 100.67.4.240 (executor 4) (49/50)\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 47.0 in stage 30.0 (TID 75) in 127 ms on 100.67.4.240 (executor 4) (50/50)\n",
       "26/02/04 01:43:52 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:52 INFO DAGScheduler: ShuffleMapStage 30 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 5.197 s\n",
       "26/02/04 01:43:52 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/04 01:43:52 INFO DAGScheduler: running: Set()\n",
       "26/02/04 01:43:52 INFO DAGScheduler: waiting: Set()\n",
       "26/02/04 01:43:52 INFO DAGScheduler: failed: Set()\n",
       "26/02/04 01:43:52 INFO SparkSQLEngineListener: Job end. Job 28 state is JobSucceeded\n",
       "26/02/04 01:43:52 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:52 INFO GpuOverrides: Plan conversion to the GPU took 1.06 ms\n",
       "26/02/04 01:43:52 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:52 INFO GpuOverrides: Plan conversion to the GPU took 0.83 ms\n",
       "26/02/04 01:43:52 INFO GpuOverrides: GPU plan transition optimization took 0.28 ms\n",
       "26/02/04 01:43:52 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Got job 29 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 1 output partitions\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Final stage: ResultStage 33 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[205] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:43:52 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 547.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:52 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 131.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 100.67.56.160:7079 (size: 131.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:52 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[205] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:43:52 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:43:52 INFO FairSchedulableBuilder: Added task set TaskSet_33.0 tasks to pool \n",
       "26/02/04 01:43:52 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 78) (100.67.4.240, executor 4, partition 0, NODE_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:52 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 100.67.4.240:46241 (size: 131.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 100.67.4.240:45730\n",
       "26/02/04 01:43:52 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 78) in 113 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:43:52 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:52 INFO DAGScheduler: ResultStage 33 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.122 s\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:43:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
       "26/02/04 01:43:52 INFO SparkSQLEngineListener: Job end. Job 29 state is JobSucceeded\n",
       "26/02/04 01:43:52 INFO DAGScheduler: Job 29 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 0.123963 s\n",
       "26/02/04 01:43:52 INFO CodeGenerator: Code generated in 28.155527 ms\n",
       "26/02/04 01:43:52 INFO Snapshot: DELTA: Done\n",
       "26/02/04 01:43:52 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:52 INFO GpuOverrides: Plan conversion to the GPU took 7.17 ms\n",
       "26/02/04 01:43:52 INFO GpuOverrides: GPU plan transition optimization took 0.49 ms\n",
       "2026-02-04T01:43:53,403Z INFO ExecuteStatement: Query[63f14563-5fcd-4d2a-88c5-368dde7daed5] in RUNNING_STATE\n",
       "2026-02-04T01:43:53,548Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:53 INFO CodeGenerator: Code generated in 385.141307 ms\n",
       "26/02/04 01:43:53 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:53 INFO GpuOverrides: Plan conversion to the GPU took 2.93 ms\n",
       "26/02/04 01:43:53 INFO GpuOverrides: GPU plan transition optimization took 0.24 ms\n",
       "26/02/04 01:43:53 INFO CodeGenerator: Code generated in 15.815429 ms\n",
       "26/02/04 01:43:53 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:43:53 INFO DAGScheduler: Got job 30 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 50 output partitions\n",
       "26/02/04 01:43:53 INFO DAGScheduler: Final stage: ResultStage 35 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:43:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\n",
       "26/02/04 01:43:53 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:53 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[212] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:43:53 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 723.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:53 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 171.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:53 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 100.67.56.160:7079 (size: 171.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:53 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:53 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 35 (MapPartitionsRDD[212] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
       "26/02/04 01:43:53 INFO TaskSchedulerImpl: Adding task set 35.0 with 50 tasks resource profile 0\n",
       "26/02/04 01:43:53 INFO FairSchedulableBuilder: Added task set TaskSet_35.0 tasks to pool \n",
       "26/02/04 01:43:53 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 79) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:53 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 80) (100.67.4.240, executor 4, partition 1, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:53 INFO TaskSetManager: Starting task 2.0 in stage 35.0 (TID 81) (100.67.4.240, executor 4, partition 2, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:53 INFO TaskSetManager: Starting task 3.0 in stage 35.0 (TID 82) (100.67.4.240, executor 4, partition 3, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:53 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 100.67.4.240:46241 (size: 171.1 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:43:54,712Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:53 INFO BlockManagerInfo: Added rdd_209_3 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:53 INFO BlockManagerInfo: Added rdd_209_1 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:53 INFO BlockManagerInfo: Added rdd_209_0 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:53 INFO BlockManagerInfo: Added rdd_209_2 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 4.0 in stage 35.0 (TID 83) (100.67.4.240, executor 4, partition 4, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 80) in 644 ms on 100.67.4.240 (executor 4) (1/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 5.0 in stage 35.0 (TID 84) (100.67.4.240, executor 4, partition 5, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 3.0 in stage 35.0 (TID 82) in 643 ms on 100.67.4.240 (executor 4) (2/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 6.0 in stage 35.0 (TID 85) (100.67.4.240, executor 4, partition 6, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 2.0 in stage 35.0 (TID 81) in 644 ms on 100.67.4.240 (executor 4) (3/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 7.0 in stage 35.0 (TID 86) (100.67.4.240, executor 4, partition 7, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 79) in 646 ms on 100.67.4.240 (executor 4) (4/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_4 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_5 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_6 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_7 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 8.0 in stage 35.0 (TID 87) (100.67.4.240, executor 4, partition 8, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 4.0 in stage 35.0 (TID 83) in 25 ms on 100.67.4.240 (executor 4) (5/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 9.0 in stage 35.0 (TID 88) (100.67.4.240, executor 4, partition 9, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 10.0 in stage 35.0 (TID 89) (100.67.4.240, executor 4, partition 10, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 6.0 in stage 35.0 (TID 85) in 29 ms on 100.67.4.240 (executor 4) (6/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 7.0 in stage 35.0 (TID 86) in 28 ms on 100.67.4.240 (executor 4) (7/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 11.0 in stage 35.0 (TID 90) (100.67.4.240, executor 4, partition 11, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 5.0 in stage 35.0 (TID 84) in 31 ms on 100.67.4.240 (executor 4) (8/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_8 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_10 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 12.0 in stage 35.0 (TID 91) (100.67.4.240, executor 4, partition 12, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 8.0 in stage 35.0 (TID 87) in 20 ms on 100.67.4.240 (executor 4) (9/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 13.0 in stage 35.0 (TID 92) (100.67.4.240, executor 4, partition 13, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 10.0 in stage 35.0 (TID 89) in 18 ms on 100.67.4.240 (executor 4) (10/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_9 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 14.0 in stage 35.0 (TID 93) (100.67.4.240, executor 4, partition 14, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 9.0 in stage 35.0 (TID 88) in 64 ms on 100.67.4.240 (executor 4) (11/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_11 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 15.0 in stage 35.0 (TID 94) (100.67.4.240, executor 4, partition 15, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 11.0 in stage 35.0 (TID 90) in 69 ms on 100.67.4.240 (executor 4) (12/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_13 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_12 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_14 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 16.0 in stage 35.0 (TID 95) (100.67.4.240, executor 4, partition 16, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 13.0 in stage 35.0 (TID 92) in 63 ms on 100.67.4.240 (executor 4) (13/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 17.0 in stage 35.0 (TID 96) (100.67.4.240, executor 4, partition 17, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 12.0 in stage 35.0 (TID 91) in 69 ms on 100.67.4.240 (executor 4) (14/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 18.0 in stage 35.0 (TID 97) (100.67.4.240, executor 4, partition 18, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_15 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 14.0 in stage 35.0 (TID 93) in 23 ms on 100.67.4.240 (executor 4) (15/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 19.0 in stage 35.0 (TID 98) (100.67.4.240, executor 4, partition 19, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 15.0 in stage 35.0 (TID 94) in 20 ms on 100.67.4.240 (executor 4) (16/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_17 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_16 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_18 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 20.0 in stage 35.0 (TID 99) (100.67.4.240, executor 4, partition 20, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 17.0 in stage 35.0 (TID 96) in 19 ms on 100.67.4.240 (executor 4) (17/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 21.0 in stage 35.0 (TID 100) (100.67.4.240, executor 4, partition 21, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 16.0 in stage 35.0 (TID 95) in 22 ms on 100.67.4.240 (executor 4) (18/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 22.0 in stage 35.0 (TID 101) (100.67.4.240, executor 4, partition 22, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 18.0 in stage 35.0 (TID 97) in 19 ms on 100.67.4.240 (executor 4) (19/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_19 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 23.0 in stage 35.0 (TID 102) (100.67.4.240, executor 4, partition 23, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 19.0 in stage 35.0 (TID 98) in 73 ms on 100.67.4.240 (executor 4) (20/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_22 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_21 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_20 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 24.0 in stage 35.0 (TID 103) (100.67.4.240, executor 4, partition 24, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 22.0 in stage 35.0 (TID 101) in 72 ms on 100.67.4.240 (executor 4) (21/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 25.0 in stage 35.0 (TID 104) (100.67.4.240, executor 4, partition 25, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 21.0 in stage 35.0 (TID 100) in 74 ms on 100.67.4.240 (executor 4) (22/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 26.0 in stage 35.0 (TID 105) (100.67.4.240, executor 4, partition 26, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 20.0 in stage 35.0 (TID 99) in 76 ms on 100.67.4.240 (executor 4) (23/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_23 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 27.0 in stage 35.0 (TID 106) (100.67.4.240, executor 4, partition 27, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 23.0 in stage 35.0 (TID 102) in 22 ms on 100.67.4.240 (executor 4) (24/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_26 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_25 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_24 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 28.0 in stage 35.0 (TID 107) (100.67.4.240, executor 4, partition 28, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 26.0 in stage 35.0 (TID 105) in 21 ms on 100.67.4.240 (executor 4) (25/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 29.0 in stage 35.0 (TID 108) (100.67.4.240, executor 4, partition 29, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 25.0 in stage 35.0 (TID 104) in 24 ms on 100.67.4.240 (executor 4) (26/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 30.0 in stage 35.0 (TID 109) (100.67.4.240, executor 4, partition 30, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 24.0 in stage 35.0 (TID 103) in 27 ms on 100.67.4.240 (executor 4) (27/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_27 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 31.0 in stage 35.0 (TID 110) (100.67.4.240, executor 4, partition 31, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 27.0 in stage 35.0 (TID 106) in 80 ms on 100.67.4.240 (executor 4) (28/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_28 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_30 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_29 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 32.0 in stage 35.0 (TID 111) (100.67.4.240, executor 4, partition 32, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 28.0 in stage 35.0 (TID 107) in 72 ms on 100.67.4.240 (executor 4) (29/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 33.0 in stage 35.0 (TID 112) (100.67.4.240, executor 4, partition 33, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 30.0 in stage 35.0 (TID 109) in 68 ms on 100.67.4.240 (executor 4) (30/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 34.0 in stage 35.0 (TID 113) (100.67.4.240, executor 4, partition 34, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 29.0 in stage 35.0 (TID 108) in 73 ms on 100.67.4.240 (executor 4) (31/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_32 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_31 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_33 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 35.0 in stage 35.0 (TID 114) (100.67.4.240, executor 4, partition 35, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 32.0 in stage 35.0 (TID 111) in 18 ms on 100.67.4.240 (executor 4) (32/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 36.0 in stage 35.0 (TID 115) (100.67.4.240, executor 4, partition 36, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 31.0 in stage 35.0 (TID 110) in 25 ms on 100.67.4.240 (executor 4) (33/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 37.0 in stage 35.0 (TID 116) (100.67.4.240, executor 4, partition 37, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 33.0 in stage 35.0 (TID 112) in 19 ms on 100.67.4.240 (executor 4) (34/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_34 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 38.0 in stage 35.0 (TID 117) (100.67.4.240, executor 4, partition 38, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 34.0 in stage 35.0 (TID 113) in 23 ms on 100.67.4.240 (executor 4) (35/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_35 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_37 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 39.0 in stage 35.0 (TID 118) (100.67.4.240, executor 4, partition 39, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 35.0 in stage 35.0 (TID 114) in 20 ms on 100.67.4.240 (executor 4) (36/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 40.0 in stage 35.0 (TID 119) (100.67.4.240, executor 4, partition 40, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 37.0 in stage 35.0 (TID 116) in 18 ms on 100.67.4.240 (executor 4) (37/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_36 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 41.0 in stage 35.0 (TID 120) (100.67.4.240, executor 4, partition 41, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 36.0 in stage 35.0 (TID 115) in 26 ms on 100.67.4.240 (executor 4) (38/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_38 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 42.0 in stage 35.0 (TID 121) (100.67.4.240, executor 4, partition 42, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 38.0 in stage 35.0 (TID 117) in 68 ms on 100.67.4.240 (executor 4) (39/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_40 in memory on 100.67.4.240:46241 (size: 870.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_39 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 43.0 in stage 35.0 (TID 122) (100.67.4.240, executor 4, partition 43, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 39.0 in stage 35.0 (TID 118) in 67 ms on 100.67.4.240 (executor 4) (40/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 44.0 in stage 35.0 (TID 123) (100.67.4.240, executor 4, partition 44, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 40.0 in stage 35.0 (TID 119) in 67 ms on 100.67.4.240 (executor 4) (41/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_41 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 45.0 in stage 35.0 (TID 124) (100.67.4.240, executor 4, partition 45, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 41.0 in stage 35.0 (TID 120) in 66 ms on 100.67.4.240 (executor 4) (42/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_42 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_43 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 46.0 in stage 35.0 (TID 125) (100.67.4.240, executor 4, partition 46, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 42.0 in stage 35.0 (TID 121) in 25 ms on 100.67.4.240 (executor 4) (43/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 47.0 in stage 35.0 (TID 126) (100.67.4.240, executor 4, partition 47, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 43.0 in stage 35.0 (TID 122) in 17 ms on 100.67.4.240 (executor 4) (44/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_45 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_44 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 48.0 in stage 35.0 (TID 127) (100.67.4.240, executor 4, partition 48, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 45.0 in stage 35.0 (TID 124) in 18 ms on 100.67.4.240 (executor 4) (45/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Starting task 49.0 in stage 35.0 (TID 128) (100.67.4.240, executor 4, partition 49, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 44.0 in stage 35.0 (TID 123) in 27 ms on 100.67.4.240 (executor 4) (46/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_47 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_46 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 47.0 in stage 35.0 (TID 126) in 73 ms on 100.67.4.240 (executor 4) (47/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 46.0 in stage 35.0 (TID 125) in 78 ms on 100.67.4.240 (executor 4) (48/50)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_49 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO BlockManagerInfo: Added rdd_209_48 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 49.0 in stage 35.0 (TID 128) in 73 ms on 100.67.4.240 (executor 4) (49/50)\n",
       "26/02/04 01:43:54 INFO TaskSetManager: Finished task 48.0 in stage 35.0 (TID 127) in 79 ms on 100.67.4.240 (executor 4) (50/50)\n",
       "26/02/04 01:43:54 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:54 INFO DAGScheduler: ResultStage 35 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 1.158 s\n",
       "26/02/04 01:43:54 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:43:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished\n",
       "26/02/04 01:43:54 INFO SparkSQLEngineListener: Job end. Job 30 state is JobSucceeded\n",
       "26/02/04 01:43:54 INFO DAGScheduler: Job 30 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 1.161538 s\n",
       "26/02/04 01:43:54 INFO CodeGenerator: Code generated in 13.546132 ms\n",
       "26/02/04 01:43:54 INFO ExecutePython:  Delta table saved to: s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes\n",
       "26/02/04 01:43:54 INFO ExecutePython: root\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- node_id: string (nullable = true)\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- scontrol_state: string (nullable = true)\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- reason: string (nullable = true)\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- updated_at: string (nullable = true)\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- stale: boolean (nullable = true)\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- reason_changed_at: string (nullable = true)\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- cluster_id: string (nullable = true)\n",
       "26/02/04 01:43:54 INFO ExecutePython:  |-- last_busy_at: string (nullable = true)\n",
       "2026-02-04T01:43:55,668Z INFO ExecuteStatement: Query[63f14563-5fcd-4d2a-88c5-368dde7daed5] in FINISHED_STATE\n",
       "2026-02-04T01:43:55,668Z INFO ExecuteStatement: Processing anonymous's query[63f14563-5fcd-4d2a-88c5-368dde7daed5]: RUNNING_STATE -> FINISHED_STATE, time taken: 22.335 seconds\n",
       "2026-02-04T01:43:55,861Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:54 INFO PrepareDeltaScan: DELTA: Filtering files for query\n",
       "26/02/04 01:43:55 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:43:55 INFO GpuOverrides: Plan conversion to the GPU took 1.20 ms\n",
       "26/02/04 01:43:55 INFO GpuOverrides: GPU plan transition optimization took 0.22 ms\n",
       "26/02/04 01:43:55 INFO CodeGenerator: Code generated in 17.323965 ms\n",
       "26/02/04 01:43:55 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Got job 31 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 50 output partitions\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Final stage: ResultStage 37 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[214] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:43:55 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 728.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 172.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 100.67.56.160:7079 (size: 172.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 37 (MapPartitionsRDD[214] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
       "26/02/04 01:43:55 INFO TaskSchedulerImpl: Adding task set 37.0 with 50 tasks resource profile 0\n",
       "26/02/04 01:43:55 INFO FairSchedulableBuilder: Added task set TaskSet_37.0 tasks to pool \n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 129) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 1.0 in stage 37.0 (TID 130) (100.67.4.240, executor 4, partition 1, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 2.0 in stage 37.0 (TID 131) (100.67.4.240, executor 4, partition 2, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 3.0 in stage 37.0 (TID 132) (100.67.4.240, executor 4, partition 3, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 100.67.4.240:46241 (size: 172.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 4.0 in stage 37.0 (TID 133) (100.67.4.240, executor 4, partition 4, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 1.0 in stage 37.0 (TID 130) in 57 ms on 100.67.4.240 (executor 4) (1/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 5.0 in stage 37.0 (TID 134) (100.67.4.240, executor 4, partition 5, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 129) in 58 ms on 100.67.4.240 (executor 4) (2/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 6.0 in stage 37.0 (TID 135) (100.67.4.240, executor 4, partition 6, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 2.0 in stage 37.0 (TID 131) in 58 ms on 100.67.4.240 (executor 4) (3/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 7.0 in stage 37.0 (TID 136) (100.67.4.240, executor 4, partition 7, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 3.0 in stage 37.0 (TID 132) in 59 ms on 100.67.4.240 (executor 4) (4/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 8.0 in stage 37.0 (TID 137) (100.67.4.240, executor 4, partition 8, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 4.0 in stage 37.0 (TID 133) in 12 ms on 100.67.4.240 (executor 4) (5/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 9.0 in stage 37.0 (TID 138) (100.67.4.240, executor 4, partition 9, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 7.0 in stage 37.0 (TID 136) in 12 ms on 100.67.4.240 (executor 4) (6/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 10.0 in stage 37.0 (TID 139) (100.67.4.240, executor 4, partition 10, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 5.0 in stage 37.0 (TID 134) in 14 ms on 100.67.4.240 (executor 4) (7/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 11.0 in stage 37.0 (TID 140) (100.67.4.240, executor 4, partition 11, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 6.0 in stage 37.0 (TID 135) in 16 ms on 100.67.4.240 (executor 4) (8/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 12.0 in stage 37.0 (TID 141) (100.67.4.240, executor 4, partition 12, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 8.0 in stage 37.0 (TID 137) in 12 ms on 100.67.4.240 (executor 4) (9/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 13.0 in stage 37.0 (TID 142) (100.67.4.240, executor 4, partition 13, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 10.0 in stage 37.0 (TID 139) in 11 ms on 100.67.4.240 (executor 4) (10/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 14.0 in stage 37.0 (TID 143) (100.67.4.240, executor 4, partition 14, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 9.0 in stage 37.0 (TID 138) in 12 ms on 100.67.4.240 (executor 4) (11/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 15.0 in stage 37.0 (TID 144) (100.67.4.240, executor 4, partition 15, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 11.0 in stage 37.0 (TID 140) in 16 ms on 100.67.4.240 (executor 4) (12/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 16.0 in stage 37.0 (TID 145) (100.67.4.240, executor 4, partition 16, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 13.0 in stage 37.0 (TID 142) in 10 ms on 100.67.4.240 (executor 4) (13/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 17.0 in stage 37.0 (TID 146) (100.67.4.240, executor 4, partition 17, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 12.0 in stage 37.0 (TID 141) in 13 ms on 100.67.4.240 (executor 4) (14/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 18.0 in stage 37.0 (TID 147) (100.67.4.240, executor 4, partition 18, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 14.0 in stage 37.0 (TID 143) in 11 ms on 100.67.4.240 (executor 4) (15/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 19.0 in stage 37.0 (TID 148) (100.67.4.240, executor 4, partition 19, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 15.0 in stage 37.0 (TID 144) in 11 ms on 100.67.4.240 (executor 4) (16/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 20.0 in stage 37.0 (TID 149) (100.67.4.240, executor 4, partition 20, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 16.0 in stage 37.0 (TID 145) in 11 ms on 100.67.4.240 (executor 4) (17/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 21.0 in stage 37.0 (TID 150) (100.67.4.240, executor 4, partition 21, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 17.0 in stage 37.0 (TID 146) in 12 ms on 100.67.4.240 (executor 4) (18/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 22.0 in stage 37.0 (TID 151) (100.67.4.240, executor 4, partition 22, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 18.0 in stage 37.0 (TID 147) in 13 ms on 100.67.4.240 (executor 4) (19/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 23.0 in stage 37.0 (TID 152) (100.67.4.240, executor 4, partition 23, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 19.0 in stage 37.0 (TID 148) in 11 ms on 100.67.4.240 (executor 4) (20/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 24.0 in stage 37.0 (TID 153) (100.67.4.240, executor 4, partition 24, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 20.0 in stage 37.0 (TID 149) in 11 ms on 100.67.4.240 (executor 4) (21/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 25.0 in stage 37.0 (TID 154) (100.67.4.240, executor 4, partition 25, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 21.0 in stage 37.0 (TID 150) in 11 ms on 100.67.4.240 (executor 4) (22/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 26.0 in stage 37.0 (TID 155) (100.67.4.240, executor 4, partition 26, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 22.0 in stage 37.0 (TID 151) in 11 ms on 100.67.4.240 (executor 4) (23/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 27.0 in stage 37.0 (TID 156) (100.67.4.240, executor 4, partition 27, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 24.0 in stage 37.0 (TID 153) in 12 ms on 100.67.4.240 (executor 4) (24/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 28.0 in stage 37.0 (TID 157) (100.67.4.240, executor 4, partition 28, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 23.0 in stage 37.0 (TID 152) in 15 ms on 100.67.4.240 (executor 4) (25/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 29.0 in stage 37.0 (TID 158) (100.67.4.240, executor 4, partition 29, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 25.0 in stage 37.0 (TID 154) in 13 ms on 100.67.4.240 (executor 4) (26/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 30.0 in stage 37.0 (TID 159) (100.67.4.240, executor 4, partition 30, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 26.0 in stage 37.0 (TID 155) in 14 ms on 100.67.4.240 (executor 4) (27/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 31.0 in stage 37.0 (TID 160) (100.67.4.240, executor 4, partition 31, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 28.0 in stage 37.0 (TID 157) in 14 ms on 100.67.4.240 (executor 4) (28/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 32.0 in stage 37.0 (TID 161) (100.67.4.240, executor 4, partition 32, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 29.0 in stage 37.0 (TID 158) in 15 ms on 100.67.4.240 (executor 4) (29/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 33.0 in stage 37.0 (TID 162) (100.67.4.240, executor 4, partition 33, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 27.0 in stage 37.0 (TID 156) in 19 ms on 100.67.4.240 (executor 4) (30/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 34.0 in stage 37.0 (TID 163) (100.67.4.240, executor 4, partition 34, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 30.0 in stage 37.0 (TID 159) in 15 ms on 100.67.4.240 (executor 4) (31/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 35.0 in stage 37.0 (TID 164) (100.67.4.240, executor 4, partition 35, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 31.0 in stage 37.0 (TID 160) in 15 ms on 100.67.4.240 (executor 4) (32/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 36.0 in stage 37.0 (TID 165) (100.67.4.240, executor 4, partition 36, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 32.0 in stage 37.0 (TID 161) in 12 ms on 100.67.4.240 (executor 4) (33/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 37.0 in stage 37.0 (TID 166) (100.67.4.240, executor 4, partition 37, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 33.0 in stage 37.0 (TID 162) in 12 ms on 100.67.4.240 (executor 4) (34/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 38.0 in stage 37.0 (TID 167) (100.67.4.240, executor 4, partition 38, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 35.0 in stage 37.0 (TID 164) in 11 ms on 100.67.4.240 (executor 4) (35/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 39.0 in stage 37.0 (TID 168) (100.67.4.240, executor 4, partition 39, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 34.0 in stage 37.0 (TID 163) in 20 ms on 100.67.4.240 (executor 4) (36/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 40.0 in stage 37.0 (TID 169) (100.67.4.240, executor 4, partition 40, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 36.0 in stage 37.0 (TID 165) in 11 ms on 100.67.4.240 (executor 4) (37/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 41.0 in stage 37.0 (TID 170) (100.67.4.240, executor 4, partition 41, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 37.0 in stage 37.0 (TID 166) in 12 ms on 100.67.4.240 (executor 4) (38/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 42.0 in stage 37.0 (TID 171) (100.67.4.240, executor 4, partition 42, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 39.0 in stage 37.0 (TID 168) in 11 ms on 100.67.4.240 (executor 4) (39/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 43.0 in stage 37.0 (TID 172) (100.67.4.240, executor 4, partition 43, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 40.0 in stage 37.0 (TID 169) in 10 ms on 100.67.4.240 (executor 4) (40/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 44.0 in stage 37.0 (TID 173) (100.67.4.240, executor 4, partition 44, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 41.0 in stage 37.0 (TID 170) in 10 ms on 100.67.4.240 (executor 4) (41/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 45.0 in stage 37.0 (TID 174) (100.67.4.240, executor 4, partition 45, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 38.0 in stage 37.0 (TID 167) in 17 ms on 100.67.4.240 (executor 4) (42/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 46.0 in stage 37.0 (TID 175) (100.67.4.240, executor 4, partition 46, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 43.0 in stage 37.0 (TID 172) in 11 ms on 100.67.4.240 (executor 4) (43/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 47.0 in stage 37.0 (TID 176) (100.67.4.240, executor 4, partition 47, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 44.0 in stage 37.0 (TID 173) in 10 ms on 100.67.4.240 (executor 4) (44/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 48.0 in stage 37.0 (TID 177) (100.67.4.240, executor 4, partition 48, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 42.0 in stage 37.0 (TID 171) in 34 ms on 100.67.4.240 (executor 4) (45/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 49.0 in stage 37.0 (TID 178) (100.67.4.240, executor 4, partition 49, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 46.0 in stage 37.0 (TID 175) in 31 ms on 100.67.4.240 (executor 4) (46/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 47.0 in stage 37.0 (TID 176) in 30 ms on 100.67.4.240 (executor 4) (47/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 45.0 in stage 37.0 (TID 174) in 38 ms on 100.67.4.240 (executor 4) (48/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 48.0 in stage 37.0 (TID 177) in 11 ms on 100.67.4.240 (executor 4) (49/50)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 49.0 in stage 37.0 (TID 178) in 13 ms on 100.67.4.240 (executor 4) (50/50)\n",
       "26/02/04 01:43:55 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:55 INFO DAGScheduler: ResultStage 37 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.238 s\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:43:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
       "26/02/04 01:43:55 INFO SparkSQLEngineListener: Job end. Job 31 state is JobSucceeded\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Job 31 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 0.241157 s\n",
       "26/02/04 01:43:55 INFO CodeGenerator: Code generated in 16.274742 ms\n",
       "26/02/04 01:43:55 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:43:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:43:55 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 01:43:55 INFO GpuOverrides: Plan conversion to the GPU took 3.97 ms\n",
       "26/02/04 01:43:55 INFO GpuOverrides: GPU plan transition optimization took 0.87 ms\n",
       "26/02/04 01:43:55 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:43:55 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 472.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 100.67.56.160:7079 (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO SparkContext: Created broadcast 43 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:43:55 WARN GpuDelta33xParquetFileFormat: Coalescing is not supported when \\`delta.enableDeletionVectors=true\\`, using the multi-threaded reader. For more details on the Parquet reader types please look at 'spark.rapids.sql.format.parquet.reader.type' config at https://nvidia.github.io/spark-rapids/docs/additional-functionality/advanced_configs.html\n",
       "26/02/04 01:43:55 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Got job 32 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Final stage: ResultStage 38 (showString at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[223] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:43:55 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 28.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 100.67.56.160:7079 (size: 13.5 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:43:55 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[223] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:43:55 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:43:55 INFO FairSchedulableBuilder: Added task set TaskSet_38.0 tasks to pool \n",
       "26/02/04 01:43:55 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 179) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10187 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:43:55 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 100.67.4.240:46241 (size: 13.5 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:55 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 100.67.4.240:46241 (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:43:55 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 179) in 195 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:43:55 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:43:55 INFO DAGScheduler: ResultStage 38 (showString at NativeMethodAccessorImpl.java:0) finished in 0.202 s\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:43:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished\n",
       "26/02/04 01:43:55 INFO SparkSQLEngineListener: Job end. Job 32 state is JobSucceeded\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Job 32 finished: showString at NativeMethodAccessorImpl.java:0, took 0.203144 s\n",
       "26/02/04 01:43:55 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:43:55 INFO ExecutePython: |node_id   |scontrol_state|reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 01:43:55 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-001|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-002|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-003|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-004|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-005|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-006|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-007|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-008|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-009|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: |cpu-dm-010|[\"IDLE\"]      |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:43:55 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:43:55 INFO ExecutePython: Processing anonymous's query[63f14563-5fcd-4d2a-88c5-368dde7daed5]: RUNNING_STATE -> FINISHED_STATE, time taken: 22.33 seconds\n",
       "26/02/04 01:43:55 INFO DAGScheduler: Asked to cancel job group 63f14563-5fcd-4d2a-88c5-368dde7daed5\n",
       "2026-02-04T01:43:56,013Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/63f14563-5fcd-4d2a-88c5-368dde7daed5/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:43:56 INFO DAGScheduler: Asked to cancel job group 63f14563-5fcd-4d2a-88c5-368dde7daed5\n",
       "26/02/04 01:43:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:44:17,067Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:44:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:44:47,067Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T01:44:47,067Z WARN KyuubiOperationManager: Operation OperationHandle [3adf5d34-b306-4ef7-a540-24d371f60c6e] is timed-out and will be closed\n",
       "2026-02-04T01:44:47,069Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:3A DF 5D 34 B3 06 4E F7 A5 40 24 D3 71 F6 0C 6E, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:44:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:44:59 WARN SparkSQLOperationManager: Operation OperationHandle [63f14563-5fcd-4d2a-88c5-368dde7daed5] is timed-out and will be closed\n",
       "2026-02-04T01:45:17,069Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T01:45:17,069Z WARN KyuubiOperationManager: Operation OperationHandle [63f14563-5fcd-4d2a-88c5-368dde7daed5] is timed-out and will be closed\n",
       "2026-02-04T01:45:17,071Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:63 F1 45 63 5F CD 4D 2A 88 C5 36 8D DE 7D AE D5, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [63f14563-5fcd-4d2a-88c5-368dde7daed5]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:45:17 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [63f14563-5fcd-4d2a-88c5-368dde7daed5]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:45:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:45:47,072Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:45:55 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.4.240:35384\n",
       "26/02/04 01:45:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:46:17,072Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:46:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:46:47,072Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:46:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:47:13,825Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/55e2cfe7-c6f3-437a-9624-23a976b9af3a\n",
       "2026-02-04T01:47:13,827Z INFO ExecuteStatement: Processing anonymous's query[55e2cfe7-c6f3-437a-9624-23a976b9af3a]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:47:13,828Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:47:13,833Z INFO ExecuteStatement: Query[55e2cfe7-c6f3-437a-9624-23a976b9af3a] in FINISHED_STATE\n",
       "2026-02-04T01:47:13,833Z INFO ExecuteStatement: Processing anonymous's query[55e2cfe7-c6f3-437a-9624-23a976b9af3a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "26/02/04 01:47:13 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/55e2cfe7-c6f3-437a-9624-23a976b9af3a\n",
       "26/02/04 01:47:13 INFO ExecutePython: Processing anonymous's query[55e2cfe7-c6f3-437a-9624-23a976b9af3a]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:47:13 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:47:13 INFO ExecutePython: Processing anonymous's query[55e2cfe7-c6f3-437a-9624-23a976b9af3a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 01:47:13 INFO DAGScheduler: Asked to cancel job group 55e2cfe7-c6f3-437a-9624-23a976b9af3a\n",
       "2026-02-04T01:47:14,372Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/d106d6cd-1ee6-4484-9754-e425ddb54bae\n",
       "2026-02-04T01:47:14,375Z INFO ExecuteStatement: Processing anonymous's query[d106d6cd-1ee6-4484-9754-e425ddb54bae]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T01:47:14,375Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/4076596d-1c27-407c-af0b-d372cb2a90f3/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:47:14,837Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d106d6cd-1ee6-4484-9754-e425ddb54bae/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:47:14 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/4076596d-1c27-407c-af0b-d372cb2a90f3/d106d6cd-1ee6-4484-9754-e425ddb54bae\n",
       "26/02/04 01:47:14 INFO ExecutePython: Processing anonymous's query[d106d6cd-1ee6-4484-9754-e425ddb54bae]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 01:47:14 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 01:47:14 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.\n",
       "26/02/04 01:47:14 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Got job 33 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Final stage: ResultStage 39 (parquet at NativeMethodAccessorImpl.java:0)\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[225] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Job 33 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Stage 39.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:47:14 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 138.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[225] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:47:14 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:47:14 INFO FairSchedulableBuilder: Added task set TaskSet_39.0 tasks to pool \n",
       "26/02/04 01:47:14 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 180) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9368 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:14 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 100.67.4.240:46241 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:14 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 180) in 106 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:47:14 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:14 INFO DAGScheduler: ResultStage 39 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.120 s\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:47:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Finished stage: Stage(39, 0); Name: 'parquet at NativeMethodAccessorImpl.java:0'; Status: succeeded; numTasks: 1; Took: 120 msec\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Job 33 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.121993 s\n",
       "26/02/04 01:47:14 INFO StatsReportListener: task runtime:(count: 1, mean: 106.000000, stdev: 0.000000, max: 106.000000, min: 106.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\n",
       "26/02/04 01:47:14 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:47:14 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:47:14 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:47:14 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 01:47:14 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 83.962264, stdev: 0.000000, max: 83.962264, min: 83.962264)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\t84 %\n",
       "26/02/04 01:47:14 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:47:14 INFO StatsReportListener: other time pct: (count: 1, mean: 16.037736, stdev: 0.000000, max: 16.037736, min: 16.037736)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\t16 %\n",
       "26/02/04 01:47:14 INFO SparkSQLEngineListener: Job end. Job 33 state is JobSucceeded\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Job 33 succeeded, 0 active jobs running\n",
       "26/02/04 01:47:14 INFO GpuOverrides: Plan conversion to the GPU took 1.17 ms\n",
       "26/02/04 01:47:14 INFO GpuOverrides: GPU plan transition optimization took 0.14 ms\n",
       "26/02/04 01:47:14 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:47:14 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:47:14 INFO GpuOverrides: Plan conversion to the GPU took 0.53 ms\n",
       "26/02/04 01:47:14 INFO GpuOverrides: GPU plan transition optimization took 0.35 ms\n",
       "26/02/04 01:47:14 INFO GpuParquetFileFormat: Using user defined output committer for Parquet: org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n",
       "26/02/04 01:47:14 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:47:14 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 472.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 100.67.56.160:7079 (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO SparkContext: Created broadcast 46 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:47:14 WARN GpuDeltaInvariantCheckerExec: GpuRapidsDeltaWriteExec returned empty metrics in getOpTimeNewMetric\n",
       "26/02/04 01:47:14 WARN GpuOptimizeWriteExchangeExec: GpuRapidsDeltaWriteExec returned empty metrics in getOpTimeNewMetric\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Registering RDD 234 (RDD at GpuExec.scala:58) as input to shuffle 5\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Got map stage job 34 (internalDoExecuteColumnar at GpuExec.scala:341) with 1 output partitions\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Final stage: ShuffleMapStage 40 (RDD at GpuExec.scala:58)\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Job 34 started with 1 stages, 1 active jobs running\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Submitting ShuffleMapStage 40 (GpuOpTimeTrackingRDD[234] at RDD at GpuExec.scala:58), which has no missing parents\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Stage 40.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:47:14 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 28.7 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 13.3 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 100.67.56.160:7079 (size: 13.3 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:14 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 40 (GpuOpTimeTrackingRDD[234] at RDD at GpuExec.scala:58) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:47:14 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:47:14 INFO FairSchedulableBuilder: Added task set TaskSet_40.0 tasks to pool \n",
       "26/02/04 01:47:14 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 181) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10131 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:14 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 100.67.4.240:46241 (size: 13.3 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:14 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 100.67.4.240:46241 (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:14 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 181) in 202 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:47:14 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:14 INFO DAGScheduler: ShuffleMapStage 40 (RDD at GpuExec.scala:58) finished in 0.207 s\n",
       "26/02/04 01:47:14 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/04 01:47:14 INFO DAGScheduler: running: Set()\n",
       "26/02/04 01:47:14 INFO DAGScheduler: waiting: Set()\n",
       "26/02/04 01:47:14 INFO DAGScheduler: failed: Set()\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Finished stage: Stage(40, 0); Name: 'RDD at GpuExec.scala:58'; Status: succeeded; numTasks: 1; Took: 207 msec\n",
       "26/02/04 01:47:14 INFO StatsReportListener: task runtime:(count: 1, mean: 202.000000, stdev: 0.000000, max: 202.000000, min: 202.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t202.0 ms\t202.0 ms\t202.0 ms\t202.0 ms\t202.0 ms\t202.0 ms\t202.0 ms\t202.0 ms\t202.0 ms\n",
       "26/02/04 01:47:14 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 2306.000000, stdev: 0.000000, max: 2306.000000, min: 2306.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\t2.3 KiB\n",
       "26/02/04 01:47:14 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:47:14 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:47:14 INFO StatsReportListener: task result size:(count: 1, mean: 8218.000000, stdev: 0.000000, max: 8218.000000, min: 8218.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t8.0 KiB\t8.0 KiB\t8.0 KiB\t8.0 KiB\t8.0 KiB\t8.0 KiB\t8.0 KiB\t8.0 KiB\t8.0 KiB\n",
       "26/02/04 01:47:14 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 94.059406, stdev: 0.000000, max: 94.059406, min: 94.059406)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\n",
       "26/02/04 01:47:14 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:47:14 INFO StatsReportListener: other time pct: (count: 1, mean: 5.940594, stdev: 0.000000, max: 5.940594, min: 5.940594)\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:14 INFO StatsReportListener: \t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\n",
       "26/02/04 01:47:14 INFO SparkSQLEngineListener: Job end. Job 34 state is JobSucceeded\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Job 34 succeeded, 0 active jobs running\n",
       "26/02/04 01:47:14 INFO SparkContext: Starting job: run at GpuDeltaDataSource.scala:57\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Got job 35 (run at GpuDeltaDataSource.scala:57) with 1 output partitions\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Final stage: ResultStage 42 (run at GpuDeltaDataSource.scala:57)\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:14 INFO DAGScheduler: Submitting ResultStage 42 (GpuOpTimeTrackingRDD[238] at RDD at GpuExec.scala:58), which has no missing parents\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Job 35 started with 2 stages, 1 active jobs running\n",
       "26/02/04 01:47:14 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Stage 42.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 01:47:15 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 321.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 119.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 100.67.56.160:7079 (size: 119.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (GpuOpTimeTrackingRDD[238] at RDD at GpuExec.scala:58) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:47:15 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:47:15 INFO FairSchedulableBuilder: Added task set TaskSet_42.0 tasks to pool \n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 182) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9238 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 100.67.4.240:46241 (size: 119.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 100.67.4.240:45730\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 182) in 189 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:47:15 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:15 INFO DAGScheduler: ResultStage 42 (run at GpuDeltaDataSource.scala:57) finished in 0.212 s\n",
       "26/02/04 01:47:15 INFO SQLOperationListener: Finished stage: Stage(42, 0); Name: 'run at GpuDeltaDataSource.scala:57'; Status: succeeded; numTasks: 1; Took: 212 msec\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:47:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Job 35 finished: run at GpuDeltaDataSource.scala:57, took 0.215471 s\n",
       "26/02/04 01:47:15 INFO StatsReportListener: task runtime:(count: 1, mean: 189.000000, stdev: 0.000000, max: 189.000000, min: 189.000000)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t189.0 ms\t189.0 ms\t189.0 ms\t189.0 ms\t189.0 ms\t189.0 ms\t189.0 ms\t189.0 ms\t189.0 ms\n",
       "26/02/04 01:47:15 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:47:15 INFO GpuDeltaFileFormatWriter: Write Job 19510deb-18b3-4fbf-8c99-641eeb56da3a committed. Elapsed time: 0 ms.\n",
       "26/02/04 01:47:15 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 01:47:15 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 01:47:15 INFO StatsReportListener: task result size:(count: 1, mean: 7523.000000, stdev: 0.000000, max: 7523.000000, min: 7523.000000)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t7.3 KiB\t7.3 KiB\t7.3 KiB\t7.3 KiB\t7.3 KiB\t7.3 KiB\t7.3 KiB\t7.3 KiB\t7.3 KiB\n",
       "26/02/04 01:47:15 INFO GpuDeltaFileFormatWriter: Finished processing stats for write job 19510deb-18b3-4fbf-8c99-641eeb56da3a.\n",
       "26/02/04 01:47:15 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 86.772487, stdev: 0.000000, max: 86.772487, min: 86.772487)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t87 %\t87 %\t87 %\t87 %\t87 %\t87 %\t87 %\t87 %\t87 %\n",
       "26/02/04 01:47:15 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 01:47:15 INFO StatsReportListener: other time pct: (count: 1, mean: 13.227513, stdev: 0.000000, max: 13.227513, min: 13.227513)\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 01:47:15 INFO StatsReportListener: \t13 %\t13 %\t13 %\t13 %\t13 %\t13 %\t13 %\t13 %\t13 %\n",
       "26/02/04 01:47:15 INFO SparkSQLEngineListener: Job end. Job 35 state is JobSucceeded\n",
       "26/02/04 01:47:15 INFO SQLOperationListener: Query [d106d6cd-1ee6-4484-9754-e425ddb54bae]: Job 35 succeeded, 0 active jobs running\n",
       "26/02/04 01:47:15 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:15 INFO GpuOverrides: Plan conversion to the GPU took 2.74 ms\n",
       "26/02/04 01:47:15 INFO GpuOverrides: GPU plan transition optimization took 0.49 ms\n",
       "26/02/04 01:47:15 INFO CodeGenerator: Code generated in 181.482691 ms\n",
       "26/02/04 01:47:15 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Got job 36 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 50 output partitions\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Final stage: ResultStage 44 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[240] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:47:15 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 697.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 162.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 100.67.56.160:7079 (size: 162.5 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:15 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 44 (MapPartitionsRDD[240] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
       "26/02/04 01:47:15 INFO TaskSchedulerImpl: Adding task set 44.0 with 50 tasks resource profile 0\n",
       "26/02/04 01:47:15 INFO FairSchedulableBuilder: Added task set TaskSet_44.0 tasks to pool \n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 183) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 1.0 in stage 44.0 (TID 184) (100.67.4.240, executor 4, partition 1, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 2.0 in stage 44.0 (TID 185) (100.67.4.240, executor 4, partition 2, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 3.0 in stage 44.0 (TID 186) (100.67.4.240, executor 4, partition 3, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 100.67.4.240:46241 (size: 162.5 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 4.0 in stage 44.0 (TID 187) (100.67.4.240, executor 4, partition 4, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 183) in 97 ms on 100.67.4.240 (executor 4) (1/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 5.0 in stage 44.0 (TID 188) (100.67.4.240, executor 4, partition 5, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 1.0 in stage 44.0 (TID 184) in 97 ms on 100.67.4.240 (executor 4) (2/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 6.0 in stage 44.0 (TID 189) (100.67.4.240, executor 4, partition 6, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 2.0 in stage 44.0 (TID 185) in 98 ms on 100.67.4.240 (executor 4) (3/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 7.0 in stage 44.0 (TID 190) (100.67.4.240, executor 4, partition 7, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 3.0 in stage 44.0 (TID 186) in 99 ms on 100.67.4.240 (executor 4) (4/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 8.0 in stage 44.0 (TID 191) (100.67.4.240, executor 4, partition 8, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 4.0 in stage 44.0 (TID 187) in 12 ms on 100.67.4.240 (executor 4) (5/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 9.0 in stage 44.0 (TID 192) (100.67.4.240, executor 4, partition 9, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 5.0 in stage 44.0 (TID 188) in 12 ms on 100.67.4.240 (executor 4) (6/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 10.0 in stage 44.0 (TID 193) (100.67.4.240, executor 4, partition 10, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 6.0 in stage 44.0 (TID 189) in 11 ms on 100.67.4.240 (executor 4) (7/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 11.0 in stage 44.0 (TID 194) (100.67.4.240, executor 4, partition 11, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 7.0 in stage 44.0 (TID 190) in 12 ms on 100.67.4.240 (executor 4) (8/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 12.0 in stage 44.0 (TID 195) (100.67.4.240, executor 4, partition 12, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 8.0 in stage 44.0 (TID 191) in 10 ms on 100.67.4.240 (executor 4) (9/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 13.0 in stage 44.0 (TID 196) (100.67.4.240, executor 4, partition 13, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 9.0 in stage 44.0 (TID 192) in 11 ms on 100.67.4.240 (executor 4) (10/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 14.0 in stage 44.0 (TID 197) (100.67.4.240, executor 4, partition 14, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 10.0 in stage 44.0 (TID 193) in 10 ms on 100.67.4.240 (executor 4) (11/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 15.0 in stage 44.0 (TID 198) (100.67.4.240, executor 4, partition 15, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 11.0 in stage 44.0 (TID 194) in 10 ms on 100.67.4.240 (executor 4) (12/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 16.0 in stage 44.0 (TID 199) (100.67.4.240, executor 4, partition 16, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 12.0 in stage 44.0 (TID 195) in 11 ms on 100.67.4.240 (executor 4) (13/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 17.0 in stage 44.0 (TID 200) (100.67.4.240, executor 4, partition 17, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 13.0 in stage 44.0 (TID 196) in 11 ms on 100.67.4.240 (executor 4) (14/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 18.0 in stage 44.0 (TID 201) (100.67.4.240, executor 4, partition 18, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 14.0 in stage 44.0 (TID 197) in 12 ms on 100.67.4.240 (executor 4) (15/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 19.0 in stage 44.0 (TID 202) (100.67.4.240, executor 4, partition 19, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 15.0 in stage 44.0 (TID 198) in 12 ms on 100.67.4.240 (executor 4) (16/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 20.0 in stage 44.0 (TID 203) (100.67.4.240, executor 4, partition 20, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 16.0 in stage 44.0 (TID 199) in 11 ms on 100.67.4.240 (executor 4) (17/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 21.0 in stage 44.0 (TID 204) (100.67.4.240, executor 4, partition 21, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 17.0 in stage 44.0 (TID 200) in 10 ms on 100.67.4.240 (executor 4) (18/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 22.0 in stage 44.0 (TID 205) (100.67.4.240, executor 4, partition 22, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 18.0 in stage 44.0 (TID 201) in 11 ms on 100.67.4.240 (executor 4) (19/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 23.0 in stage 44.0 (TID 206) (100.67.4.240, executor 4, partition 23, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 19.0 in stage 44.0 (TID 202) in 10 ms on 100.67.4.240 (executor 4) (20/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 24.0 in stage 44.0 (TID 207) (100.67.4.240, executor 4, partition 24, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 20.0 in stage 44.0 (TID 203) in 13 ms on 100.67.4.240 (executor 4) (21/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 25.0 in stage 44.0 (TID 208) (100.67.4.240, executor 4, partition 25, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 23.0 in stage 44.0 (TID 206) in 12 ms on 100.67.4.240 (executor 4) (22/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 26.0 in stage 44.0 (TID 209) (100.67.4.240, executor 4, partition 26, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 21.0 in stage 44.0 (TID 204) in 13 ms on 100.67.4.240 (executor 4) (23/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 27.0 in stage 44.0 (TID 210) (100.67.4.240, executor 4, partition 27, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 22.0 in stage 44.0 (TID 205) in 13 ms on 100.67.4.240 (executor 4) (24/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 28.0 in stage 44.0 (TID 211) (100.67.4.240, executor 4, partition 28, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 26.0 in stage 44.0 (TID 209) in 10 ms on 100.67.4.240 (executor 4) (25/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 29.0 in stage 44.0 (TID 212) (100.67.4.240, executor 4, partition 29, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 24.0 in stage 44.0 (TID 207) in 13 ms on 100.67.4.240 (executor 4) (26/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 30.0 in stage 44.0 (TID 213) (100.67.4.240, executor 4, partition 30, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 25.0 in stage 44.0 (TID 208) in 13 ms on 100.67.4.240 (executor 4) (27/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 31.0 in stage 44.0 (TID 214) (100.67.4.240, executor 4, partition 31, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 27.0 in stage 44.0 (TID 210) in 13 ms on 100.67.4.240 (executor 4) (28/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 32.0 in stage 44.0 (TID 215) (100.67.4.240, executor 4, partition 32, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 29.0 in stage 44.0 (TID 212) in 23 ms on 100.67.4.240 (executor 4) (29/50)\n",
       "7590.825: [GC (Allocation Failure) [PSYoungGen: 1014513K->33248K(1008640K)] 1104294K->123037K(4851200K), 0.0285554 secs] [Times: user=0.12 sys=0.00, real=0.03 secs] \n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 33.0 in stage 44.0 (TID 216) (100.67.4.240, executor 4, partition 33, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 31.0 in stage 44.0 (TID 214) in 54 ms on 100.67.4.240 (executor 4) (30/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 34.0 in stage 44.0 (TID 217) (100.67.4.240, executor 4, partition 34, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 30.0 in stage 44.0 (TID 213) in 59 ms on 100.67.4.240 (executor 4) (31/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 35.0 in stage 44.0 (TID 218) (100.67.4.240, executor 4, partition 35, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 28.0 in stage 44.0 (TID 211) in 64 ms on 100.67.4.240 (executor 4) (32/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 36.0 in stage 44.0 (TID 219) (100.67.4.240, executor 4, partition 36, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 32.0 in stage 44.0 (TID 215) in 47 ms on 100.67.4.240 (executor 4) (33/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 100.67.56.160:7079 in memory (size: 171.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 37.0 in stage 44.0 (TID 220) (100.67.4.240, executor 4, partition 37, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 33.0 in stage 44.0 (TID 216) in 48 ms on 100.67.4.240 (executor 4) (34/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 100.67.4.240:46241 in memory (size: 171.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 38.0 in stage 44.0 (TID 221) (100.67.4.240, executor 4, partition 38, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 34.0 in stage 44.0 (TID 217) in 13 ms on 100.67.4.240 (executor 4) (35/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 39.0 in stage 44.0 (TID 222) (100.67.4.240, executor 4, partition 39, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 40.0 in stage 44.0 (TID 223) (100.67.4.240, executor 4, partition 40, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 35.0 in stage 44.0 (TID 218) in 64 ms on 100.67.4.240 (executor 4) (36/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 100.67.56.160:7079 in memory (size: 119.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 36.0 in stage 44.0 (TID 219) in 62 ms on 100.67.4.240 (executor 4) (37/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 100.67.4.240:46241 in memory (size: 119.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 41.0 in stage 44.0 (TID 224) (100.67.4.240, executor 4, partition 41, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 37.0 in stage 44.0 (TID 220) in 56 ms on 100.67.4.240 (executor 4) (38/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 100.67.56.160:7079 in memory (size: 131.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 100.67.4.240:46241 in memory (size: 131.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 42.0 in stage 44.0 (TID 225) (100.67.4.240, executor 4, partition 42, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 100.67.56.160:7079 in memory (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 38.0 in stage 44.0 (TID 221) in 63 ms on 100.67.4.240 (executor 4) (39/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 100.67.4.240:46241 in memory (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 43.0 in stage 44.0 (TID 226) (100.67.4.240, executor 4, partition 43, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 39.0 in stage 44.0 (TID 222) in 13 ms on 100.67.4.240 (executor 4) (40/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 100.67.56.160:7079 in memory (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 44.0 in stage 44.0 (TID 227) (100.67.4.240, executor 4, partition 44, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 40.0 in stage 44.0 (TID 223) in 14 ms on 100.67.4.240 (executor 4) (41/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 100.67.4.240:46241 in memory (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 45.0 in stage 44.0 (TID 228) (100.67.4.240, executor 4, partition 45, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 41.0 in stage 44.0 (TID 224) in 16 ms on 100.67.4.240 (executor 4) (42/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 100.67.56.160:7079 in memory (size: 146.0 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 100.67.4.240:46241 in memory (size: 146.0 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:47:16,002Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d106d6cd-1ee6-4484-9754-e425ddb54bae/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 46.0 in stage 44.0 (TID 229) (100.67.4.240, executor 4, partition 46, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 42.0 in stage 44.0 (TID 225) in 14 ms on 100.67.4.240 (executor 4) (43/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 100.67.4.240:46241 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 47.0 in stage 44.0 (TID 230) (100.67.4.240, executor 4, partition 47, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 48.0 in stage 44.0 (TID 231) (100.67.4.240, executor 4, partition 48, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 43.0 in stage 44.0 (TID 226) in 18 ms on 100.67.4.240 (executor 4) (44/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 44.0 in stage 44.0 (TID 227) in 15 ms on 100.67.4.240 (executor 4) (45/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 100.67.56.160:7079 in memory (size: 13.5 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 100.67.4.240:46241 in memory (size: 13.5 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Starting task 49.0 in stage 44.0 (TID 232) (100.67.4.240, executor 4, partition 49, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 45.0 in stage 44.0 (TID 228) in 16 ms on 100.67.4.240 (executor 4) (46/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 100.67.56.160:7079 in memory (size: 13.3 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 100.67.4.240:46241 in memory (size: 13.3 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 100.67.56.160:7079 in memory (size: 172.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 47.0 in stage 44.0 (TID 230) in 12 ms on 100.67.4.240 (executor 4) (47/50)\n",
       "26/02/04 01:47:15 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 100.67.4.240:46241 in memory (size: 172.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 46.0 in stage 44.0 (TID 229) in 20 ms on 100.67.4.240 (executor 4) (48/50)\n",
       "26/02/04 01:47:15 INFO TaskSetManager: Finished task 49.0 in stage 44.0 (TID 232) in 12 ms on 100.67.4.240 (executor 4) (49/50)\n",
       "26/02/04 01:47:16 INFO TaskSetManager: Finished task 48.0 in stage 44.0 (TID 231) in 69 ms on 100.67.4.240 (executor 4) (50/50)\n",
       "26/02/04 01:47:16 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:16 INFO DAGScheduler: ResultStage 44 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.398 s\n",
       "26/02/04 01:47:16 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:47:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished\n",
       "26/02/04 01:47:16 INFO SparkSQLEngineListener: Job end. Job 36 state is JobSucceeded\n",
       "26/02/04 01:47:16 INFO DAGScheduler: Job 36 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 0.400776 s\n",
       "26/02/04 01:47:16 INFO GpuOptimisticTransaction: [tableId=ccb95324,txnId=e3cf5f8b] Attempting to commit version 1 with 3 actions with Serializable isolation level\n",
       "26/02/04 01:47:16 INFO GpuOptimisticTransaction: Incremental commit: starting with snapshot version 0\n",
       "26/02/04 01:47:16 INFO DeltaLog: Creating a new snapshot v1 for commit version 1\n",
       "26/02/04 01:47:16 INFO DeltaLog: Loading version 1.\n",
       "26/02/04 01:47:16 INFO Snapshot: [tableId=ccb95324-b4a4-48e2-a197-060894e9e731] Created snapshot Snapshot(path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log, version=1, metadata=Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)), logSegment=LogSegment(s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log,1,List(S3AFileStatus{path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000000.json; isDirectory=false; length=2216; replication=1; blocksize=67108864; modification_time=1770169421000; access_time=0; owner=spring; group=spring; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=\"1a1dd94afc5dcab27160a6d56fa240d3\" versionId=null, S3AFileStatus{path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000001.json; isDirectory=false; length=1511; replication=1; blocksize=67108864; modification_time=1770169637000; access_time=0; owner=spring; group=spring; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=\"ab89919b7cd26a0dd5de570ccaee0522\" versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,1770169637000), checksumOpt=Some(VersionChecksum(Some(e3cf5f8b-4019-42b4-b3fb-fde164c0a2e4),2175,1,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00000-2b4baf88-b103-4967-a27b-67948c58e72f-c000.snappy.parquet,Map(),2175,1770169636000,false,{\"numRecords\":10,\"minValues\":{\"node_id\":\"cpu-dm-001\",\"scontrol_state\":\"[\\\\\"IDLE\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"maxValues\":{\"node_id\":\"cpu-dm-010\",\"scontrol_state\":\"[\\\\\"MIXED\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"nullCount\":{\"node_id\":0,\"scontrol_state\":0,\"reason\":0,\"updated_at\":0,\"stale\":0,\"reason_changed_at\":0,\"cluster_id\":0,\"last_busy_at\":0}},null,null,None,None,None))))))\n",
       "26/02/04 01:47:16 INFO DeltaLog: Updated snapshot to Snapshot(path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log, version=1, metadata=Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)), logSegment=LogSegment(s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log,1,List(S3AFileStatus{path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000000.json; isDirectory=false; length=2216; replication=1; blocksize=67108864; modification_time=1770169421000; access_time=0; owner=spring; group=spring; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=\"1a1dd94afc5dcab27160a6d56fa240d3\" versionId=null, S3AFileStatus{path=s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000001.json; isDirectory=false; length=1511; replication=1; blocksize=67108864; modification_time=1770169637000; access_time=0; owner=spring; group=spring; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=\"ab89919b7cd26a0dd5de570ccaee0522\" versionId=null),org.apache.spark.sql.delta.EmptyCheckpointProvider\\$@12129912,1770169637000), checksumOpt=Some(VersionChecksum(Some(e3cf5f8b-4019-42b4-b3fb-fde164c0a2e4),2175,1,None,None,1,1,None,Some(Stream()),Some(Stream()),Metadata(ccb95324-b4a4-48e2-a197-060894e9e731,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"node_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"scontrol_state\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"updated_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"stale\",\"type\":\"boolean\",\"nullable\":true,\"metadata\":{}},{\"name\":\"reason_changed_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"cluster_id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"last_busy_at\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1770169413741)),Protocol(1,2),None,None,Some(Stream(AddFile(part-00000-2b4baf88-b103-4967-a27b-67948c58e72f-c000.snappy.parquet,Map(),2175,1770169636000,false,{\"numRecords\":10,\"minValues\":{\"node_id\":\"cpu-dm-001\",\"scontrol_state\":\"[\\\\\"IDLE\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"maxValues\":{\"node_id\":\"cpu-dm-010\",\"scontrol_state\":\"[\\\\\"MIXED\\\\\"]\",\"reason\":\"\",\"updated_at\":\"2026-02-04T01:00:06.201168+00:00\",\"reason_changed_at\":\"1970-01-01T00:00:00+00:00\",\"cluster_id\":\"gcp-iad-cs-001-v1\",\"last_busy_at\":\"2026-02-03T23:41:25+00:00\"},\"nullCount\":{\"node_id\":0,\"scontrol_state\":0,\"reason\":0,\"updated_at\":0,\"stale\":0,\"reason_changed_at\":0,\"cluster_id\":0,\"last_busy_at\":0}},null,null,None,None,None))))))\n",
       "26/02/04 01:47:16 INFO MapPartitionsRDD: Removing RDD 199 from persistence list\n",
       "26/02/04 01:47:16 INFO BlockManager: Removing RDD 199\n",
       "26/02/04 01:47:16 INFO MapPartitionsRDD: Removing RDD 209 from persistence list\n",
       "26/02/04 01:47:16 INFO BlockManager: Removing RDD 209\n",
       "26/02/04 01:47:16 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 2, totalFileSize: 3727)\n",
       "26/02/04 01:47:16 INFO GpuOptimisticTransaction: [tableId=ccb95324,txnId=e3cf5f8b] Committed delta #1 to s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log\n",
       "26/02/04 01:47:16 INFO ChecksumHook: Writing checksum file for table path s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log version 1\n",
       "26/02/04 01:47:16 WARN CheckpointFileManager: Could not use FileContext API for managing Structured Streaming checkpoint files at s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log. Using FileSystem API instead for managing log files. If the implementation of FileSystem.rename() is not atomic, then the correctness and fault-tolerance ofyour Structured Streaming is not guaranteed.\n",
       "26/02/04 01:47:16 INFO CheckpointFileManager: Writing atomically to s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000001.crc using temp file s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/.00000000000000000001.crc.4d9fc3f0-f68f-49d1-bad4-9f62a5897d8a.tmp\n",
       "26/02/04 01:47:16 INFO CheckpointFileManager: Renamed temp file s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/.00000000000000000001.crc.4d9fc3f0-f68f-49d1-bad4-9f62a5897d8a.tmp to s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes/_delta_log/00000000000000000001.crc\n",
       "26/02/04 01:47:16 INFO ExecutePython:  Delta table saved to: s3://dcartm-team/hongy/delta/GCP_EAST4_maestro_slurm_nodes\n",
       "26/02/04 01:47:16 INFO ExecutePython: root\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- node_id: string (nullable = true)\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- scontrol_state: string (nullable = true)\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- reason: string (nullable = true)\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- updated_at: string (nullable = true)\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- stale: boolean (nullable = true)\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- reason_changed_at: string (nullable = true)\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- cluster_id: string (nullable = true)\n",
       "26/02/04 01:47:16 INFO ExecutePython:  |-- last_busy_at: string (nullable = true)\n",
       "26/02/04 01:47:16 INFO PrepareDeltaScan: DELTA: Filtering files for query\n",
       "26/02/04 01:47:16 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 472.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:16 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:16 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 100.67.56.160:7079 (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:16 INFO SparkContext: Created broadcast 50 from \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "2026-02-04T01:47:17,100Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T01:47:17,144Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d106d6cd-1ee6-4484-9754-e425ddb54bae/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:47:16 INFO DataSourceStrategy: Pruning directories with: \n",
       "26/02/04 01:47:16 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:47:16 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 41.62 ms\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 11.61 ms\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 0.22 ms\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 13.67 ms\n",
       "26/02/04 01:47:17 INFO GpuOverrides: GPU plan transition optimization took 0.16 ms\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 2.70 ms\n",
       "26/02/04 01:47:17 INFO GpuOverrides: GPU plan transition optimization took 0.42 ms\n",
       "26/02/04 01:47:17 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 467.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 100.67.56.160:7079 (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO SparkContext: Created broadcast 51 from \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:47:17 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Registering RDD 244 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) as input to shuffle 6\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Got map stage job 37 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 2 output partitions\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Final stage: ShuffleMapStage 45 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[244] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:47:17 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 106.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 100.67.56.160:7079 (size: 32.7 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[244] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))\n",
       "26/02/04 01:47:17 INFO TaskSchedulerImpl: Adding task set 45.0 with 2 tasks resource profile 0\n",
       "26/02/04 01:47:17 INFO FairSchedulableBuilder: Added task set TaskSet_45.0 tasks to pool \n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 233) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9890 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 1.0 in stage 45.0 (TID 234) (100.67.4.240, executor 4, partition 1, PROCESS_LOCAL, 9890 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 100.67.4.240:46241 (size: 32.7 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 100.67.4.240:46241 (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 233) in 92 ms on 100.67.4.240 (executor 4) (1/2)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 1.0 in stage 45.0 (TID 234) in 105 ms on 100.67.4.240 (executor 4) (2/2)\n",
       "26/02/04 01:47:17 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:17 INFO DAGScheduler: ShuffleMapStage 45 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.113 s\n",
       "26/02/04 01:47:17 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/04 01:47:17 INFO DAGScheduler: running: Set()\n",
       "26/02/04 01:47:17 INFO DAGScheduler: waiting: Set()\n",
       "26/02/04 01:47:17 INFO DAGScheduler: failed: Set()\n",
       "26/02/04 01:47:17 INFO SparkSQLEngineListener: Job end. Job 37 state is JobSucceeded\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 7.27 ms\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 6.79 ms\n",
       "26/02/04 01:47:17 INFO GpuOverrides: GPU plan transition optimization took 1.04 ms\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 1.71 ms\n",
       "26/02/04 01:47:17 INFO GpuOverrides: GPU plan transition optimization took 0.34 ms\n",
       "26/02/04 01:47:17 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:17 INFO GpuOverrides: Plan conversion to the GPU took 0.60 ms\n",
       "26/02/04 01:47:17 INFO GpuOverrides: GPU plan transition optimization took 0.23 ms\n",
       "26/02/04 01:47:17 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Got job 38 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 50 output partitions\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Final stage: ResultStage 47 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[257] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:47:17 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 728.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 172.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 100.67.56.160:7079 (size: 172.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:17 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:17 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 47 (MapPartitionsRDD[257] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
       "26/02/04 01:47:17 INFO TaskSchedulerImpl: Adding task set 47.0 with 50 tasks resource profile 0\n",
       "26/02/04 01:47:17 INFO FairSchedulableBuilder: Added task set TaskSet_47.0 tasks to pool \n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 40.0 in stage 47.0 (TID 235) (100.67.4.240, executor 4, partition 40, NODE_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 42.0 in stage 47.0 (TID 236) (100.67.4.240, executor 4, partition 42, NODE_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 48.0 in stage 47.0 (TID 237) (100.67.4.240, executor 4, partition 48, NODE_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 238) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 100.67.4.240:46241 (size: 172.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 100.67.4.240:45730\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_0 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_40 in memory on 100.67.4.240:46241 (size: 422.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_0 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_40 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 239) (100.67.4.240, executor 4, partition 1, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 238) in 63 ms on 100.67.4.240 (executor 4) (1/50)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 240) (100.67.4.240, executor 4, partition 2, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 40.0 in stage 47.0 (TID 235) in 66 ms on 100.67.4.240 (executor 4) (2/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_48 in memory on 100.67.4.240:46241 (size: 1078.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_42 in memory on 100.67.4.240:46241 (size: 1151.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_48 in memory on 100.67.4.240:46241 (size: 870.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_42 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 3.0 in stage 47.0 (TID 241) (100.67.4.240, executor 4, partition 3, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 42.0 in stage 47.0 (TID 236) in 79 ms on 100.67.4.240 (executor 4) (3/50)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 4.0 in stage 47.0 (TID 242) (100.67.4.240, executor 4, partition 4, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 48.0 in stage 47.0 (TID 237) in 80 ms on 100.67.4.240 (executor 4) (4/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_1 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_2 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_1 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_2 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 5.0 in stage 47.0 (TID 243) (100.67.4.240, executor 4, partition 5, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 239) in 47 ms on 100.67.4.240 (executor 4) (5/50)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 6.0 in stage 47.0 (TID 244) (100.67.4.240, executor 4, partition 6, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 240) in 86 ms on 100.67.4.240 (executor 4) (6/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_3 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_4 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_3 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 7.0 in stage 47.0 (TID 245) (100.67.4.240, executor 4, partition 7, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 3.0 in stage 47.0 (TID 241) in 91 ms on 100.67.4.240 (executor 4) (7/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_4 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 8.0 in stage 47.0 (TID 246) (100.67.4.240, executor 4, partition 8, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 4.0 in stage 47.0 (TID 242) in 93 ms on 100.67.4.240 (executor 4) (8/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_5 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_6 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_6 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_5 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 9.0 in stage 47.0 (TID 247) (100.67.4.240, executor 4, partition 9, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 6.0 in stage 47.0 (TID 244) in 47 ms on 100.67.4.240 (executor 4) (9/50)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 10.0 in stage 47.0 (TID 248) (100.67.4.240, executor 4, partition 10, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 5.0 in stage 47.0 (TID 243) in 88 ms on 100.67.4.240 (executor 4) (10/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_7 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "2026-02-04T01:47:18,291Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d106d6cd-1ee6-4484-9754-e425ddb54bae/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_8 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_7 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 11.0 in stage 47.0 (TID 249) (100.67.4.240, executor 4, partition 11, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 7.0 in stage 47.0 (TID 245) in 85 ms on 100.67.4.240 (executor 4) (11/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_8 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 12.0 in stage 47.0 (TID 250) (100.67.4.240, executor 4, partition 12, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 8.0 in stage 47.0 (TID 246) in 86 ms on 100.67.4.240 (executor 4) (12/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_9 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_255_9 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Starting task 13.0 in stage 47.0 (TID 251) (100.67.4.240, executor 4, partition 13, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:17 INFO TaskSetManager: Finished task 9.0 in stage 47.0 (TID 247) in 75 ms on 100.67.4.240 (executor 4) (13/50)\n",
       "26/02/04 01:47:17 INFO BlockManagerInfo: Added rdd_251_10 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_10 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 14.0 in stage 47.0 (TID 252) (100.67.4.240, executor 4, partition 14, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 10.0 in stage 47.0 (TID 248) in 93 ms on 100.67.4.240 (executor 4) (14/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_11 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_12 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_11 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_12 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 15.0 in stage 47.0 (TID 253) (100.67.4.240, executor 4, partition 15, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 11.0 in stage 47.0 (TID 249) in 49 ms on 100.67.4.240 (executor 4) (15/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 16.0 in stage 47.0 (TID 254) (100.67.4.240, executor 4, partition 16, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 12.0 in stage 47.0 (TID 250) in 48 ms on 100.67.4.240 (executor 4) (16/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_13 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_13 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 17.0 in stage 47.0 (TID 255) (100.67.4.240, executor 4, partition 17, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 13.0 in stage 47.0 (TID 251) in 43 ms on 100.67.4.240 (executor 4) (17/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_14 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_14 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 18.0 in stage 47.0 (TID 256) (100.67.4.240, executor 4, partition 18, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 14.0 in stage 47.0 (TID 252) in 66 ms on 100.67.4.240 (executor 4) (18/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_15 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_15 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 19.0 in stage 47.0 (TID 257) (100.67.4.240, executor 4, partition 19, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 15.0 in stage 47.0 (TID 253) in 66 ms on 100.67.4.240 (executor 4) (19/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_17 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_17 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_16 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 20.0 in stage 47.0 (TID 258) (100.67.4.240, executor 4, partition 20, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 17.0 in stage 47.0 (TID 255) in 76 ms on 100.67.4.240 (executor 4) (20/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_16 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_18 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 21.0 in stage 47.0 (TID 259) (100.67.4.240, executor 4, partition 21, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 16.0 in stage 47.0 (TID 254) in 88 ms on 100.67.4.240 (executor 4) (21/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_18 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 22.0 in stage 47.0 (TID 260) (100.67.4.240, executor 4, partition 22, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 18.0 in stage 47.0 (TID 256) in 44 ms on 100.67.4.240 (executor 4) (22/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_19 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_21 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_19 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 23.0 in stage 47.0 (TID 261) (100.67.4.240, executor 4, partition 23, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 19.0 in stage 47.0 (TID 257) in 62 ms on 100.67.4.240 (executor 4) (23/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_21 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_22 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 24.0 in stage 47.0 (TID 262) (100.67.4.240, executor 4, partition 24, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 21.0 in stage 47.0 (TID 259) in 59 ms on 100.67.4.240 (executor 4) (24/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_22 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 25.0 in stage 47.0 (TID 263) (100.67.4.240, executor 4, partition 25, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 22.0 in stage 47.0 (TID 260) in 59 ms on 100.67.4.240 (executor 4) (25/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_20 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_20 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 26.0 in stage 47.0 (TID 264) (100.67.4.240, executor 4, partition 26, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 20.0 in stage 47.0 (TID 258) in 84 ms on 100.67.4.240 (executor 4) (26/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_24 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_24 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_25 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 27.0 in stage 47.0 (TID 265) (100.67.4.240, executor 4, partition 27, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 24.0 in stage 47.0 (TID 262) in 43 ms on 100.67.4.240 (executor 4) (27/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_25 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 28.0 in stage 47.0 (TID 266) (100.67.4.240, executor 4, partition 28, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 25.0 in stage 47.0 (TID 263) in 44 ms on 100.67.4.240 (executor 4) (28/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_23 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_23 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 29.0 in stage 47.0 (TID 267) (100.67.4.240, executor 4, partition 29, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 23.0 in stage 47.0 (TID 261) in 80 ms on 100.67.4.240 (executor 4) (29/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_26 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_26 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_27 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_28 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 30.0 in stage 47.0 (TID 268) (100.67.4.240, executor 4, partition 30, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 26.0 in stage 47.0 (TID 264) in 86 ms on 100.67.4.240 (executor 4) (30/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_27 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 31.0 in stage 47.0 (TID 269) (100.67.4.240, executor 4, partition 31, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_28 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 27.0 in stage 47.0 (TID 265) in 67 ms on 100.67.4.240 (executor 4) (31/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 32.0 in stage 47.0 (TID 270) (100.67.4.240, executor 4, partition 32, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 28.0 in stage 47.0 (TID 266) in 62 ms on 100.67.4.240 (executor 4) (32/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_29 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_29 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 33.0 in stage 47.0 (TID 271) (100.67.4.240, executor 4, partition 33, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 29.0 in stage 47.0 (TID 267) in 72 ms on 100.67.4.240 (executor 4) (33/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_30 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_31 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_30 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_31 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 34.0 in stage 47.0 (TID 272) (100.67.4.240, executor 4, partition 34, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 30.0 in stage 47.0 (TID 268) in 48 ms on 100.67.4.240 (executor 4) (34/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 35.0 in stage 47.0 (TID 273) (100.67.4.240, executor 4, partition 35, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 31.0 in stage 47.0 (TID 269) in 44 ms on 100.67.4.240 (executor 4) (35/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_32 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_32 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 36.0 in stage 47.0 (TID 274) (100.67.4.240, executor 4, partition 36, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 32.0 in stage 47.0 (TID 270) in 94 ms on 100.67.4.240 (executor 4) (36/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_33 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_33 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 37.0 in stage 47.0 (TID 275) (100.67.4.240, executor 4, partition 37, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 33.0 in stage 47.0 (TID 271) in 88 ms on 100.67.4.240 (executor 4) (37/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_36 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_36 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 38.0 in stage 47.0 (TID 276) (100.67.4.240, executor 4, partition 38, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 36.0 in stage 47.0 (TID 274) in 45 ms on 100.67.4.240 (executor 4) (38/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_35 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_34 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_35 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_34 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 39.0 in stage 47.0 (TID 277) (100.67.4.240, executor 4, partition 39, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 35.0 in stage 47.0 (TID 273) in 107 ms on 100.67.4.240 (executor 4) (39/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 41.0 in stage 47.0 (TID 278) (100.67.4.240, executor 4, partition 41, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 34.0 in stage 47.0 (TID 272) in 109 ms on 100.67.4.240 (executor 4) (40/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_37 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_37 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 43.0 in stage 47.0 (TID 279) (100.67.4.240, executor 4, partition 43, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 37.0 in stage 47.0 (TID 275) in 90 ms on 100.67.4.240 (executor 4) (41/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_38 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_38 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 44.0 in stage 47.0 (TID 280) (100.67.4.240, executor 4, partition 44, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 38.0 in stage 47.0 (TID 276) in 81 ms on 100.67.4.240 (executor 4) (42/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_41 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_41 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 45.0 in stage 47.0 (TID 281) (100.67.4.240, executor 4, partition 45, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 41.0 in stage 47.0 (TID 278) in 83 ms on 100.67.4.240 (executor 4) (43/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_39 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_39 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 46.0 in stage 47.0 (TID 282) (100.67.4.240, executor 4, partition 46, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 39.0 in stage 47.0 (TID 277) in 96 ms on 100.67.4.240 (executor 4) (44/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_43 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_43 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 47.0 in stage 47.0 (TID 283) (100.67.4.240, executor 4, partition 47, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 43.0 in stage 47.0 (TID 279) in 94 ms on 100.67.4.240 (executor 4) (45/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_44 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_44 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 49.0 in stage 47.0 (TID 284) (100.67.4.240, executor 4, partition 49, PROCESS_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 44.0 in stage 47.0 (TID 280) in 94 ms on 100.67.4.240 (executor 4) (46/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_45 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_45 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 45.0 in stage 47.0 (TID 281) in 102 ms on 100.67.4.240 (executor 4) (47/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_46 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_46 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_47 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 46.0 in stage 47.0 (TID 282) in 98 ms on 100.67.4.240 (executor 4) (48/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_47 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 47.0 in stage 47.0 (TID 283) in 61 ms on 100.67.4.240 (executor 4) (49/50)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_251_49 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added rdd_255_49 in memory on 100.67.4.240:46241 (size: 4.0 B, free: 9.0 GiB)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 49.0 in stage 47.0 (TID 284) in 60 ms on 100.67.4.240 (executor 4) (50/50)\n",
       "26/02/04 01:47:18 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:18 INFO DAGScheduler: ResultStage 47 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.946 s\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:47:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished\n",
       "26/02/04 01:47:18 INFO SparkSQLEngineListener: Job end. Job 38 state is JobSucceeded\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Job 38 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 0.950423 s\n",
       "26/02/04 01:47:18 INFO Snapshot: DELTA: Compute snapshot for version: 1\n",
       "26/02/04 01:47:18 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:18 INFO GpuOverrides: Plan conversion to the GPU took 2.16 ms\n",
       "26/02/04 01:47:18 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:18 INFO GpuOverrides: Plan conversion to the GPU took 2.29 ms\n",
       "26/02/04 01:47:18 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:18 INFO GpuOverrides: Plan conversion to the GPU took 0.16 ms\n",
       "26/02/04 01:47:18 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:18 INFO GpuOverrides: Plan conversion to the GPU took 3.89 ms\n",
       "26/02/04 01:47:18 INFO GpuOverrides: GPU plan transition optimization took 0.13 ms\n",
       "26/02/04 01:47:18 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:18 INFO GpuOverrides: Plan conversion to the GPU took 1.33 ms\n",
       "26/02/04 01:47:18 INFO GpuOverrides: GPU plan transition optimization took 0.34 ms\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Registering RDD 260 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) as input to shuffle 7\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Got map stage job 39 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 50 output partitions\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Final stage: ShuffleMapStage 49 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[260] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:47:18 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 615.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:18 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 146.1 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 100.67.56.160:7079 (size: 146.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:18 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:18 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[260] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
       "26/02/04 01:47:18 INFO TaskSchedulerImpl: Adding task set 49.0 with 50 tasks resource profile 0\n",
       "26/02/04 01:47:18 INFO FairSchedulableBuilder: Added task set TaskSet_49.0 tasks to pool \n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 285) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 286) (100.67.4.240, executor 4, partition 1, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 2.0 in stage 49.0 (TID 287) (100.67.4.240, executor 4, partition 2, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 3.0 in stage 49.0 (TID 288) (100.67.4.240, executor 4, partition 3, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 100.67.4.240:46241 (size: 146.1 KiB, free: 9.0 GiB)\n",
       "2026-02-04T01:47:19,377Z INFO ExecuteStatement: Query[d106d6cd-1ee6-4484-9754-e425ddb54bae] in RUNNING_STATE\n",
       "2026-02-04T01:47:19,442Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d106d6cd-1ee6-4484-9754-e425ddb54bae/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 4.0 in stage 49.0 (TID 289) (100.67.4.240, executor 4, partition 4, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 2.0 in stage 49.0 (TID 287) in 38 ms on 100.67.4.240 (executor 4) (1/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 5.0 in stage 49.0 (TID 290) (100.67.4.240, executor 4, partition 5, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 285) in 39 ms on 100.67.4.240 (executor 4) (2/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 6.0 in stage 49.0 (TID 291) (100.67.4.240, executor 4, partition 6, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 286) in 47 ms on 100.67.4.240 (executor 4) (3/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 7.0 in stage 49.0 (TID 292) (100.67.4.240, executor 4, partition 7, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 3.0 in stage 49.0 (TID 288) in 48 ms on 100.67.4.240 (executor 4) (4/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 8.0 in stage 49.0 (TID 293) (100.67.4.240, executor 4, partition 8, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 5.0 in stage 49.0 (TID 290) in 30 ms on 100.67.4.240 (executor 4) (5/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 9.0 in stage 49.0 (TID 294) (100.67.4.240, executor 4, partition 9, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 4.0 in stage 49.0 (TID 289) in 35 ms on 100.67.4.240 (executor 4) (6/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 10.0 in stage 49.0 (TID 295) (100.67.4.240, executor 4, partition 10, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 7.0 in stage 49.0 (TID 292) in 26 ms on 100.67.4.240 (executor 4) (7/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 11.0 in stage 49.0 (TID 296) (100.67.4.240, executor 4, partition 11, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 6.0 in stage 49.0 (TID 291) in 31 ms on 100.67.4.240 (executor 4) (8/50)\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Starting task 12.0 in stage 49.0 (TID 297) (100.67.4.240, executor 4, partition 12, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:18 INFO TaskSetManager: Finished task 8.0 in stage 49.0 (TID 293) in 29 ms on 100.67.4.240 (executor 4) (9/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 13.0 in stage 49.0 (TID 298) (100.67.4.240, executor 4, partition 13, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 10.0 in stage 49.0 (TID 295) in 27 ms on 100.67.4.240 (executor 4) (10/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 14.0 in stage 49.0 (TID 299) (100.67.4.240, executor 4, partition 14, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 9.0 in stage 49.0 (TID 294) in 32 ms on 100.67.4.240 (executor 4) (11/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 15.0 in stage 49.0 (TID 300) (100.67.4.240, executor 4, partition 15, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 11.0 in stage 49.0 (TID 296) in 26 ms on 100.67.4.240 (executor 4) (12/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 16.0 in stage 49.0 (TID 301) (100.67.4.240, executor 4, partition 16, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 12.0 in stage 49.0 (TID 297) in 28 ms on 100.67.4.240 (executor 4) (13/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 17.0 in stage 49.0 (TID 302) (100.67.4.240, executor 4, partition 17, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 14.0 in stage 49.0 (TID 299) in 27 ms on 100.67.4.240 (executor 4) (14/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 18.0 in stage 49.0 (TID 303) (100.67.4.240, executor 4, partition 18, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 13.0 in stage 49.0 (TID 298) in 30 ms on 100.67.4.240 (executor 4) (15/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 19.0 in stage 49.0 (TID 304) (100.67.4.240, executor 4, partition 19, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 15.0 in stage 49.0 (TID 300) in 60 ms on 100.67.4.240 (executor 4) (16/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 20.0 in stage 49.0 (TID 305) (100.67.4.240, executor 4, partition 20, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 17.0 in stage 49.0 (TID 302) in 49 ms on 100.67.4.240 (executor 4) (17/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 21.0 in stage 49.0 (TID 306) (100.67.4.240, executor 4, partition 21, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 18.0 in stage 49.0 (TID 303) in 49 ms on 100.67.4.240 (executor 4) (18/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 22.0 in stage 49.0 (TID 307) (100.67.4.240, executor 4, partition 22, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 16.0 in stage 49.0 (TID 301) in 59 ms on 100.67.4.240 (executor 4) (19/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 23.0 in stage 49.0 (TID 308) (100.67.4.240, executor 4, partition 23, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 19.0 in stage 49.0 (TID 304) in 23 ms on 100.67.4.240 (executor 4) (20/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 24.0 in stage 49.0 (TID 309) (100.67.4.240, executor 4, partition 24, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 20.0 in stage 49.0 (TID 305) in 28 ms on 100.67.4.240 (executor 4) (21/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 25.0 in stage 49.0 (TID 310) (100.67.4.240, executor 4, partition 25, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 22.0 in stage 49.0 (TID 307) in 27 ms on 100.67.4.240 (executor 4) (22/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 26.0 in stage 49.0 (TID 311) (100.67.4.240, executor 4, partition 26, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 21.0 in stage 49.0 (TID 306) in 35 ms on 100.67.4.240 (executor 4) (23/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 27.0 in stage 49.0 (TID 312) (100.67.4.240, executor 4, partition 27, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 23.0 in stage 49.0 (TID 308) in 35 ms on 100.67.4.240 (executor 4) (24/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 28.0 in stage 49.0 (TID 313) (100.67.4.240, executor 4, partition 28, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 24.0 in stage 49.0 (TID 309) in 63 ms on 100.67.4.240 (executor 4) (25/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 29.0 in stage 49.0 (TID 314) (100.67.4.240, executor 4, partition 29, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 25.0 in stage 49.0 (TID 310) in 66 ms on 100.67.4.240 (executor 4) (26/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 30.0 in stage 49.0 (TID 315) (100.67.4.240, executor 4, partition 30, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 26.0 in stage 49.0 (TID 311) in 71 ms on 100.67.4.240 (executor 4) (27/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 31.0 in stage 49.0 (TID 316) (100.67.4.240, executor 4, partition 31, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 27.0 in stage 49.0 (TID 312) in 70 ms on 100.67.4.240 (executor 4) (28/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 32.0 in stage 49.0 (TID 317) (100.67.4.240, executor 4, partition 32, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 28.0 in stage 49.0 (TID 313) in 28 ms on 100.67.4.240 (executor 4) (29/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 33.0 in stage 49.0 (TID 318) (100.67.4.240, executor 4, partition 33, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 30.0 in stage 49.0 (TID 315) in 27 ms on 100.67.4.240 (executor 4) (30/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 34.0 in stage 49.0 (TID 319) (100.67.4.240, executor 4, partition 34, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 29.0 in stage 49.0 (TID 314) in 40 ms on 100.67.4.240 (executor 4) (31/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 35.0 in stage 49.0 (TID 320) (100.67.4.240, executor 4, partition 35, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 31.0 in stage 49.0 (TID 316) in 30 ms on 100.67.4.240 (executor 4) (32/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 36.0 in stage 49.0 (TID 321) (100.67.4.240, executor 4, partition 36, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 32.0 in stage 49.0 (TID 317) in 72 ms on 100.67.4.240 (executor 4) (33/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 37.0 in stage 49.0 (TID 322) (100.67.4.240, executor 4, partition 37, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 33.0 in stage 49.0 (TID 318) in 68 ms on 100.67.4.240 (executor 4) (34/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 38.0 in stage 49.0 (TID 323) (100.67.4.240, executor 4, partition 38, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 35.0 in stage 49.0 (TID 320) in 62 ms on 100.67.4.240 (executor 4) (35/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 39.0 in stage 49.0 (TID 324) (100.67.4.240, executor 4, partition 39, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 34.0 in stage 49.0 (TID 319) in 79 ms on 100.67.4.240 (executor 4) (36/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 40.0 in stage 49.0 (TID 325) (100.67.4.240, executor 4, partition 40, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 36.0 in stage 49.0 (TID 321) in 27 ms on 100.67.4.240 (executor 4) (37/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 41.0 in stage 49.0 (TID 326) (100.67.4.240, executor 4, partition 41, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 37.0 in stage 49.0 (TID 322) in 27 ms on 100.67.4.240 (executor 4) (38/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 42.0 in stage 49.0 (TID 327) (100.67.4.240, executor 4, partition 42, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 38.0 in stage 49.0 (TID 323) in 28 ms on 100.67.4.240 (executor 4) (39/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 43.0 in stage 49.0 (TID 328) (100.67.4.240, executor 4, partition 43, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 39.0 in stage 49.0 (TID 324) in 29 ms on 100.67.4.240 (executor 4) (40/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 44.0 in stage 49.0 (TID 329) (100.67.4.240, executor 4, partition 44, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 41.0 in stage 49.0 (TID 326) in 66 ms on 100.67.4.240 (executor 4) (41/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 45.0 in stage 49.0 (TID 330) (100.67.4.240, executor 4, partition 45, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 40.0 in stage 49.0 (TID 325) in 76 ms on 100.67.4.240 (executor 4) (42/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 46.0 in stage 49.0 (TID 331) (100.67.4.240, executor 4, partition 46, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 42.0 in stage 49.0 (TID 327) in 62 ms on 100.67.4.240 (executor 4) (43/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 47.0 in stage 49.0 (TID 332) (100.67.4.240, executor 4, partition 47, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 43.0 in stage 49.0 (TID 328) in 67 ms on 100.67.4.240 (executor 4) (44/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 48.0 in stage 49.0 (TID 333) (100.67.4.240, executor 4, partition 48, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 46.0 in stage 49.0 (TID 331) in 26 ms on 100.67.4.240 (executor 4) (45/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 49.0 in stage 49.0 (TID 334) (100.67.4.240, executor 4, partition 49, PROCESS_LOCAL, 9218 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 45.0 in stage 49.0 (TID 330) in 27 ms on 100.67.4.240 (executor 4) (46/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 44.0 in stage 49.0 (TID 329) in 39 ms on 100.67.4.240 (executor 4) (47/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 48.0 in stage 49.0 (TID 333) in 68 ms on 100.67.4.240 (executor 4) (48/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 49.0 in stage 49.0 (TID 334) in 67 ms on 100.67.4.240 (executor 4) (49/50)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 47.0 in stage 49.0 (TID 332) in 77 ms on 100.67.4.240 (executor 4) (50/50)\n",
       "26/02/04 01:47:19 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:19 INFO DAGScheduler: ShuffleMapStage 49 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.637 s\n",
       "26/02/04 01:47:19 INFO DAGScheduler: looking for newly runnable stages\n",
       "26/02/04 01:47:19 INFO DAGScheduler: running: Set()\n",
       "26/02/04 01:47:19 INFO DAGScheduler: waiting: Set()\n",
       "26/02/04 01:47:19 INFO DAGScheduler: failed: Set()\n",
       "26/02/04 01:47:19 INFO SparkSQLEngineListener: Job end. Job 39 state is JobSucceeded\n",
       "26/02/04 01:47:19 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:19 INFO GpuOverrides: Plan conversion to the GPU took 1.54 ms\n",
       "26/02/04 01:47:19 WARN GpuOverrides: Can't replace any part of this plan due to: Delta Lake metadata queries are not efficient on GPU\n",
       "26/02/04 01:47:19 INFO GpuOverrides: Plan conversion to the GPU took 1.36 ms\n",
       "26/02/04 01:47:19 INFO GpuOverrides: GPU plan transition optimization took 0.31 ms\n",
       "26/02/04 01:47:19 INFO SparkContext: Starting job: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Got job 40 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) with 1 output partitions\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Final stage: ResultStage 52 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128)\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 51)\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[263] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128), which has no missing parents\n",
       "26/02/04 01:47:19 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 547.0 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 131.2 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 100.67.56.160:7079 (size: 131.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[263] at \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:47:19 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:47:19 INFO FairSchedulableBuilder: Added task set TaskSet_52.0 tasks to pool \n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 335) (100.67.4.240, executor 4, partition 0, NODE_LOCAL, 9229 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 100.67.4.240:46241 (size: 131.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:19 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 100.67.4.240:45730\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 335) in 41 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:47:19 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:19 INFO DAGScheduler: ResultStage 52 (\\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128) finished in 0.050 s\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:47:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished\n",
       "26/02/04 01:47:19 INFO SparkSQLEngineListener: Job end. Job 40 state is JobSucceeded\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Job 40 finished: \\$anonfun\\$recordDeltaOperationInternal\\$1 at DatabricksLogging.scala:128, took 0.051872 s\n",
       "26/02/04 01:47:19 INFO Snapshot: DELTA: Done\n",
       "26/02/04 01:47:19 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 01:47:19 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 01:47:19 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 01:47:19 INFO GpuOverrides: Plan conversion to the GPU took 1.48 ms\n",
       "26/02/04 01:47:19 INFO GpuOverrides: GPU plan transition optimization took 0.81 ms\n",
       "26/02/04 01:47:19 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 01:47:19 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 472.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 48.4 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 100.67.56.160:7079 (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO SparkContext: Created broadcast 56 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 01:47:19 WARN GpuDelta33xParquetFileFormat: Coalescing is not supported when \\`delta.enableDeletionVectors=true\\`, using the multi-threaded reader. For more details on the Parquet reader types please look at 'spark.rapids.sql.format.parquet.reader.type' config at https://nvidia.github.io/spark-rapids/docs/additional-functionality/advanced_configs.html\n",
       "26/02/04 01:47:19 INFO SparkContext: Starting job: showString at <unknown>:0\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Got job 41 (showString at <unknown>:0) with 1 output partitions\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Final stage: ResultStage 53 (showString at <unknown>:0)\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[272] at showString at <unknown>:0), which has no missing parents\n",
       "26/02/04 01:47:19 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 28.5 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 100.67.56.160:7079 (size: 13.6 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:47:19 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[272] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 01:47:19 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0\n",
       "26/02/04 01:47:19 INFO FairSchedulableBuilder: Added task set TaskSet_53.0 tasks to pool \n",
       "26/02/04 01:47:19 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 336) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10187 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 01:47:19 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 100.67.4.240:46241 (size: 13.6 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:19 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 100.67.4.240:46241 (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:47:19 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 336) in 162 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 01:47:19 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
       "26/02/04 01:47:19 INFO DAGScheduler: ResultStage 53 (showString at <unknown>:0) finished in 0.167 s\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 01:47:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished\n",
       "26/02/04 01:47:19 INFO SparkSQLEngineListener: Job end. Job 41 state is JobSucceeded\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Job 41 finished: showString at <unknown>:0, took 0.168584 s\n",
       "26/02/04 01:47:19 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:47:19 INFO ExecutePython: |node_id   |scontrol_state|reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 01:47:19 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-001|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-002|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-003|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-004|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-005|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-006|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-007|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-008|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-009|[\"MIXED\"]     |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: |cpu-dm-010|[\"IDLE\"]      |      |2026-02-04T01:00:06.201168+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 01:47:19 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 01:47:19 INFO GpuOverrides: Plan conversion to the GPU took 0.16 ms\n",
       "26/02/04 01:47:19 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 01:47:19 INFO CreateDeltaTableCommand: Table is path-based table: false. Update catalog with mode: Create\n",
       "2026-02-04T01:47:19,991Z INFO ExecuteStatement: Query[d106d6cd-1ee6-4484-9754-e425ddb54bae] in ERROR_STATE\n",
       "2026-02-04T01:47:19,992Z INFO ExecuteStatement: Processing anonymous's query[d106d6cd-1ee6-4484-9754-e425ddb54bae]: RUNNING_STATE -> ERROR_STATE, time taken: 5.617 seconds\n",
       "2026-02-04T01:47:20,583Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d106d6cd-1ee6-4484-9754-e425ddb54bae/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T01:47:20,728Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/d106d6cd-1ee6-4484-9754-e425ddb54bae/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 01:47:19 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table \\`spark_catalog\\`.\\`dcartm\\`.\\`gcp_east4_maestro_slurm_nodes\\` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
       "26/02/04 01:47:19 INFO ExecutePython: Processing anonymous's query[d106d6cd-1ee6-4484-9754-e425ddb54bae]: RUNNING_STATE -> ERROR_STATE, time taken: 5.613 seconds\n",
       "26/02/04 01:47:19 INFO DAGScheduler: Asked to cancel job group d106d6cd-1ee6-4484-9754-e425ddb54bae\n",
       "26/02/04 01:47:20 INFO DAGScheduler: Asked to cancel job group d106d6cd-1ee6-4484-9754-e425ddb54bae\n",
       "26/02/04 01:47:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:47:47,100Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:47:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:48:17,100Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T01:48:17,100Z WARN KyuubiOperationManager: Operation OperationHandle [55e2cfe7-c6f3-437a-9624-23a976b9af3a] is timed-out and will be closed\n",
       "2026-02-04T01:48:17,104Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:55 E2 CF E7 C6 F3 43 7A 96 24 23 A9 76 B9 AF 3A, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 01:48:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:48:29 WARN SparkSQLOperationManager: Operation OperationHandle [d106d6cd-1ee6-4484-9754-e425ddb54bae] is timed-out and will be closed\n",
       "2026-02-04T01:48:47,104Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T01:48:47,104Z WARN KyuubiOperationManager: Operation OperationHandle [d106d6cd-1ee6-4484-9754-e425ddb54bae] is timed-out and will be closed\n",
       "2026-02-04T01:48:47,106Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:D1 06 D6 CD 1E E6 44 84 97 54 E4 25 DD B5 4B AE, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [d106d6cd-1ee6-4484-9754-e425ddb54bae]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:48:47 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [d106d6cd-1ee6-4484-9754-e425ddb54bae]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 01:48:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 01:49:15 INFO KubernetesClusterSchedulerBackend\\$KubernetesDriverEndpoint: No executor found for 100.67.4.240:42406\n",
       "2026-02-04T01:49:17,106Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:49:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:49:47,107Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:49:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:50:17,107Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:50:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:50:47,107Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:50:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:51:17,107Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:51:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:51:47,108Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:51:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:52:17,108Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:52:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:52:47,108Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:52:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:53:17,108Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "7963.484: [GC (Allocation Failure) [PSYoungGen: 1008608K->30991K(996864K)] 1098397K->120788K(4839424K), 0.0230649 secs] [Times: user=0.06 sys=0.00, real=0.02 secs] \n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 100.67.4.240:46241 in memory (size: 146.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 100.67.56.160:7079 in memory (size: 146.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 100.67.4.240:46241 in memory (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 100.67.56.160:7079 in memory (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 100.67.4.240:46241 in memory (size: 48.4 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 100.67.56.160:7079 in memory (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManager: Removing RDD 209\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 100.67.56.160:7079 in memory (size: 172.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 100.67.4.240:46241 in memory (size: 172.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 100.67.56.160:7079 in memory (size: 131.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 100.67.4.240:46241 in memory (size: 131.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 100.67.56.160:7079 in memory (size: 13.6 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 100.67.4.240:46241 in memory (size: 13.6 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 100.67.56.160:7079 in memory (size: 162.5 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 100.67.4.240:46241 in memory (size: 162.5 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 100.67.56.160:7079 in memory (size: 32.7 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 100.67.4.240:46241 in memory (size: 32.7 KiB, free: 9.0 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 100.67.56.160:7079 in memory (size: 48.4 KiB, free: 8.4 GiB)\n",
       "26/02/04 01:53:28 INFO BlockManager: Removing RDD 199\n",
       "26/02/04 01:53:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:53:47,109Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:53:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:54:17,109Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:54:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:54:47,109Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:54:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:55:17,109Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:55:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:55:47,109Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:55:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:56:17,110Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:56:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:56:47,110Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:56:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:57:17,110Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:57:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:57:47,110Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:57:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:58:17,111Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:58:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:58:47,111Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:58:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:59:17,111Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:59:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T01:59:47,111Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 01:59:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T02:00:17,112Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 02:00:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T02:00:47,112Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 02:00:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T02:01:17,112Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "26/02/04 02:01:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T02:01:47,112Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 2\n",
       "2026-02-04T02:01:55,214Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T02:01:55,214Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T02:01:55,215Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T02:01:55,215Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@5d7901e1\n",
       "2026-02-04T02:01:55,215Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T02:01:55,215Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T02:01:55,216Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T02:01:55,216Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:01:55,218Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-04T02:01:55,218Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T02:01:55,219Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:55586, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-04T02:01:55,223Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f520a0, negotiated timeout = 120000\n",
       "2026-02-04T02:01:55,223Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T02:01:55,224Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:01:55,225Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:01:55,225Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T02:01:55,226Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:01:55,330Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f520a0\n",
       "2026-02-04T02:01:55,330Z INFO ZooKeeper: Session: 0x200d89556f520a0 closed\n",
       "2026-02-04T02:01:55,333Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:01:55,582Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.216.117\n",
       "2026-02-04T02:01:55,582Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T02:01:55,583Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T02:01:55,583Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/98b0288e-266a-473e-b052-506b0b79addd\n",
       "2026-02-04T02:01:55,584Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6]/kernel-v329affbc318ea2a9554c48ecc04d62b2cd817da87 is opened, current opening sessions 3\n",
       "2026-02-04T02:01:55,584Z INFO LaunchEngine: Processing anonymous's query[98b0288e-266a-473e-b052-506b0b79addd]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:01:55,584Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:01:55,585Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T02:01:55,585Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@36e00d94\n",
       "2026-02-04T02:01:55,585Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T02:01:55,585Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T02:01:55,586Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T02:01:55,586Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-04T02:01:55,587Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T02:01:55,587Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:38948, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-04T02:01:55,593Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a490830b4, negotiated timeout = 120000\n",
       "2026-02-04T02:01:55,593Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T02:01:55,594Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:01:55,594Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:01:55,595Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:01:55,609Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6]]\n",
       "2026-02-04T02:01:55,610Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T02:01:55,714Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a490830b4\n",
       "2026-02-04T02:01:55,714Z INFO ZooKeeper: Session: 0x100f75a490830b4 closed\n",
       "2026-02-04T02:01:55,715Z INFO LaunchEngine: Processing anonymous's query[98b0288e-266a-473e-b052-506b0b79addd]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.131 seconds\n",
       "2026-02-04T02:01:55,768Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204020154-kxk923ye]: associated with Kyuubi SessionHandle: [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6]\n",
       "2026-02-04T02:01:55,769Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/94de39e2-66cb-48d1-a524-153a9d53ab62\n",
       "2026-02-04T02:01:55,769Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T02:01:55,769Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-04T02:01:55,774Z INFO ExecuteStatement: Processing anonymous's query[94de39e2-66cb-48d1-a524-153a9d53ab62]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:01:55,774Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204020154-kxk923ye\n",
       "2026-02-04T02:01:55,779Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/94de39e2-66cb-48d1-a524-153a9d53ab62/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:01:55 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 02:01:55 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 02:01:55 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6]\n",
       "26/02/04 02:01:55 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6]/kernel-v329affbc318ea2a9554c48ecc04d62b2cd817da87 is opened, current opening sessions 5\n",
       "26/02/04 02:01:55 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 02:01:55 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 02:01:55 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [4b022bae-fd66-45c7-9f41-e4f54e5d0519]\n",
       "26/02/04 02:01:55 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [4b022bae-fd66-45c7-9f41-e4f54e5d0519]/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6_aliveness_probe is opened, current opening sessions 6\n",
       "26/02/04 02:01:55 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204020154-kxk923ye)]: associated with spark-sql operation session: [6c7d3d26-3f57-4e41-a332-1ef6f4f671c6]\n",
       "26/02/04 02:01:55 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-2707fcac-1652-4a9b-aea8-e49b16a2f91d/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:2:7:0:7:f:c:a:c:-:1:6:5:2:-:4:a:9:b:-:a:e:a:8:-:e:4:9:b:1:6:a:2:f:9:1:d\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-2707fcac-1652-4a9b-aea8-e49b16a2f91d/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=6c7d3d26-3f57-4e41-a332-1ef6f4f671c6\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/04 02:01:55 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/94de39e2-66cb-48d1-a524-153a9d53ab62\n",
       "2026-02-04T02:01:56,262Z INFO ExecuteStatement: Query[94de39e2-66cb-48d1-a524-153a9d53ab62] in FINISHED_STATE\n",
       "2026-02-04T02:01:56,263Z INFO ExecuteStatement: Processing anonymous's query[94de39e2-66cb-48d1-a524-153a9d53ab62]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.489 seconds\n",
       "2026-02-04T02:01:56,784Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/94de39e2-66cb-48d1-a524-153a9d53ab62/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:01:56,793Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/94de39e2-66cb-48d1-a524-153a9d53ab62/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:01:56,940Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/3416723f-5955-45b0-bc64-bc784fc29d89\n",
       "2026-02-04T02:01:56,942Z INFO ExecuteStatement: Processing anonymous's query[3416723f-5955-45b0-bc64-bc784fc29d89]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:01:56,942Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:01:56,946Z INFO ExecuteStatement: Query[3416723f-5955-45b0-bc64-bc784fc29d89] in FINISHED_STATE\n",
       "2026-02-04T02:01:56,946Z INFO ExecuteStatement: Processing anonymous's query[3416723f-5955-45b0-bc64-bc784fc29d89]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "26/02/04 02:01:56 INFO ExecutePython: Processing anonymous's query[94de39e2-66cb-48d1-a524-153a9d53ab62]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:01:56 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:01:56 INFO ExecutePython: Processing anonymous's query[94de39e2-66cb-48d1-a524-153a9d53ab62]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.007 seconds\n",
       "26/02/04 02:01:56 INFO DAGScheduler: Asked to cancel job group 94de39e2-66cb-48d1-a524-153a9d53ab62\n",
       "26/02/04 02:01:56 INFO DAGScheduler: Asked to cancel job group 94de39e2-66cb-48d1-a524-153a9d53ab62\n",
       "26/02/04 02:01:56 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/3416723f-5955-45b0-bc64-bc784fc29d89\n",
       "26/02/04 02:01:56 INFO ExecutePython: Processing anonymous's query[3416723f-5955-45b0-bc64-bc784fc29d89]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:01:56 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:01:56 INFO ExecutePython: Processing anonymous's query[3416723f-5955-45b0-bc64-bc784fc29d89]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 02:01:56 INFO DAGScheduler: Asked to cancel job group 3416723f-5955-45b0-bc64-bc784fc29d89\n",
       "2026-02-04T02:01:57,464Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/dde1f4f9-acfc-47bb-8001-c70c8d3d8153\n",
       "2026-02-04T02:01:57,466Z INFO ExecuteStatement: Processing anonymous's query[dde1f4f9-acfc-47bb-8001-c70c8d3d8153]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:01:57,466Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:01:57,511Z INFO ExecuteStatement: Query[dde1f4f9-acfc-47bb-8001-c70c8d3d8153] in FINISHED_STATE\n",
       "2026-02-04T02:01:57,511Z INFO ExecuteStatement: Processing anonymous's query[dde1f4f9-acfc-47bb-8001-c70c8d3d8153]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.045 seconds\n",
       "2026-02-04T02:01:57,959Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/dde1f4f9-acfc-47bb-8001-c70c8d3d8153/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:01:57 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/dde1f4f9-acfc-47bb-8001-c70c8d3d8153\n",
       "26/02/04 02:01:57 INFO ExecutePython: Processing anonymous's query[dde1f4f9-acfc-47bb-8001-c70c8d3d8153]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:01:57 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:01:57 INFO ExecutePython: Processing anonymous's query[dde1f4f9-acfc-47bb-8001-c70c8d3d8153]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.041 seconds\n",
       "26/02/04 02:01:57 INFO DAGScheduler: Asked to cancel job group dde1f4f9-acfc-47bb-8001-c70c8d3d8153\n",
       "2026-02-04T02:01:58,099Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/dde1f4f9-acfc-47bb-8001-c70c8d3d8153/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:01:58 INFO DAGScheduler: Asked to cancel job group dde1f4f9-acfc-47bb-8001-c70c8d3d8153\n",
       "26/02/04 02:01:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 6\n",
       "2026-02-04T02:02:00,246Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/254b2837-9aa7-43b8-80d7-39bd97bd8841\n",
       "2026-02-04T02:02:00,248Z INFO ExecuteStatement: Processing anonymous's query[254b2837-9aa7-43b8-80d7-39bd97bd8841]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:00,248Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:00,252Z INFO ExecuteStatement: Query[254b2837-9aa7-43b8-80d7-39bd97bd8841] in FINISHED_STATE\n",
       "2026-02-04T02:02:00,252Z INFO ExecuteStatement: Processing anonymous's query[254b2837-9aa7-43b8-80d7-39bd97bd8841]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "2026-02-04T02:02:00,735Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/febb1cd3-5aef-47d7-b3d1-1005ad3c4adc\n",
       "2026-02-04T02:02:00,737Z INFO ExecuteStatement: Processing anonymous's query[febb1cd3-5aef-47d7-b3d1-1005ad3c4adc]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:00,737Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:02:00 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/254b2837-9aa7-43b8-80d7-39bd97bd8841\n",
       "26/02/04 02:02:00 INFO ExecutePython: Processing anonymous's query[254b2837-9aa7-43b8-80d7-39bd97bd8841]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:02:00 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:02:00 INFO ExecutePython: Processing anonymous's query[254b2837-9aa7-43b8-80d7-39bd97bd8841]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.0 seconds\n",
       "26/02/04 02:02:00 INFO DAGScheduler: Asked to cancel job group 254b2837-9aa7-43b8-80d7-39bd97bd8841\n",
       "26/02/04 02:02:00 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/6c7d3d26-3f57-4e41-a332-1ef6f4f671c6/febb1cd3-5aef-47d7-b3d1-1005ad3c4adc\n",
       "26/02/04 02:02:00 INFO ExecutePython: Processing anonymous's query[febb1cd3-5aef-47d7-b3d1-1005ad3c4adc]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:02:00 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:02:00 INFO GpuOverrides: Plan conversion to the GPU took 0.16 ms\n",
       "26/02/04 02:02:00 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "2026-02-04T02:02:01,212Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/febb1cd3-5aef-47d7-b3d1-1005ad3c4adc/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:02:01 INFO GpuOverrides: Plan conversion to the GPU took 0.36 ms\n",
       "26/02/04 02:02:01 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 02:02:01 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.\n",
       "26/02/04 02:02:01 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Got job 42 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Final stage: ResultStage 54 (sql at <unknown>:0)\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[274] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:02:01 INFO SQLOperationListener: Query [febb1cd3-5aef-47d7-b3d1-1005ad3c4adc]: Job 42 started with 1 stages, 1 active jobs running\n",
       "26/02/04 02:02:01 INFO SQLOperationListener: Query [febb1cd3-5aef-47d7-b3d1-1005ad3c4adc]: Stage 54.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 02:02:01 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:01 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:01 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:02:01 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[274] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:02:01 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:02:01 INFO FairSchedulableBuilder: Added task set TaskSet_54.0 tasks to pool \n",
       "26/02/04 02:02:01 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 337) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9376 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:02:01 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 100.67.4.240:46241 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:02:01 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 337) in 127 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:02:01 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:02:01 INFO DAGScheduler: ResultStage 54 (sql at <unknown>:0) finished in 0.139 s\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:02:01 INFO SQLOperationListener: Finished stage: Stage(54, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 139 msec\n",
       "26/02/04 02:02:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished\n",
       "26/02/04 02:02:01 INFO DAGScheduler: Job 42 finished: sql at <unknown>:0, took 0.142503 s\n",
       "26/02/04 02:02:01 INFO StatsReportListener: task runtime:(count: 1, mean: 127.000000, stdev: 0.000000, max: 127.000000, min: 127.000000)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t127.0 ms\t127.0 ms\t127.0 ms\t127.0 ms\t127.0 ms\t127.0 ms\t127.0 ms\t127.0 ms\t127.0 ms\n",
       "26/02/04 02:02:01 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:02:01 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 02:02:01 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:02:01 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 02:02:01 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 89.763780, stdev: 0.000000, max: 89.763780, min: 89.763780)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\t90 %\n",
       "26/02/04 02:02:01 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 02:02:01 INFO StatsReportListener: other time pct: (count: 1, mean: 10.236220, stdev: 0.000000, max: 10.236220, min: 10.236220)\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:01 INFO StatsReportListener: \t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\t10 %\n",
       "26/02/04 02:02:01 INFO SparkSQLEngineListener: Job end. Job 42 state is JobSucceeded\n",
       "26/02/04 02:02:01 INFO SQLOperationListener: Query [febb1cd3-5aef-47d7-b3d1-1005ad3c4adc]: Job 42 succeeded, 0 active jobs running\n",
       "26/02/04 02:02:01 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`oci_hsg_maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 02:02:01 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`oci_hsg_maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "2026-02-04T02:02:02,356Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/febb1cd3-5aef-47d7-b3d1-1005ad3c4adc/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:02,589Z INFO ExecuteStatement: Query[febb1cd3-5aef-47d7-b3d1-1005ad3c4adc] in FINISHED_STATE\n",
       "2026-02-04T02:02:02,589Z INFO ExecuteStatement: Processing anonymous's query[febb1cd3-5aef-47d7-b3d1-1005ad3c4adc]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.852 seconds\n",
       "26/02/04 02:02:02 INFO GpuOverrides: Plan conversion to the GPU took 0.25 ms\n",
       "26/02/04 02:02:02 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 02:02:02 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#2796 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#2797 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#2798 could run on GPU\n",
       "\n",
       "26/02/04 02:02:02 INFO GpuOverrides: Plan conversion to the GPU took 0.34 ms\n",
       "26/02/04 02:02:02 INFO GpuOverrides: GPU plan transition optimization took 0.14 ms\n",
       "26/02/04 02:02:02 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:02:02 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 02:02:02 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:02:02 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 02:02:02 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:02:02 INFO InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.\n",
       "26/02/04 02:02:02 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 02:02:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 02:02:02 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 02:02:02 INFO GpuOverrides: Plan conversion to the GPU took 1.71 ms\n",
       "26/02/04 02:02:02 INFO GpuOverrides: GPU plan transition optimization took 0.68 ms\n",
       "26/02/04 02:02:02 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 02:02:02 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:02 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:02 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:02:02 INFO SparkContext: Created broadcast 59 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 02:02:02 INFO SparkContext: Starting job: showString at <unknown>:0\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Got job 43 (showString at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Final stage: ResultStage 55 (showString at <unknown>:0)\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[283] at showString at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:02:02 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:02 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:02 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:02:02 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[283] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:02:02 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:02:02 INFO FairSchedulableBuilder: Added task set TaskSet_55.0 tasks to pool \n",
       "26/02/04 02:02:02 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 338) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10150 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:02:02 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:02:02 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:02:02 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 338) in 153 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:02:02 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:02:02 INFO DAGScheduler: ResultStage 55 (showString at <unknown>:0) finished in 0.159 s\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:02:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished\n",
       "26/02/04 02:02:02 INFO SparkSQLEngineListener: Job end. Job 43 state is JobSucceeded\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Job 43 finished: showString at <unknown>:0, took 0.160169 s\n",
       "26/02/04 02:02:02 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:02:02 INFO ExecutePython: |node_id     |scontrol_state                     |reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 02:02:02 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72001-T02|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:38:30+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72001-T09|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:55+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72001-T10|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:16+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72001-T12|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:49+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72001-T16|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:10+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72003-T14|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:58+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72003-T18|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:58:47+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72004-T05|[\"FUTURE\"]                         |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-01T00:33:21+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72007-T01|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:06+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: |nvl72007-T02|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:19+00:00|\n",
       "26/02/04 02:02:02 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:02:02 INFO ExecutePython: Processing anonymous's query[febb1cd3-5aef-47d7-b3d1-1005ad3c4adc]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.848 seconds\n",
       "26/02/04 02:02:02 INFO DAGScheduler: Asked to cancel job group febb1cd3-5aef-47d7-b3d1-1005ad3c4adc\n",
       "2026-02-04T02:02:03,539Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/febb1cd3-5aef-47d7-b3d1-1005ad3c4adc/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:03,696Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/febb1cd3-5aef-47d7-b3d1-1005ad3c4adc/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:02:03 INFO DAGScheduler: Asked to cancel job group febb1cd3-5aef-47d7-b3d1-1005ad3c4adc\n",
       "2026-02-04T02:02:17,113Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 3\n",
       "2026-02-04T02:02:23,424Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T02:02:23,424Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T02:02:23,425Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T02:02:23,425Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@8e26b3f\n",
       "2026-02-04T02:02:23,426Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T02:02:23,426Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T02:02:23,426Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T02:02:23,427Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:02:23,427Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-04T02:02:23,427Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T02:02:23,428Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:58328, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-04T02:02:23,432Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a49083228, negotiated timeout = 120000\n",
       "2026-02-04T02:02:23,433Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T02:02:23,434Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:02:23,434Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:02:23,436Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:02:23,437Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T02:02:23,542Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a49083228\n",
       "2026-02-04T02:02:23,542Z INFO ZooKeeper: Session: 0x100f75a49083228 closed\n",
       "2026-02-04T02:02:23,547Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:23,764Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.216.117\n",
       "2026-02-04T02:02:23,765Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T02:02:23,765Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T02:02:23,766Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/d76a95a0-5c8b-4633-a82d-8b6bb75a9c03\n",
       "2026-02-04T02:02:23,766Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [bb01be09-b85f-4827-88f2-28b71e7a7cc9]/kernel-v3d2b310a8d4a3e8e2e9b4f0e6bc2978bf478430d3 is opened, current opening sessions 4\n",
       "2026-02-04T02:02:23,766Z INFO LaunchEngine: Processing anonymous's query[d76a95a0-5c8b-4633-a82d-8b6bb75a9c03]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:23,766Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:23,767Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T02:02:23,767Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@2be96c5c\n",
       "2026-02-04T02:02:23,767Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T02:02:23,767Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T02:02:23,768Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181.\n",
       "2026-02-04T02:02:23,768Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T02:02:23,769Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T02:02:23,770Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:60744, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181\n",
       "2026-02-04T02:02:23,774Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.28.97:2181, session id = 0x200d89556f52215, negotiated timeout = 120000\n",
       "2026-02-04T02:02:23,775Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T02:02:23,776Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:02:23,776Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:02:23,777Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:02:23,788Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [bb01be09-b85f-4827-88f2-28b71e7a7cc9] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [bb01be09-b85f-4827-88f2-28b71e7a7cc9]]\n",
       "2026-02-04T02:02:23,788Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T02:02:23,893Z INFO ZooKeeper: Session: 0x200d89556f52215 closed\n",
       "2026-02-04T02:02:23,893Z INFO ClientCnxn: EventThread shut down for session: 0x200d89556f52215\n",
       "2026-02-04T02:02:23,893Z INFO LaunchEngine: Processing anonymous's query[d76a95a0-5c8b-4633-a82d-8b6bb75a9c03]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.127 seconds\n",
       "2026-02-04T02:02:24,006Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204020223-yflykt5m]: associated with Kyuubi SessionHandle: [bb01be09-b85f-4827-88f2-28b71e7a7cc9]\n",
       "2026-02-04T02:02:24,006Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/11125d88-d625-4ec1-aa6e-f802f4c2143d\n",
       "2026-02-04T02:02:24,007Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [bb01be09-b85f-4827-88f2-28b71e7a7cc9] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T02:02:24,007Z INFO KyuubiSessionImpl: [anonymous:100.67.216.117] SessionHandle [bb01be09-b85f-4827-88f2-28b71e7a7cc9] - Engine has been launched, elapsed time: 0 s\n",
       "26/02/04 02:02:23 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 02:02:23 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 02:02:23 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [bb01be09-b85f-4827-88f2-28b71e7a7cc9]\n",
       "26/02/04 02:02:23 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [bb01be09-b85f-4827-88f2-28b71e7a7cc9]/kernel-v3d2b310a8d4a3e8e2e9b4f0e6bc2978bf478430d3 is opened, current opening sessions 7\n",
       "26/02/04 02:02:23 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 02:02:23 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 02:02:23 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [485a77ec-580f-48a4-a903-1148b04cfded]\n",
       "26/02/04 02:02:23 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [485a77ec-580f-48a4-a903-1148b04cfded]/bb01be09-b85f-4827-88f2-28b71e7a7cc9_aliveness_probe is opened, current opening sessions 8\n",
       "26/02/04 02:02:24 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204020223-yflykt5m)]: associated with spark-sql operation session: [bb01be09-b85f-4827-88f2-28b71e7a7cc9]\n",
       "26/02/04 02:02:24 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-8d0adb20-34c6-4655-9ab7-28076577e734/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:8:d:0:a:d:b:2:0:-:3:4:c:6:-:4:6:5:5:-:9:a:b:7:-:2:8:0:7:6:5:7:7:e:7:3:4\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-8d0adb20-34c6-4655-9ab7-28076577e734/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=bb01be09-b85f-4827-88f2-28b71e7a7cc9\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "2026-02-04T02:02:24,014Z INFO ExecuteStatement: Processing anonymous's query[11125d88-d625-4ec1-aa6e-f802f4c2143d]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:24,014Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/bb01be09-b85f-4827-88f2-28b71e7a7cc9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204020223-yflykt5m\n",
       "2026-02-04T02:02:24,019Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/11125d88-d625-4ec1-aa6e-f802f4c2143d/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:24,508Z INFO ExecuteStatement: Query[11125d88-d625-4ec1-aa6e-f802f4c2143d] in FINISHED_STATE\n",
       "2026-02-04T02:02:24,508Z INFO ExecuteStatement: Processing anonymous's query[11125d88-d625-4ec1-aa6e-f802f4c2143d]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.494 seconds\n",
       "26/02/04 02:02:24 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/11125d88-d625-4ec1-aa6e-f802f4c2143d\n",
       "26/02/04 02:02:24 INFO ExecutePython: Processing anonymous's query[11125d88-d625-4ec1-aa6e-f802f4c2143d]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:02:24 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:02:24 INFO ExecutePython: Processing anonymous's query[11125d88-d625-4ec1-aa6e-f802f4c2143d]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.008 seconds\n",
       "26/02/04 02:02:24 INFO DAGScheduler: Asked to cancel job group 11125d88-d625-4ec1-aa6e-f802f4c2143d\n",
       "2026-02-04T02:02:25,025Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/11125d88-d625-4ec1-aa6e-f802f4c2143d/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:25,036Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/11125d88-d625-4ec1-aa6e-f802f4c2143d/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:25,180Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/90bf544f-f7cc-4af6-84e5-462dcea86f9a\n",
       "2026-02-04T02:02:25,182Z INFO ExecuteStatement: Processing anonymous's query[90bf544f-f7cc-4af6-84e5-462dcea86f9a]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:25,182Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/bb01be09-b85f-4827-88f2-28b71e7a7cc9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:25,187Z INFO ExecuteStatement: Query[90bf544f-f7cc-4af6-84e5-462dcea86f9a] in FINISHED_STATE\n",
       "2026-02-04T02:02:25,187Z INFO ExecuteStatement: Processing anonymous's query[90bf544f-f7cc-4af6-84e5-462dcea86f9a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T02:02:25,702Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/ba2c06dc-b4e1-4c10-904c-72b7098c4892\n",
       "2026-02-04T02:02:25,706Z INFO ExecuteStatement: Processing anonymous's query[ba2c06dc-b4e1-4c10-904c-72b7098c4892]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:25,706Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/bb01be09-b85f-4827-88f2-28b71e7a7cc9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:25,772Z INFO ExecuteStatement: Query[ba2c06dc-b4e1-4c10-904c-72b7098c4892] in FINISHED_STATE\n",
       "2026-02-04T02:02:25,772Z INFO ExecuteStatement: Processing anonymous's query[ba2c06dc-b4e1-4c10-904c-72b7098c4892]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.066 seconds\n",
       "26/02/04 02:02:25 INFO DAGScheduler: Asked to cancel job group 11125d88-d625-4ec1-aa6e-f802f4c2143d\n",
       "26/02/04 02:02:25 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/90bf544f-f7cc-4af6-84e5-462dcea86f9a\n",
       "26/02/04 02:02:25 INFO ExecutePython: Processing anonymous's query[90bf544f-f7cc-4af6-84e5-462dcea86f9a]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:02:25 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:02:25 INFO ExecutePython: Processing anonymous's query[90bf544f-f7cc-4af6-84e5-462dcea86f9a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 02:02:25 INFO DAGScheduler: Asked to cancel job group 90bf544f-f7cc-4af6-84e5-462dcea86f9a\n",
       "26/02/04 02:02:25 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/ba2c06dc-b4e1-4c10-904c-72b7098c4892\n",
       "26/02/04 02:02:25 INFO ExecutePython: Processing anonymous's query[ba2c06dc-b4e1-4c10-904c-72b7098c4892]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:02:25 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:02:25 INFO ExecutePython: Processing anonymous's query[ba2c06dc-b4e1-4c10-904c-72b7098c4892]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.061 seconds\n",
       "26/02/04 02:02:25 INFO DAGScheduler: Asked to cancel job group ba2c06dc-b4e1-4c10-904c-72b7098c4892\n",
       "2026-02-04T02:02:26,205Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ba2c06dc-b4e1-4c10-904c-72b7098c4892/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:26,344Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ba2c06dc-b4e1-4c10-904c-72b7098c4892/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:02:26 INFO DAGScheduler: Asked to cancel job group ba2c06dc-b4e1-4c10-904c-72b7098c4892\n",
       "2026-02-04T02:02:28,497Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/f94dc42a-c017-4148-b689-979aeec00e76\n",
       "2026-02-04T02:02:28,499Z INFO ExecuteStatement: Processing anonymous's query[f94dc42a-c017-4148-b689-979aeec00e76]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:28,500Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/bb01be09-b85f-4827-88f2-28b71e7a7cc9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:28,505Z INFO ExecuteStatement: Query[f94dc42a-c017-4148-b689-979aeec00e76] in FINISHED_STATE\n",
       "2026-02-04T02:02:28,505Z INFO ExecuteStatement: Processing anonymous's query[f94dc42a-c017-4148-b689-979aeec00e76]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.005 seconds\n",
       "2026-02-04T02:02:28,991Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/ef326310-707e-43d3-bf13-1340c1209f83\n",
       "2026-02-04T02:02:28,993Z INFO ExecuteStatement: Processing anonymous's query[ef326310-707e-43d3-bf13-1340c1209f83]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:02:28,993Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/bb01be09-b85f-4827-88f2-28b71e7a7cc9/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:02:28 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/f94dc42a-c017-4148-b689-979aeec00e76\n",
       "26/02/04 02:02:28 INFO ExecutePython: Processing anonymous's query[f94dc42a-c017-4148-b689-979aeec00e76]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:02:28 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:02:28 INFO ExecutePython: Processing anonymous's query[f94dc42a-c017-4148-b689-979aeec00e76]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 02:02:28 INFO DAGScheduler: Asked to cancel job group f94dc42a-c017-4148-b689-979aeec00e76\n",
       "26/02/04 02:02:28 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/bb01be09-b85f-4827-88f2-28b71e7a7cc9/ef326310-707e-43d3-bf13-1340c1209f83\n",
       "26/02/04 02:02:28 INFO ExecutePython: Processing anonymous's query[ef326310-707e-43d3-bf13-1340c1209f83]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:02:28 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:02:29 INFO GpuOverrides: Plan conversion to the GPU took 0.24 ms\n",
       "26/02/04 02:02:29 INFO GpuOverrides: GPU plan transition optimization took 0.14 ms\n",
       "2026-02-04T02:02:29,493Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ef326310-707e-43d3-bf13-1340c1209f83/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:02:29 INFO GpuOverrides: Plan conversion to the GPU took 0.28 ms\n",
       "26/02/04 02:02:29 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 02:02:29 INFO InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.\n",
       "26/02/04 02:02:29 INFO SparkContext: Starting job: sql at <unknown>:0\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Got job 44 (sql at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Final stage: ResultStage 56 (sql at <unknown>:0)\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[285] at sql at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:02:29 INFO SQLOperationListener: Query [ef326310-707e-43d3-bf13-1340c1209f83]: Job 44 started with 1 stages, 1 active jobs running\n",
       "26/02/04 02:02:29 INFO SQLOperationListener: Query [ef326310-707e-43d3-bf13-1340c1209f83]: Stage 56.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 02:02:29 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 137.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:29 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 51.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:29 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 100.67.56.160:7079 (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:02:29 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[285] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:02:29 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:02:29 INFO FairSchedulableBuilder: Added task set TaskSet_56.0 tasks to pool \n",
       "26/02/04 02:02:29 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 339) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 9378 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:02:29 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 100.67.4.240:46241 (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:02:29 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 339) in 106 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:02:29 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:02:29 INFO DAGScheduler: ResultStage 56 (sql at <unknown>:0) finished in 0.128 s\n",
       "26/02/04 02:02:29 INFO SQLOperationListener: Finished stage: Stage(56, 0); Name: 'sql at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 128 msec\n",
       "26/02/04 02:02:29 INFO StatsReportListener: task runtime:(count: 1, mean: 106.000000, stdev: 0.000000, max: 106.000000, min: 106.000000)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\t106.0 ms\n",
       "26/02/04 02:02:29 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:02:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished\n",
       "26/02/04 02:02:29 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO DAGScheduler: Job 44 finished: sql at <unknown>:0, took 0.130075 s\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 02:02:29 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:02:29 INFO StatsReportListener: task result size:(count: 1, mean: 1889.000000, stdev: 0.000000, max: 1889.000000, min: 1889.000000)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\t1889.0 B\n",
       "26/02/04 02:02:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "26/02/04 02:02:29 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 87.735849, stdev: 0.000000, max: 87.735849, min: 87.735849)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t88 %\t88 %\t88 %\t88 %\t88 %\t88 %\t88 %\t88 %\t88 %\n",
       "26/02/04 02:02:29 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 02:02:29 INFO StatsReportListener: other time pct: (count: 1, mean: 12.264151, stdev: 0.000000, max: 12.264151, min: 12.264151)\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:02:29 INFO StatsReportListener: \t12 %\t12 %\t12 %\t12 %\t12 %\t12 %\t12 %\t12 %\t12 %\n",
       "26/02/04 02:02:29 INFO SparkSQLEngineListener: Job end. Job 44 state is JobSucceeded\n",
       "26/02/04 02:02:29 INFO SQLOperationListener: Query [ef326310-707e-43d3-bf13-1340c1209f83]: Job 44 succeeded, 0 active jobs running\n",
       "26/02/04 02:02:29 INFO HiveExternalCatalog: Persisting file based data source table \\`spark_catalog\\`.\\`default\\`.\\`gcp_east4_maestro_slurm_nodes\\` into Hive metastore in Hive compatible format.\n",
       "26/02/04 02:02:29 WARN HiveExternalCatalog: Could not persist \\`spark_catalog\\`.\\`default\\`.\\`gcp_east4_maestro_slurm_nodes\\` in a Hive compatible way. Persisting it into Hive metastore in Spark SQL specific format.\n",
       "org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:869)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
       "\tat org.apache.spark.sql.hive.client.Shim_v0_12.createTable(HiveShim.scala:614)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$createTable\\$1(HiveClientImpl.scala:573)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.\\$anonfun\\$withHiveState\\$1(HiveClientImpl.scala:303)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1\\$1(HiveClientImpl.scala:234)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283)\n",
       "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:571)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:526)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:415)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.\\$anonfun\\$createTable\\$1(HiveExternalCatalog.scala:274)\n",
       "\tat scala.runtime.java8.JFunction0\\$mcV\\$sp.apply(JFunction0\\$mcV\\$sp.java:23)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\n",
       "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:408)\n",
       "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableCommand.run(createDataSourceTables.scala:120)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\\$lzycompute(commands.scala:75)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.\\$anonfun\\$applyOrElse\\$1(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$6(SQLExecution.scala:125)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withSQLConfPropagated(SQLExecution.scala:201)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.\\$anonfun\\$withNewExecutionId\\$1(SQLExecution.scala:108)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution\\$.withNewExecutionId(SQLExecution.scala:66)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:107)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution\\$\\$anonfun\\$eagerlyExecuteCommands\\$1.applyOrElse(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.\\$anonfun\\$transformDownWithPruning\\$1(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin\\$.withOrigin(origin.scala:76)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\\$apache\\$spark\\$sql\\$catalyst\\$plans\\$logical\\$AnalysisHelper\\$\\$super\\$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\\$(AnalysisHelper.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted\\$lzycompute(QueryExecution.scala:85)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
       "\tat org.apache.spark.sql.Dataset\\$.\\$anonfun\\$ofRows\\$2(Dataset.scala:100)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.Dataset\\$.ofRows(Dataset.scala:97)\n",
       "\tat org.apache.spark.sql.SparkSession.\\$anonfun\\$sql\\$1(SparkSession.scala:638)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
       "\tat sun.reflect.GeneratedMethodAccessor255.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: MetaException(message:Got exception: org.apache.hadoop.fs.UnsupportedFileSystemException No FileSystem for scheme \"s3\")\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42225)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result\\$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:42193)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:42119)\n",
       "\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1203)\n",
       "\tat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore\\$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1189)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat sun.reflect.GeneratedMethodAccessor258.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient\\$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2327)\n",
       "\tat com.sun.proxy.\\$Proxy53.createTable(Unknown Source)\n",
       "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
       "\t... 60 more\n",
       "2026-02-04T02:02:30,638Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ef326310-707e-43d3-bf13-1340c1209f83/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:30,799Z INFO ExecuteStatement: Query[ef326310-707e-43d3-bf13-1340c1209f83] in FINISHED_STATE\n",
       "2026-02-04T02:02:30,799Z INFO ExecuteStatement: Processing anonymous's query[ef326310-707e-43d3-bf13-1340c1209f83]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.806 seconds\n",
       "26/02/04 02:02:30 INFO GpuOverrides: Plan conversion to the GPU took 0.85 ms\n",
       "26/02/04 02:02:30 INFO GpuOverrides: GPU plan transition optimization took 0.17 ms\n",
       "26/02/04 02:02:30 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#2873 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#2874 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#2875 could run on GPU\n",
       "\n",
       "26/02/04 02:02:30 INFO GpuOverrides: Plan conversion to the GPU took 0.43 ms\n",
       "26/02/04 02:02:30 INFO GpuOverrides: GPU plan transition optimization took 0.17 ms\n",
       "26/02/04 02:02:30 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:02:30 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 02:02:30 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:02:30 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 02:02:30 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:02:30 INFO InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.\n",
       "26/02/04 02:02:30 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 02:02:30 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 02:02:30 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 02:02:30 INFO GpuOverrides: Plan conversion to the GPU took 1.38 ms\n",
       "26/02/04 02:02:30 INFO GpuOverrides: GPU plan transition optimization took 0.80 ms\n",
       "26/02/04 02:02:30 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 02:02:30 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:30 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:30 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:02:30 INFO SparkContext: Created broadcast 62 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 02:02:30 INFO SparkContext: Starting job: showString at <unknown>:0\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Got job 45 (showString at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Final stage: ResultStage 57 (showString at <unknown>:0)\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[294] at showString at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:02:30 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:30 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 02:02:30 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:02:30 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[294] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:02:30 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:02:30 INFO FairSchedulableBuilder: Added task set TaskSet_57.0 tasks to pool \n",
       "26/02/04 02:02:30 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 340) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10152 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:02:30 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:02:30 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:02:30 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 340) in 159 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:02:30 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:02:30 INFO DAGScheduler: ResultStage 57 (showString at <unknown>:0) finished in 0.165 s\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:02:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished\n",
       "26/02/04 02:02:30 INFO SparkSQLEngineListener: Job end. Job 45 state is JobSucceeded\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Job 45 finished: showString at <unknown>:0, took 0.166303 s\n",
       "26/02/04 02:02:30 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:02:30 INFO ExecutePython: |node_id   |scontrol_state|reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 02:02:30 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-001|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-002|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-003|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-004|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-005|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-006|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-007|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-008|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-009|[\"IDLE\"]      |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: |cpu-dm-010|[\"IDLE\"]      |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
       "26/02/04 02:02:30 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:02:30 INFO ExecutePython: Processing anonymous's query[ef326310-707e-43d3-bf13-1340c1209f83]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.802 seconds\n",
       "26/02/04 02:02:30 INFO DAGScheduler: Asked to cancel job group ef326310-707e-43d3-bf13-1340c1209f83\n",
       "2026-02-04T02:02:31,791Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ef326310-707e-43d3-bf13-1340c1209f83/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:02:31,937Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ef326310-707e-43d3-bf13-1340c1209f83/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:02:31 INFO DAGScheduler: Asked to cancel job group ef326310-707e-43d3-bf13-1340c1209f83\n",
       "2026-02-04T02:02:47,113Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:02:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "26/02/04 02:02:59 WARN SparkSQLOperationManager: Operation OperationHandle [3416723f-5955-45b0-bc64-bc784fc29d89] is timed-out and will be closed\n",
       "26/02/04 02:02:59 WARN SparkSQLOperationManager: Operation OperationHandle [dde1f4f9-acfc-47bb-8001-c70c8d3d8153] is timed-out and will be closed\n",
       "26/02/04 02:02:59 WARN SparkSQLOperationManager: Operation OperationHandle [94de39e2-66cb-48d1-a524-153a9d53ab62] is timed-out and will be closed\n",
       "2026-02-04T02:03:17,113Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T02:03:17,113Z WARN KyuubiOperationManager: Operation OperationHandle [3416723f-5955-45b0-bc64-bc784fc29d89] is timed-out and will be closed\n",
       "2026-02-04T02:03:17,113Z WARN KyuubiOperationManager: Operation OperationHandle [dde1f4f9-acfc-47bb-8001-c70c8d3d8153] is timed-out and will be closed\n",
       "2026-02-04T02:03:17,113Z WARN KyuubiOperationManager: Operation OperationHandle [94de39e2-66cb-48d1-a524-153a9d53ab62] is timed-out and will be closed\n",
       "2026-02-04T02:03:17,113Z WARN KyuubiOperationManager: Operation OperationHandle [254b2837-9aa7-43b8-80d7-39bd97bd8841] is timed-out and will be closed\n",
       "2026-02-04T02:03:17,113Z WARN KyuubiOperationManager: Operation OperationHandle [febb1cd3-5aef-47d7-b3d1-1005ad3c4adc] is timed-out and will be closed\n",
       "2026-02-04T02:03:17,113Z WARN KyuubiOperationManager: Operation OperationHandle [98b0288e-266a-473e-b052-506b0b79addd] is timed-out and will be closed\n",
       "2026-02-04T02:03:17,115Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:34 16 72 3F 59 55 45 B0 BC 64 BC 78 4F C2 9D 89, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [3416723f-5955-45b0-bc64-bc784fc29d89]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T02:03:17,117Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:DD E1 F4 F9 AC FC 47 BB 80 01 C7 0C 8D 3D 81 53, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [dde1f4f9-acfc-47bb-8001-c70c8d3d8153]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T02:03:17,118Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:94 DE 39 E2 66 CB 48 D1 A5 24 15 3A 9D 53 AB 62, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [94de39e2-66cb-48d1-a524-153a9d53ab62]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T02:03:17,119Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:25 4B 28 37 9A A7 43 B8 80 D7 39 BD 97 BD 88 41, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T02:03:17,120Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:FE BB 1C D3 5A EF 47 D7 B3 D1 10 05 AD 3C 4A DC, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "26/02/04 02:03:17 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [3416723f-5955-45b0-bc64-bc784fc29d89]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:17 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [dde1f4f9-acfc-47bb-8001-c70c8d3d8153]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:17 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [94de39e2-66cb-48d1-a524-153a9d53ab62]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "26/02/04 02:03:29 WARN SparkSQLOperationManager: Operation OperationHandle [11125d88-d625-4ec1-aa6e-f802f4c2143d] is timed-out and will be closed\n",
       "26/02/04 02:03:29 WARN SparkSQLOperationManager: Operation OperationHandle [ba2c06dc-b4e1-4c10-904c-72b7098c4892] is timed-out and will be closed\n",
       "26/02/04 02:03:29 WARN SparkSQLOperationManager: Operation OperationHandle [90bf544f-f7cc-4af6-84e5-462dcea86f9a] is timed-out and will be closed\n",
       "26/02/04 02:03:29 WARN SparkSQLOperationManager: Operation OperationHandle [f94dc42a-c017-4148-b689-979aeec00e76] is timed-out and will be closed\n",
       "2026-02-04T02:03:47,120Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "2026-02-04T02:03:47,121Z WARN KyuubiOperationManager: Operation OperationHandle [11125d88-d625-4ec1-aa6e-f802f4c2143d] is timed-out and will be closed\n",
       "2026-02-04T02:03:47,121Z WARN KyuubiOperationManager: Operation OperationHandle [d76a95a0-5c8b-4633-a82d-8b6bb75a9c03] is timed-out and will be closed\n",
       "2026-02-04T02:03:47,121Z WARN KyuubiOperationManager: Operation OperationHandle [ba2c06dc-b4e1-4c10-904c-72b7098c4892] is timed-out and will be closed\n",
       "2026-02-04T02:03:47,121Z WARN KyuubiOperationManager: Operation OperationHandle [ef326310-707e-43d3-bf13-1340c1209f83] is timed-out and will be closed\n",
       "2026-02-04T02:03:47,121Z WARN KyuubiOperationManager: Operation OperationHandle [90bf544f-f7cc-4af6-84e5-462dcea86f9a] is timed-out and will be closed\n",
       "2026-02-04T02:03:47,121Z WARN KyuubiOperationManager: Operation OperationHandle [f94dc42a-c017-4148-b689-979aeec00e76] is timed-out and will be closed\n",
       "2026-02-04T02:03:47,123Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:11 12 5D 88 D6 25 4E C1 AA 6E F8 02 F4 C2 14 3D, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [11125d88-d625-4ec1-aa6e-f802f4c2143d]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T02:03:47,124Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:BA 2C 06 DC B4 E1 4C 10 90 4C 72 B7 09 8C 48 92, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [ba2c06dc-b4e1-4c10-904c-72b7098c4892]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T02:03:47,125Z INFO KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:EF 32 63 10 70 7E 43 D3 BF 13 13 40 C1 20 9F 83, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) succeed on engine side\n",
       "2026-02-04T02:03:47,125Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:90 BF 54 4F F7 CC 4A F6 84 E5 46 2D CE A8 6F 9A, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [90bf544f-f7cc-4af6-84e5-462dcea86f9a]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "2026-02-04T02:03:47,126Z WARN KyuubiSyncThriftClient: TCloseOperationReq(operationHandle:TOperationHandle(operationId:THandleIdentifier(guid:F9 4D C4 2A C0 17 41 48 B6 89 97 9A EE C0 0E 76, secret:C2 EE 5B 97 3E A0 41 FC AC 16 9B D7 08 ED 8F 38), operationType:EXECUTE_STATEMENT, hasResultSet:true)) failed on engine side\n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [f94dc42a-c017-4148-b689-979aeec00e76]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:47 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [11125d88-d625-4ec1-aa6e-f802f4c2143d]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:47 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [ba2c06dc-b4e1-4c10-904c-72b7098c4892]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:47 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [90bf544f-f7cc-4af6-84e5-462dcea86f9a]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:47 ERROR SparkTBinaryFrontendService: Error closing operation: \n",
       "org.apache.kyuubi.KyuubiSQLException: Invalid OperationHandle [f94dc42a-c017-4148-b689-979aeec00e76]\n",
       "\tat org.apache.kyuubi.KyuubiSQLException\\$.apply(KyuubiSQLException.scala:69)\n",
       "\tat org.apache.kyuubi.operation.OperationManager.getOperation(OperationManager.scala:107)\n",
       "\tat org.apache.kyuubi.service.AbstractBackendService.closeOperation(AbstractBackendService.scala:192)\n",
       "\tat org.apache.kyuubi.service.TFrontendService.CloseOperation(TFrontendService.scala:498)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1797)\n",
       "\tat org.apache.kyuubi.shade.org.apache.hive.service.rpc.thrift.TCLIService\\$Processor\\$CloseOperation.getResult(TCLIService.java:1782)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n",
       "\tat org.apache.kyuubi.service.authentication.TSetIpAddressProcessor.process(TSetIpAddressProcessor.scala:36)\n",
       "\tat org.apache.kyuubi.shade.org.apache.thrift.server.TThreadPoolServer\\$WorkerProcess.run(TThreadPoolServer.java:286)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor\\$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "26/02/04 02:03:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:04:17,127Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:04:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:04:47,127Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:04:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:05:17,127Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:05:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:05:47,127Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:05:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:06:17,127Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:06:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:06:47,128Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:06:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:07:17,128Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:07:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:07:47,128Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 4\n",
       "26/02/04 02:07:59 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 8\n",
       "2026-02-04T02:08:00,172Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T02:08:00,172Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T02:08:00,173Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T02:08:00,173Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@3f61bfea\n",
       "2026-02-04T02:08:00,174Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T02:08:00,174Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T02:08:00,174Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T02:08:00,174Z INFO AdminResource: Listing engine nodes for /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:08:00,178Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181.\n",
       "2026-02-04T02:08:00,178Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T02:08:00,178Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:38958, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181\n",
       "2026-02-04T02:08:00,183Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.116.15:2181, session id = 0x100f75a49083cd0, negotiated timeout = 120000\n",
       "2026-02-04T02:08:00,183Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T02:08:00,184Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:08:00,186Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:08:00,186Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T02:08:00,187Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:08:00,290Z INFO ZooKeeper: Session: 0x100f75a49083cd0 closed\n",
       "2026-02-04T02:08:00,290Z INFO ClientCnxn: EventThread shut down for session: 0x100f75a49083cd0\n",
       "2026-02-04T02:08:00,293Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/admin/engine\tparams=subdomain=cluster-20260203202803-yawkv5ak&groupname=default&sharelevel=GROUP\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:00,536Z INFO KyuubiSessionManager: Opening session for anonymous@100.67.97.22\n",
       "2026-02-04T02:08:00,536Z WARN KyuubiConf: The Kyuubi config 'kyuubi.frontend.bind.port' has been deprecated in Kyuubi v1.4.0 and may be removed in the future. Use kyuubi.frontend.thrift.binary.bind.port instead\n",
       "2026-02-04T02:08:00,536Z WARN KyuubiConf: The Kyuubi config 'kyuubi.engine.connection.url.use.hostname' has been deprecated in Kyuubi v1.5.0 and may be removed in the future. Use kyuubi.frontend.connection.url.use.hostname instead\n",
       "2026-02-04T02:08:00,537Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/2ce40fbf-ebbe-4b03-9ce3-016282832f9d\n",
       "2026-02-04T02:08:00,537Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [58dd32a9-405c-47a7-abbb-c1daa648e609]/kernel-v3ccf6af19086051851e6f615661cadb9623ccd0a7 is opened, current opening sessions 5\n",
       "2026-02-04T02:08:00,537Z INFO LaunchEngine: Processing anonymous's query[2ce40fbf-ebbe-4b03-9ce3-016282832f9d]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:00,538Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:00,538Z INFO CuratorFrameworkImpl: Starting\n",
       "2026-02-04T02:08:00,538Z INFO ZooKeeper: Initiating client connection, connectString=nvspark-zookeeper-headless.nvspark.svc.cluster.local:2181 sessionTimeout=120000 watcher=org.apache.kyuubi.shaded.curator.ConnectionState@6ae97d98\n",
       "2026-02-04T02:08:00,538Z INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\n",
       "2026-02-04T02:08:00,539Z INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\n",
       "2026-02-04T02:08:00,539Z INFO CuratorFrameworkImpl: Default schema\n",
       "2026-02-04T02:08:00,540Z INFO ClientCnxn: Opening socket connection to server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181.\n",
       "2026-02-04T02:08:00,541Z INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
       "2026-02-04T02:08:00,542Z INFO ClientCnxn: Socket connection established, initiating session, client: /100.67.56.160:33294, server: nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181\n",
       "2026-02-04T02:08:00,546Z INFO ClientCnxn: Session establishment complete on server nvspark-zookeeper-headless.nvspark.svc.cluster.local/100.67.143.230:2181, session id = 0x30263a585b6310d, negotiated timeout = 120000\n",
       "2026-02-04T02:08:00,546Z INFO ConnectionStateManager: State change: CONNECTED\n",
       "2026-02-04T02:08:00,548Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:08:00,548Z INFO EnsembleTracker: New config event received: {server.1=nvspark-zookeeper-0.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, version=0, server.3=nvspark-zookeeper-2.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181, server.2=nvspark-zookeeper-1.nvspark-zookeeper-headless.nvspark.svc.cluster.local:2888:3888:participant;0.0.0.0:2181}\n",
       "2026-02-04T02:08:00,550Z INFO ZookeeperDiscoveryClient: Get service instance:100.67.56.160:37281 engine id:spark-cdd9e5d0115345edb431647f51f82517 and version:1.8.0.5-SNAPSHOT under /kyuubi_1.8.0.5-SNAPSHOT_GROUP_SPARK_SQL/default/cluster-20260203202803-yawkv5ak\n",
       "2026-02-04T02:08:00,561Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [58dd32a9-405c-47a7-abbb-c1daa648e609] - Connected to engine [100.67.56.160:37281]/[spark-cdd9e5d0115345edb431647f51f82517] with SessionHandle [58dd32a9-405c-47a7-abbb-c1daa648e609]]\n",
       "2026-02-04T02:08:00,561Z INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting\n",
       "2026-02-04T02:08:00,666Z INFO ZooKeeper: Session: 0x30263a585b6310d closed\n",
       "2026-02-04T02:08:00,666Z INFO ClientCnxn: EventThread shut down for session: 0x30263a585b6310d\n",
       "2026-02-04T02:08:00,666Z INFO LaunchEngine: Processing anonymous's query[2ce40fbf-ebbe-4b03-9ce3-016282832f9d]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.129 seconds\n",
       "2026-02-04T02:08:00,745Z INFO SessionsResource: Sparkaas- [Transaction:transaction-20260204020759-ghpsrwwe]: associated with Kyuubi SessionHandle: [58dd32a9-405c-47a7-abbb-c1daa648e609]\n",
       "2026-02-04T02:08:00,746Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/1efeb561-1fa5-4e58-bfb5-cff020815673\n",
       "2026-02-04T02:08:00,746Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [58dd32a9-405c-47a7-abbb-c1daa648e609] - Starting to wait the launch engine operation finished\n",
       "2026-02-04T02:08:00,746Z INFO KyuubiSessionImpl: [anonymous:100.67.97.22] SessionHandle [58dd32a9-405c-47a7-abbb-c1daa648e609] - Engine has been launched, elapsed time: 0 s\n",
       "2026-02-04T02:08:00,751Z INFO ExecuteStatement: Processing anonymous's query[1efeb561-1fa5-4e58-bfb5-cff020815673]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:00,751Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/58dd32a9-405c-47a7-abbb-c1daa648e609/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=transaction-20260204020759-ghpsrwwe\n",
       "2026-02-04T02:08:00,761Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/1efeb561-1fa5-4e58-bfb5-cff020815673/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:08:00 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 02:08:00 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 02:08:00 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [58dd32a9-405c-47a7-abbb-c1daa648e609]\n",
       "26/02/04 02:08:00 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [58dd32a9-405c-47a7-abbb-c1daa648e609]/kernel-v3ccf6af19086051851e6f615661cadb9623ccd0a7 is opened, current opening sessions 9\n",
       "26/02/04 02:08:00 INFO SparkTBinaryFrontendService: Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V1\n",
       "26/02/04 02:08:00 INFO SparkSQLSessionManager: Opening session for anonymous@100.67.56.160\n",
       "26/02/04 02:08:00 INFO KyuubiPythonGatewayServer: Starting KyuubiPythonGatewayServer for session handle SessionHandle [2620b1de-3115-4a55-93ed-1a4f4963db2f]\n",
       "26/02/04 02:08:00 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [2620b1de-3115-4a55-93ed-1a4f4963db2f]/58dd32a9-405c-47a7-abbb-c1daa648e609_aliveness_probe is opened, current opening sessions 10\n",
       "26/02/04 02:08:00 INFO SparkSQLOperationManager: Sparkaas- [Transaction:Some(transaction-20260204020759-ghpsrwwe)]: associated with spark-sql operation session: [58dd32a9-405c-47a7-abbb-c1daa648e609]\n",
       "26/02/04 02:08:00 INFO ExecutePython: \n",
       "launch python worker command: /usr/bin/python3 /tmp/kyuubi-c24b935c-fd89-4019-9781-4d7158277384/execute_python.py\n",
       "environment:\n",
       "PATH=/opt/spark/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre/bin:/usr/lib/jvm/java-1.8.0-openjdk-amd64/bin\n",
       "NV_LIBCUSPARSE_VERSION=12.5.7.53-1\n",
       "NV_NVTX_VERSION=12.8.55-1\n",
       "NV_LIBCUBLAS_PACKAGE_NAME=libcublas-12-8\n",
       "MAESTRO_T1_PORT_80_TCP_ADDR=172.20.168.58\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT=tcp://172.20.209.244:80\n",
       "XDG_CACHE_HOME=/opt/spark/work-dir\n",
       "YH102_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT_HTTP=8888\n",
       "PY4J_PATH=/opt/spark/python/lib/py4j-0.10.9.7-src.zip\n",
       "ACE_DATALAKE_BUCKET_FORMAT=s3://<namespace>-xp\n",
       "SPARK_ENGINE_HOME=/opt/kyuubi/externals/engines/spark\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT=80\n",
       "YH104_PORT_80_TCP_PORT=80\n",
       "AWS_ACCOUNT_OWNER=kratos\n",
       "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP=tcp://172.20.201.113:8888\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP=tcp://172.20.209.244:80\n",
       "KRATOS_SHARD_DNS=xp.kratos.nvidia.com\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_ADDR=172.20.102.22\n",
       "YH102_SERVICE_PORT_HTTP_YH102=80\n",
       "PWD=/opt/kyuubi/work/default\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_PORT=8888\n",
       "KYUUBI_CTL_JAVA_OPTS= -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "ACE_KAFKA_AZ=us-west-1b\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PROTO=tcp\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_PORT_HTTP=80\n",
       "SPARK_CONF_DIR=/opt/spark/conf\n",
       "AWS_STS_REGIONAL_ENDPOINTS=regional\n",
       "YH102_PORT_80_TCP_PORT=80\n",
       "KYUUBI_CONF_DIR=/opt/kyuubi/conf\n",
       "YH104_PORT_80_TCP_PROTO=tcp\n",
       "SPARK_DRIVER_BIND_ADDRESS=100.67.56.160\n",
       "MAESTRO_T1_SERVICE_PORT_HTTP_MAESTRO_T1=80\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT=4040\n",
       "KYUUBI_WORK_DIR_ROOT=/opt/kyuubi/work\n",
       "NVSPARK_CLUSTER_HONGY_PORT=tcp://172.20.119.79:4040\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_SERVICE_HOST=172.20.201.113\n",
       "KUBERNETES_NAMESPACE=dcartm-team\n",
       "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
       "YH102_SERVICE_HOST=172.20.41.103\n",
       "SHLVL=0\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_ADDR=172.20.119.79\n",
       "KUBERNETES_PORT=tcp://172.20.0.1:443\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PROTO=tcp\n",
       "NV_CUDA_LIB_VERSION=12.8.0-1\n",
       "CUDA_VERSION=12.8.0\n",
       "AWS_DEFAULT_REGION=us-west-1\n",
       "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64\n",
       "YH102_PORT_22_TCP=tcp://172.20.41.103:22\n",
       "KYUUBI_SCALA_VERSION=2.12\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PORT=8888\n",
       "KYUUBI_PID_DIR=/run/kyuubi\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_PORT_SPARK_UI=4040\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP=tcp://172.20.102.22:4040\n",
       "SPARK_SCALA_VERSION=2.12\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT=4040\n",
       "PYSPARK_PYTHON=/usr/bin/python3\n",
       "NVSPARK_CLUSTER_HONGY_SERVICE_HOST=172.20.119.79\n",
       "SPARK_HOME=/opt/spark\n",
       "YH104_PORT_22_TCP=tcp://172.20.182.88:22\n",
       "MAGIC_ENABLED=true\n",
       "KYUUBI_LOG_DIR=/opt/kyuubi/logs\n",
       "YH102_PORT_22_TCP_PORT=22\n",
       "KUBERNETES_PORT_443_TCP_ADDR=172.20.0.1\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_HOST=172.20.102.22\n",
       "AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token\n",
       "FLINK_HOME=\n",
       "ML_PIPELINE_UI_ARTIFACT_SERVICE_HOST=172.20.209.244\n",
       "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
       "HOST_TYPE=aws\n",
       "YH104_PORT_22_TCP_ADDR=172.20.182.88\n",
       "KYUUBI_GC_LOG_OPTS= -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_SERVICE_PORT_SPARK_UI=4040\n",
       "MAESTRO_T1_SERVICE_PORT=80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT=tcp://172.20.201.113:8888\n",
       "KRATOS_GRAFANA_SPEC={\"url\": \"https://xp.kratos.nvidia.com/ops\", \"dashboards\": {\"kube_pod_compute\": \"kratos_xp_k8_namespace_pods\", \"xp_pipelines\": \"iEBlpH_7z\"}}\n",
       "SPARK_USER=spring\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PORT=80\n",
       "YH104_PORT_80_TCP_ADDR=172.20.182.88\n",
       "NV_LIBCUBLAS_VERSION=12.8.3.14-1\n",
       "NCCL_VERSION=2.25.1-1\n",
       "NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
       "SPARK_ENV_LOADED=1\n",
       "NVIDIA_PRODUCT_NAME=CUDA\n",
       "YH104_PORT_22_TCP_PROTO=tcp\n",
       "ACE_HIVE_META_STORE=hivemetastore3-cluster.kratos.nvidia.com:3306/metastore\n",
       "YH104_SERVICE_PORT=80\n",
       "MAESTRO_T1_PORT_80_TCP_PORT=80\n",
       "FLINK_ENGINE_HOME=/opt/kyuubi/externals/engines/flink\n",
       "DATABRICKS_HOST=https://nvidia-kratos-ca1.cloud.databricks.com\n",
       "NV_LIBNPP_VERSION=12.3.3.65-1\n",
       "NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
       "KUBERNETES_PORT_443_TCP=tcp://172.20.0.1:443\n",
       "PYTHONPATH=/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/spark/python/lib/pyspark.zip:/:t:m:p:/:k:y:u:u:b:i:-:c:2:4:b:9:3:5:c:-:f:d:8:9:-:4:0:1:9:-:9:7:8:1:-:4:d:7:1:5:8:2:7:7:3:8:4\n",
       "HIVE_ENGINE_HOME=/opt/kyuubi/externals/engines/hive\n",
       "MAESTRO_T1_SERVICE_HOST=172.20.168.58\n",
       "AWS_REGION=us-west-1\n",
       "NVSPARK_CLUSTER_HONGY_PORT_4040_TCP_PORT=4040\n",
       "NV_CUDA_CUDART_VERSION=12.8.57-1\n",
       "YH104_SERVICE_HOST=172.20.182.88\n",
       "NVSPARK_SPEC={\"zones\": [\"us-west-1a\", \"us-west-1b\"]}\n",
       "S3_BUCKET_NAME=dcartm-team\n",
       "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
       "ACE_URL=https://xp.kratos.nvidia.com\n",
       "YH104_SERVICE_PORT_HTTP_YH104=80\n",
       "KYUUBI_HEAP_SIZE=2048m\n",
       "MAESTRO_T1_PORT_80_TCP_PROTO=tcp\n",
       "YH104_PORT=tcp://172.20.182.88:80\n",
       "DEBIAN_FRONTEND=noninteractive\n",
       "POD_NAME=cluster-20260203202803-yawkv5ak-driver\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_PROTO=tcp\n",
       "KYUUBI_JAVA_OPTS=-Xmx2048m  -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit  -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:/opt/kyuubi/logs/kyuubi-server-gc-%t.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=20M -XX:+IgnoreUnrecognizedVMOptions -Dio.netty.tryReflectionSetAccessible=true --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.security.tools.keytool=ALL-UNNAMED --add-opens=java.base/sun.security.x509=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED\n",
       "YH104_SERVICE_PORT_TCP_YH104=22\n",
       "KYUUBI_GC_OPTS= -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+UnlockDiagnosticVMOptions -XX:+UseCondCardMark -XX:+UseGCOverheadLimit\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT=tcp://172.20.102.22:4040\n",
       "ML_PIPELINE_UI_ARTIFACT_PORT_80_TCP_ADDR=172.20.209.244\n",
       "ACE_PACKAGES=s3://kratos-services-xp/packages\n",
       "YH102_PORT_22_TCP_PROTO=tcp\n",
       "KYUUBI_HOME=/opt/kyuubi\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_PROTO=tcp\n",
       "AWS_ROLE_ARN=arn:aws:iam::900732750576:role/xp-dcartm-team-role\n",
       "NVIDIA_VISIBLE_DEVICES=all\n",
       "YH102_PORT_22_TCP_ADDR=172.20.41.103\n",
       "NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
       "ACE_ID=kratos-xp-xp\n",
       "YH104_PORT_80_TCP=tcp://172.20.182.88:80\n",
       "KUBERNETES_SERVICE_HOST=172.20.0.1\n",
       "LANG=en_US.UTF-8\n",
       "YH102_PORT_80_TCP=tcp://172.20.41.103:80\n",
       "ML_PIPELINE_VISUALIZATIONSERVER_PORT_8888_TCP_ADDR=172.20.201.113\n",
       "SPARK_LOCAL_DIRS=/var/data/spark-dbc00d20-334d-441c-a507-991867431d91\n",
       "NV_LIBCUBLAS_PACKAGE=libcublas-12-8=12.8.3.14-1\n",
       "YH102_PORT_80_TCP_ADDR=172.20.41.103\n",
       "TINI_VERSION=v0.18.0\n",
       "PYTHONHASHSEED=0\n",
       "YH102_PORT=tcp://172.20.41.103:80\n",
       "NVSPARK_CLUSTER_RSOMVANSHI_PORT_4040_TCP_PORT=4040\n",
       "TRINO_ENGINE_HOME=/opt/kyuubi/externals/engines/trino\n",
       "PYTHON_GATEWAY_CONNECTION_INFO=/tmp/kyuubi-c24b935c-fd89-4019-9781-4d7158277384/connection.info\n",
       "NVARCH=x86_64\n",
       "SPARK_APPLICATION_ID=spark-f56cf32f2d4f4db791cd3aac9bb7ce07\n",
       "MAESTRO_T1_PORT_80_TCP=tcp://172.20.168.58:80\n",
       "YH102_PORT_80_TCP_PROTO=tcp\n",
       "KUBERNETES_SERVICE_PORT=443\n",
       "NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
       "YH104_PORT_22_TCP_PORT=22\n",
       "MAESTRO_T1_PORT=tcp://172.20.168.58:80\n",
       "NV_LIBNPP_PACKAGE=libnpp-12-8=12.3.3.65-1\n",
       "HOSTNAME=cluster-20260203202803-yawkv5ak-driver\n",
       "KYUUBI_SPARK_SESSION_UUID=58dd32a9-405c-47a7-abbb-c1daa648e609\n",
       "YH102_SERVICE_PORT_TCP_YH102=22\n",
       "KUBERNETES_PORT_443_TCP_PORT=443\n",
       "HOME=/root\n",
       "\n",
       "26/02/04 02:08:00 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/1efeb561-1fa5-4e58-bfb5-cff020815673\n",
       "2026-02-04T02:08:01,309Z INFO ExecuteStatement: Query[1efeb561-1fa5-4e58-bfb5-cff020815673] in FINISHED_STATE\n",
       "2026-02-04T02:08:01,309Z INFO ExecuteStatement: Processing anonymous's query[1efeb561-1fa5-4e58-bfb5-cff020815673]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.558 seconds\n",
       "2026-02-04T02:08:01,765Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/1efeb561-1fa5-4e58-bfb5-cff020815673/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:01,774Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/1efeb561-1fa5-4e58-bfb5-cff020815673/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:01,923Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/4d1978b4-5777-477a-b69f-23846cae7f7a\n",
       "2026-02-04T02:08:01,925Z INFO ExecuteStatement: Processing anonymous's query[4d1978b4-5777-477a-b69f-23846cae7f7a]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:01,925Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/58dd32a9-405c-47a7-abbb-c1daa648e609/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:01,929Z INFO ExecuteStatement: Query[4d1978b4-5777-477a-b69f-23846cae7f7a] in FINISHED_STATE\n",
       "2026-02-04T02:08:01,929Z INFO ExecuteStatement: Processing anonymous's query[4d1978b4-5777-477a-b69f-23846cae7f7a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "26/02/04 02:08:01 INFO ExecutePython: Processing anonymous's query[1efeb561-1fa5-4e58-bfb5-cff020815673]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:08:01 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:08:01 INFO ExecutePython: Processing anonymous's query[1efeb561-1fa5-4e58-bfb5-cff020815673]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.088 seconds\n",
       "26/02/04 02:08:01 INFO DAGScheduler: Asked to cancel job group 1efeb561-1fa5-4e58-bfb5-cff020815673\n",
       "26/02/04 02:08:01 INFO DAGScheduler: Asked to cancel job group 1efeb561-1fa5-4e58-bfb5-cff020815673\n",
       "26/02/04 02:08:01 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/4d1978b4-5777-477a-b69f-23846cae7f7a\n",
       "26/02/04 02:08:01 INFO ExecutePython: Processing anonymous's query[4d1978b4-5777-477a-b69f-23846cae7f7a]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:08:01 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:08:01 INFO ExecutePython: Processing anonymous's query[4d1978b4-5777-477a-b69f-23846cae7f7a]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 02:08:01 INFO DAGScheduler: Asked to cancel job group 4d1978b4-5777-477a-b69f-23846cae7f7a\n",
       "2026-02-04T02:08:02,470Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/48a93da1-632c-4c63-8686-0bbdeb83a891\n",
       "2026-02-04T02:08:02,472Z INFO ExecuteStatement: Processing anonymous's query[48a93da1-632c-4c63-8686-0bbdeb83a891]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:02,472Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/58dd32a9-405c-47a7-abbb-c1daa648e609/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:02,517Z INFO ExecuteStatement: Query[48a93da1-632c-4c63-8686-0bbdeb83a891] in FINISHED_STATE\n",
       "2026-02-04T02:08:02,517Z INFO ExecuteStatement: Processing anonymous's query[48a93da1-632c-4c63-8686-0bbdeb83a891]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.045 seconds\n",
       "2026-02-04T02:08:03,007Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/48a93da1-632c-4c63-8686-0bbdeb83a891/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:08:02 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/48a93da1-632c-4c63-8686-0bbdeb83a891\n",
       "26/02/04 02:08:02 INFO ExecutePython: Processing anonymous's query[48a93da1-632c-4c63-8686-0bbdeb83a891]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:08:02 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:08:02 INFO ExecutePython: Processing anonymous's query[48a93da1-632c-4c63-8686-0bbdeb83a891]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.041 seconds\n",
       "26/02/04 02:08:02 INFO DAGScheduler: Asked to cancel job group 48a93da1-632c-4c63-8686-0bbdeb83a891\n",
       "2026-02-04T02:08:03,155Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/48a93da1-632c-4c63-8686-0bbdeb83a891/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:08:03 INFO DAGScheduler: Asked to cancel job group 48a93da1-632c-4c63-8686-0bbdeb83a891\n",
       "2026-02-04T02:08:05,343Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/751fb789-294e-42bc-a530-037719db42fe\n",
       "2026-02-04T02:08:05,345Z INFO ExecuteStatement: Processing anonymous's query[751fb789-294e-42bc-a530-037719db42fe]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:05,345Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/58dd32a9-405c-47a7-abbb-c1daa648e609/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:05,349Z INFO ExecuteStatement: Query[751fb789-294e-42bc-a530-037719db42fe] in FINISHED_STATE\n",
       "2026-02-04T02:08:05,349Z INFO ExecuteStatement: Processing anonymous's query[751fb789-294e-42bc-a530-037719db42fe]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "2026-02-04T02:08:05,855Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/595c0c20-6f1c-420f-a823-d0ec77424b5c\n",
       "2026-02-04T02:08:05,857Z INFO ExecuteStatement: Processing anonymous's query[595c0c20-6f1c-420f-a823-d0ec77424b5c]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:05,858Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/58dd32a9-405c-47a7-abbb-c1daa648e609/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:08:05 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/751fb789-294e-42bc-a530-037719db42fe\n",
       "26/02/04 02:08:05 INFO ExecutePython: Processing anonymous's query[751fb789-294e-42bc-a530-037719db42fe]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:08:05 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:08:05 INFO ExecutePython: Processing anonymous's query[751fb789-294e-42bc-a530-037719db42fe]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 02:08:05 INFO DAGScheduler: Asked to cancel job group 751fb789-294e-42bc-a530-037719db42fe\n",
       "26/02/04 02:08:05 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/595c0c20-6f1c-420f-a823-d0ec77424b5c\n",
       "26/02/04 02:08:05 INFO ExecutePython: Processing anonymous's query[595c0c20-6f1c-420f-a823-d0ec77424b5c]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:08:05 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:08:06 INFO InMemoryFileIndex: It took 32 ms to list leaf files for 1 paths.\n",
       "26/02/04 02:08:06 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 02:08:06 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 02:08:06 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 02:08:06 INFO GpuOverrides: Plan conversion to the GPU took 1.04 ms\n",
       "2026-02-04T02:08:06,380Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/595c0c20-6f1c-420f-a823-d0ec77424b5c/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:08:06 INFO GpuOverrides: GPU plan transition optimization took 0.57 ms\n",
       "26/02/04 02:08:06 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO SparkContext: Created broadcast 64 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 02:08:06 INFO SparkContext: Starting job: showString at <unknown>:0\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Got job 46 (showString at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Final stage: ResultStage 58 (showString at <unknown>:0)\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[303] at showString at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:08:06 INFO SQLOperationListener: Query [595c0c20-6f1c-420f-a823-d0ec77424b5c]: Job 46 started with 1 stages, 1 active jobs running\n",
       "26/02/04 02:08:06 INFO SQLOperationListener: Query [595c0c20-6f1c-420f-a823-d0ec77424b5c]: Stage 58.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[303] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:08:06 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:08:06 INFO FairSchedulableBuilder: Added task set TaskSet_58.0 tasks to pool \n",
       "26/02/04 02:08:06 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 341) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10152 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:06 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 341) in 216 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:08:06 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:08:06 INFO DAGScheduler: ResultStage 58 (showString at <unknown>:0) finished in 0.221 s\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:08:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished\n",
       "26/02/04 02:08:06 INFO SQLOperationListener: Finished stage: Stage(58, 0); Name: 'showString at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 221 msec\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Job 46 finished: showString at <unknown>:0, took 0.224525 s\n",
       "26/02/04 02:08:06 INFO StatsReportListener: task runtime:(count: 1, mean: 216.000000, stdev: 0.000000, max: 216.000000, min: 216.000000)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t216.0 ms\t216.0 ms\t216.0 ms\t216.0 ms\t216.0 ms\t216.0 ms\t216.0 ms\t216.0 ms\t216.0 ms\n",
       "26/02/04 02:08:06 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:08:06 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 02:08:06 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:08:06 INFO StatsReportListener: task result size:(count: 1, mean: 4159.000000, stdev: 0.000000, max: 4159.000000, min: 4159.000000)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t4.1 KiB\t4.1 KiB\t4.1 KiB\t4.1 KiB\t4.1 KiB\t4.1 KiB\t4.1 KiB\t4.1 KiB\t4.1 KiB\n",
       "26/02/04 02:08:06 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 94.907407, stdev: 0.000000, max: 94.907407, min: 94.907407)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\t95 %\n",
       "26/02/04 02:08:06 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 02:08:06 INFO StatsReportListener: other time pct: (count: 1, mean: 5.092593, stdev: 0.000000, max: 5.092593, min: 5.092593)\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:06 INFO StatsReportListener: \t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\t 5 %\n",
       "26/02/04 02:08:06 INFO SparkSQLEngineListener: Job end. Job 46 state is JobSucceeded\n",
       "26/02/04 02:08:06 INFO SQLOperationListener: Query [595c0c20-6f1c-420f-a823-d0ec77424b5c]: Job 46 succeeded, 0 active jobs running\n",
       "26/02/04 02:08:06 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:06 INFO ExecutePython: |node_id   |scontrol_state|reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 02:08:06 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-001|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-002|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-003|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-004|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-005|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-006|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-007|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-008|[\"MIXED\"]     |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-009|[\"IDLE\"]      |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cpu-dm-010|[\"IDLE\"]      |      |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: +----------+--------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:06 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.\n",
       "26/02/04 02:08:06 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 02:08:06 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 02:08:06 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 02:08:06 INFO GpuOverrides: Plan conversion to the GPU took 1.38 ms\n",
       "26/02/04 02:08:06 INFO GpuOverrides: GPU plan transition optimization took 1.24 ms\n",
       "26/02/04 02:08:06 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO SparkContext: Created broadcast 66 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 02:08:06 INFO SparkContext: Starting job: showString at <unknown>:0\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Got job 47 (showString at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Final stage: ResultStage 59 (showString at <unknown>:0)\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[312] at showString at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:06 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[312] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:08:06 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:08:06 INFO FairSchedulableBuilder: Added task set TaskSet_59.0 tasks to pool \n",
       "26/02/04 02:08:06 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 342) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10150 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:06 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:06 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 342) in 170 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:08:06 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:08:06 INFO DAGScheduler: ResultStage 59 (showString at <unknown>:0) finished in 0.176 s\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:08:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished\n",
       "26/02/04 02:08:06 INFO SparkSQLEngineListener: Job end. Job 47 state is JobSucceeded\n",
       "26/02/04 02:08:06 INFO DAGScheduler: Job 47 finished: showString at <unknown>:0, took 0.178066 s\n",
       "26/02/04 02:08:06 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:06 INFO ExecutePython: |node_id     |scontrol_state                     |reason|updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 02:08:06 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72001-T02|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:38:30+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72001-T09|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:55+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72001-T10|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:16+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72001-T12|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:49+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72001-T16|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:10+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72003-T14|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:58+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72003-T18|[\"ALLOCATED\"]                      |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:58:47+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72004-T05|[\"FUTURE\"]                         |      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-01T00:33:21+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72007-T01|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:06+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: |nvl72007-T02|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]|      |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:19+00:00|\n",
       "26/02/04 02:08:06 INFO ExecutePython: +------------+-----------------------------------+------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:06 INFO GpuOverrides: Plan conversion to the GPU took 0.22 ms\n",
       "26/02/04 02:08:06 INFO GpuOverrides: GPU plan transition optimization took 0.10 ms\n",
       "26/02/04 02:08:06 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#3050 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#3051 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#3052 could run on GPU\n",
       "\n",
       "26/02/04 02:08:06 INFO GpuOverrides: Plan conversion to the GPU took 0.33 ms\n",
       "26/02/04 02:08:06 INFO GpuOverrides: GPU plan transition optimization took 0.15 ms\n",
       "26/02/04 02:08:06 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:06 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 02:08:06 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:06 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 02:08:06 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:06 INFO GpuOverrides: Plan conversion to the GPU took 0.21 ms\n",
       "26/02/04 02:08:06 INFO GpuOverrides: GPU plan transition optimization took 0.11 ms\n",
       "26/02/04 02:08:07 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#3077 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#3078 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#3079 could run on GPU\n",
       "\n",
       "26/02/04 02:08:07 INFO GpuOverrides: Plan conversion to the GPU took 0.41 ms\n",
       "26/02/04 02:08:07 INFO GpuOverrides: GPU plan transition optimization took 0.18 ms\n",
       "26/02/04 02:08:07 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:07 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 02:08:07 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:07 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 02:08:07 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:07 INFO ExecutePython: Processing anonymous's query[595c0c20-6f1c-420f-a823-d0ec77424b5c]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.181 seconds\n",
       "26/02/04 02:08:07 INFO DAGScheduler: Asked to cancel job group 595c0c20-6f1c-420f-a823-d0ec77424b5c\n",
       "2026-02-04T02:08:07,043Z INFO ExecuteStatement: Query[595c0c20-6f1c-420f-a823-d0ec77424b5c] in FINISHED_STATE\n",
       "2026-02-04T02:08:07,043Z INFO ExecuteStatement: Processing anonymous's query[595c0c20-6f1c-420f-a823-d0ec77424b5c]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.185 seconds\n",
       "2026-02-04T02:08:07,523Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/595c0c20-6f1c-420f-a823-d0ec77424b5c/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:07,671Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/595c0c20-6f1c-420f-a823-d0ec77424b5c/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:08:07 INFO DAGScheduler: Asked to cancel job group 595c0c20-6f1c-420f-a823-d0ec77424b5c\n",
       "2026-02-04T02:08:17,128Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "26/02/04 02:08:29 INFO SparkSQLSessionManager: Checking sessions timeout, current count: 10\n",
       "2026-02-04T02:08:47,129Z INFO KyuubiSessionManager: Checking sessions timeout, current count: 5\n",
       "2026-02-04T02:08:47,129Z INFO KyuubiSessionManager: Closing session 5d19cc21-fea8-4341-b33b-b5ae4053cabe that has been idle for more than 3600000 ms\n",
       "2026-02-04T02:08:47,129Z INFO KyuubiSessionManager: anonymous's session with SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]/kernel-v3b8e3c6138a1cf45790ec5e0b52755323126f16f3 is closed, current opening sessions 4\n",
       "26/02/04 02:08:47 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]\n",
       "26/02/04 02:08:47 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]\n",
       "26/02/04 02:08:47 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]/kernel-v3b8e3c6138a1cf45790ec5e0b52755323126f16f3 is closed, current opening sessions 9\n",
       "26/02/04 02:08:47 INFO ExecutePython: send SIGCONT to 5925\n",
       "26/02/04 02:08:47 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [5d19cc21-fea8-4341-b33b-b5ae4053cabe]\n",
       "26/02/04 02:08:47 WARN ExecutePython: Process has been interrupted\n",
       "26/02/04 02:08:47 INFO SparkTBinaryFrontendService: Received request of closing SessionHandle [750c7b4d-5cf2-46d1-944d-b235af1c7485]\n",
       "26/02/04 02:08:47 INFO KyuubiPythonGatewayServer: Shutting down KyuubiPythonGatewayServer for session handle SessionHandle [750c7b4d-5cf2-46d1-944d-b235af1c7485]\n",
       "26/02/04 02:08:47 INFO SparkSQLSessionManager: anonymous's session with SessionHandle [750c7b4d-5cf2-46d1-944d-b235af1c7485]/5d19cc21-fea8-4341-b33b-b5ae4053cabe_aliveness_probe is closed, current opening sessions 8\n",
       "26/02/04 02:08:47 INFO SparkTBinaryFrontendService: Finished closing SessionHandle [750c7b4d-5cf2-46d1-944d-b235af1c7485]\n",
       "2026-02-04T02:08:49,243Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/6aaa5437-2c75-48a7-97f2-31ae7b13481f\n",
       "2026-02-04T02:08:49,245Z INFO ExecuteStatement: Processing anonymous's query[6aaa5437-2c75-48a7-97f2-31ae7b13481f]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:49,245Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/58dd32a9-405c-47a7-abbb-c1daa648e609/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:49,249Z INFO ExecuteStatement: Query[6aaa5437-2c75-48a7-97f2-31ae7b13481f] in FINISHED_STATE\n",
       "2026-02-04T02:08:49,249Z INFO ExecuteStatement: Processing anonymous's query[6aaa5437-2c75-48a7-97f2-31ae7b13481f]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.004 seconds\n",
       "2026-02-04T02:08:49,749Z INFO OperationLog: Creating operation log file /opt/kyuubi/work/server_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/ad642ba3-a922-491e-9b43-357da004b387\n",
       "2026-02-04T02:08:49,751Z INFO ExecuteStatement: Processing anonymous's query[ad642ba3-a922-491e-9b43-357da004b387]: PENDING_STATE -> RUNNING_STATE\n",
       "2026-02-04T02:08:49,751Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=POST\turi=/api/v1/sessions/58dd32a9-405c-47a7-abbb-c1daa648e609/operations/statement\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "26/02/04 02:08:49 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/6aaa5437-2c75-48a7-97f2-31ae7b13481f\n",
       "26/02/04 02:08:49 INFO ExecutePython: Processing anonymous's query[6aaa5437-2c75-48a7-97f2-31ae7b13481f]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:08:49 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:08:49 INFO ExecutePython: Processing anonymous's query[6aaa5437-2c75-48a7-97f2-31ae7b13481f]: RUNNING_STATE -> FINISHED_STATE, time taken: 0.001 seconds\n",
       "26/02/04 02:08:49 INFO DAGScheduler: Asked to cancel job group 6aaa5437-2c75-48a7-97f2-31ae7b13481f\n",
       "26/02/04 02:08:49 INFO OperationLog: Creating operation log file /opt/kyuubi/work/engine_operation_logs/58dd32a9-405c-47a7-abbb-c1daa648e609/ad642ba3-a922-491e-9b43-357da004b387\n",
       "26/02/04 02:08:49 INFO ExecutePython: Processing anonymous's query[ad642ba3-a922-491e-9b43-357da004b387]: PENDING_STATE -> RUNNING_STATE\n",
       "26/02/04 02:08:49 INFO ExecutePython: \n",
       "           Spark application name: kyuubi_GROUP_SPARK_SQL_default_cluster-20260203202803-yawkv5ak_cfc8b837-b0f7-40ee-b388-391d8316c4dc\n",
       "                 application ID: spark-cdd9e5d0115345edb431647f51f82517\n",
       "                 application web UI: http://100.67.56.160:4040\n",
       "                 master: k8s://https://F78D18BAE4583C447D0B4251E682CFEB.yl4.us-west-1.eks.amazonaws.com\n",
       "                 deploy mode: cluster\n",
       "                 version: 3.5.3\n",
       "           Start time: 2026-02-03T23:40:48.311\n",
       "           User: spring\n",
       "26/02/04 02:08:49 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 02:08:49 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 02:08:49 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 02:08:49 INFO GpuOverrides: Plan conversion to the GPU took 1.08 ms\n",
       "26/02/04 02:08:49 INFO GpuOverrides: GPU plan transition optimization took 0.54 ms\n",
       "26/02/04 02:08:49 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 02:08:49 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:49 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:49 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:49 INFO SparkContext: Created broadcast 68 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "26/02/04 02:08:49 INFO SparkContext: Starting job: showString at <unknown>:0\n",
       "26/02/04 02:08:49 INFO DAGScheduler: Got job 48 (showString at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:08:49 INFO DAGScheduler: Final stage: ResultStage 60 (showString at <unknown>:0)\n",
       "26/02/04 02:08:49 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:08:49 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:08:49 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[321] at showString at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:08:49 INFO SQLOperationListener: Query [ad642ba3-a922-491e-9b43-357da004b387]: Job 48 started with 1 stages, 1 active jobs running\n",
       "26/02/04 02:08:49 INFO SQLOperationListener: Query [ad642ba3-a922-491e-9b43-357da004b387]: Stage 60.0 started with 1 tasks, 1 active stages running\n",
       "26/02/04 02:08:49 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:49 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:49 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:49 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:08:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[321] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:08:49 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:08:49 INFO FairSchedulableBuilder: Added task set TaskSet_60.0 tasks to pool \n",
       "26/02/04 02:08:49 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 343) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10152 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:08:49 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:49 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "2026-02-04T02:08:50,230Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.107.230\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ad642ba3-a922-491e-9b43-357da004b387/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:50,823Z INFO ExecuteStatement: Query[ad642ba3-a922-491e-9b43-357da004b387] in FINISHED_STATE\n",
       "2026-02-04T02:08:50,823Z INFO ExecuteStatement: Processing anonymous's query[ad642ba3-a922-491e-9b43-357da004b387]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.072 seconds\n",
       "26/02/04 02:08:50 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 343) in 191 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:08:50 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:08:50 INFO DAGScheduler: ResultStage 60 (showString at <unknown>:0) finished in 0.194 s\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:08:50 INFO SQLOperationListener: Finished stage: Stage(60, 0); Name: 'showString at <unknown>:0'; Status: succeeded; numTasks: 1; Took: 194 msec\n",
       "26/02/04 02:08:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Job 48 finished: showString at <unknown>:0, took 0.199084 s\n",
       "26/02/04 02:08:50 INFO StatsReportListener: task runtime:(count: 1, mean: 191.000000, stdev: 0.000000, max: 191.000000, min: 191.000000)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t191.0 ms\t191.0 ms\t191.0 ms\t191.0 ms\t191.0 ms\t191.0 ms\t191.0 ms\t191.0 ms\t191.0 ms\n",
       "26/02/04 02:08:50 INFO StatsReportListener: shuffle bytes written:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:08:50 INFO StatsReportListener: fetch wait time:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\t0.0 ms\n",
       "26/02/04 02:08:50 INFO StatsReportListener: remote bytes read:(count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\t0.0 B\n",
       "26/02/04 02:08:50 INFO StatsReportListener: task result size:(count: 1, mean: 4605.000000, stdev: 0.000000, max: 4605.000000, min: 4605.000000)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t4.5 KiB\t4.5 KiB\t4.5 KiB\t4.5 KiB\t4.5 KiB\t4.5 KiB\t4.5 KiB\t4.5 KiB\t4.5 KiB\n",
       "26/02/04 02:08:50 INFO StatsReportListener: executor (non-fetch) time pct: (count: 1, mean: 94.240838, stdev: 0.000000, max: 94.240838, min: 94.240838)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\t94 %\n",
       "26/02/04 02:08:50 INFO StatsReportListener: fetch wait time pct: (count: 1, mean: 0.000000, stdev: 0.000000, max: 0.000000, min: 0.000000)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\t 0 %\n",
       "26/02/04 02:08:50 INFO StatsReportListener: other time pct: (count: 1, mean: 5.759162, stdev: 0.000000, max: 5.759162, min: 5.759162)\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t0%\t5%\t10%\t25%\t50%\t75%\t90%\t95%\t100%\n",
       "26/02/04 02:08:50 INFO StatsReportListener: \t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\t 6 %\n",
       "26/02/04 02:08:50 INFO SparkSQLEngineListener: Job end. Job 48 state is JobSucceeded\n",
       "26/02/04 02:08:50 INFO SQLOperationListener: Query [ad642ba3-a922-491e-9b43-357da004b387]: Job 48 succeeded, 0 active jobs running\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |node_id      |scontrol_state                        |reason                            |updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-001   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-002   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-003   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-004   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-005   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-006   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-007   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-008   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-009   |[\"IDLE\"]                              |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-010   |[\"IDLE\"]                              |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-dm-011   |[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T07:56:12+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-large-004|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T08:10:44+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cpu-large-005|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-02T23:00:14+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72D004-T01|[\"DOWN\", \"RESERVED\", \"NOT_RESPONDING\"]|[A] DCA Automation; Not responding|2026-02-04T01:55:05.320984+00:00|false|2026-02-02T18:36:58+00:00|gcp-iad-cs-001-v1|2026-02-02T17:58:39+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72D005-T07|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:10:58+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72D005-T08|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:11:00+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72D005-T09|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:11:00+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72D006-T17|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T08:51:28+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72D007-T02|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T20:20:21+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72D010-T05|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T19:15:20+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: only showing top 20 rows\n",
       "26/02/04 02:08:50 INFO FileSourceStrategy: Pushed Filters: \n",
       "26/02/04 02:08:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
       "26/02/04 02:08:50 WARN GpuOverrides: \n",
       "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
       "  @Partitioning <SinglePartition\\$> could run on GPU\n",
       "\n",
       "26/02/04 02:08:50 INFO GpuOverrides: Plan conversion to the GPU took 1.34 ms\n",
       "26/02/04 02:08:50 INFO GpuOverrides: GPU plan transition optimization took 0.78 ms\n",
       "26/02/04 02:08:50 INFO GpuFileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
       "26/02/04 02:08:50 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 472.3 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 48.2 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 100.67.56.160:7079 (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO SparkContext: Created broadcast 70 from internalDoExecuteColumnar at GpuExec.scala:341\n",
       "8885.219: [GC (Allocation Failure) [PSYoungGen: 996623K->36178K(992256K)] 1086420K->125983K(4834816K), 0.0252772 secs] [Times: user=0.05 sys=0.02, real=0.03 secs] \n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 100.67.4.240:46241 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO SparkContext: Starting job: showString at <unknown>:0\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Got job 49 (showString at <unknown>:0) with 1 output partitions\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Final stage: ResultStage 61 (showString at <unknown>:0)\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Parents of final stage: List()\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Missing parents: List()\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[330] at showString at <unknown>:0), which has no missing parents\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 100.67.4.240:46241 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 27.9 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 100.67.4.240:46241 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 100.67.56.160:7079 (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1585\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[330] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
       "26/02/04 02:08:50 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0\n",
       "26/02/04 02:08:50 INFO FairSchedulableBuilder: Added task set TaskSet_61.0 tasks to pool \n",
       "26/02/04 02:08:50 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 344) (100.67.4.240, executor 4, partition 0, PROCESS_LOCAL, 10150 bytes) taskResourceAssignments Map(gpu -> [name: gpu, addresses: 0])\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 100.67.4.240:46241 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 100.67.4.240:46241 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 100.67.4.240:46241 (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 100.67.56.160:7079 in memory (size: 51.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 100.67.4.240:46241 in memory (size: 51.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 100.67.56.160:7079 in memory (size: 13.1 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 100.67.4.240:46241 in memory (size: 13.1 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 100.67.4.240:46241 (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 100.67.56.160:7079 in memory (size: 48.2 KiB, free: 8.4 GiB)\n",
       "26/02/04 02:08:50 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 100.67.4.240:46241 in memory (size: 48.2 KiB, free: 9.0 GiB)\n",
       "26/02/04 02:08:50 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 344) in 150 ms on 100.67.4.240 (executor 4) (1/1)\n",
       "26/02/04 02:08:50 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool \n",
       "26/02/04 02:08:50 INFO DAGScheduler: ResultStage 61 (showString at <unknown>:0) finished in 0.155 s\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
       "26/02/04 02:08:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished\n",
       "26/02/04 02:08:50 INFO SparkSQLEngineListener: Job end. Job 49 state is JobSucceeded\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Job 49 finished: showString at <unknown>:0, took 0.156232 s\n",
       "26/02/04 02:08:50 INFO ExecutePython: +------------+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |node_id     |scontrol_state                              |reason                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
       "26/02/04 02:08:50 INFO ExecutePython: +------------+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72001-T02|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:38:30+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72001-T09|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:55+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72001-T10|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:16+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72001-T12|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:49+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72001-T16|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:10+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72003-T14|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:58+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72003-T18|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:58:47+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72004-T05|[\"FUTURE\"]                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-01T00:33:21+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72007-T01|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:06+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72007-T02|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:19+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72007-T03|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:19+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72007-T04|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:58+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72007-T05|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:48+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72007-T06|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:27+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72008-T11|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:02+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72008-T12|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:03+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72008-T13|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:03+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72008-T14|[\"IDLE\", \"RESERVED\", \"DRAIN\", \"MAINTENANCE\"]|[HC] REBOOT (qtD9Xenw) hw_drv_gpu_compute_apps: found process python with pid 2229262 on gpu @01, pci @01 failed to get process info, pci @01 failed to get process info, pci @01 failed to get process info; hw_drv_gpu_clock_idle: gpu 01 gpu util: 100% >= 0%, gpu 01 gpu util: 100% >= 0%, gpu 01 gpu util: 100% >= 0%, gpu 01 gpu util: 100% >= 0%;; [HC] <JobID:1650135> REBOOT hw_drv_gpu_compute_apps: found process python with pid 2229262 on gpu @01,pci @01 failed to get process info,pci @01 failed to get process info,pci @01 failed to get process info; hw_drv_gpu_clock_idle: gpu 01 gpu util: 100% >= 0%,gpu 01 gpu util: 100% >= 0%,gpu 01 gpu util: 100% >= 0%,gpu 01 gpu util: 100% >= 0%; |2026-02-04T01:55:43.953461+00:00|false|2026-02-02T21:04:38+00:00|oci-hsg-cs-001-v3|2026-02-03T21:00:12+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72008-T15|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:03+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: |nvl72011-T12|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:07+00:00|\n",
       "26/02/04 02:08:50 INFO ExecutePython: +------------+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: only showing top 20 rows\n",
       "26/02/04 02:08:50 INFO GpuOverrides: Plan conversion to the GPU took 0.22 ms\n",
       "26/02/04 02:08:50 INFO GpuOverrides: GPU plan transition optimization took 0.10 ms\n",
       "26/02/04 02:08:50 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#3186 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#3187 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#3188 could run on GPU\n",
       "\n",
       "26/02/04 02:08:50 INFO GpuOverrides: Plan conversion to the GPU took 0.32 ms\n",
       "26/02/04 02:08:50 INFO GpuOverrides: GPU plan transition optimization took 0.12 ms\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:50 INFO GpuOverrides: Plan conversion to the GPU took 0.26 ms\n",
       "26/02/04 02:08:50 INFO GpuOverrides: GPU plan transition optimization took 0.12 ms\n",
       "26/02/04 02:08:50 WARN GpuOverrides: \n",
       "! <LocalTableScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.LocalTableScanExec\n",
       "  @Expression <AttributeReference> toprettystring(col_name)#3213 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(data_type)#3214 could run on GPU\n",
       "  @Expression <AttributeReference> toprettystring(comment)#3215 could run on GPU\n",
       "\n",
       "26/02/04 02:08:50 INFO GpuOverrides: Plan conversion to the GPU took 0.36 ms\n",
       "26/02/04 02:08:50 INFO GpuOverrides: GPU plan transition optimization took 0.18 ms\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |col_name         |data_type|comment|\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: |node_id          |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |scontrol_state   |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |reason           |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |updated_at       |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |stale            |boolean  |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |reason_changed_at|string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |cluster_id       |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: |last_busy_at     |string   |NULL   |\n",
       "26/02/04 02:08:50 INFO ExecutePython: +-----------------+---------+-------+\n",
       "26/02/04 02:08:50 INFO ExecutePython: Processing anonymous's query[ad642ba3-a922-491e-9b43-357da004b387]: RUNNING_STATE -> FINISHED_STATE, time taken: 1.067 seconds\n",
       "26/02/04 02:08:50 INFO DAGScheduler: Asked to cancel job group ad642ba3-a922-491e-9b43-357da004b387\n",
       "2026-02-04T02:08:51,373Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.97.22\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ad642ba3-a922-491e-9b43-357da004b387/event\tparams=null\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "2026-02-04T02:08:51,513Z INFO AuthenticationAuditLogger: user=anonymous(auth:BASIC)\tip=100.67.216.117\tproxyIp=null\tmethod=GET\turi=/api/v1/operations/ad642ba3-a922-491e-9b43-357da004b387/rowset\tparams=fetchorientation=FETCH_FIRST\tprotocol=HTTP/1.1\tstatus=200\tSparkaas-Transaction=\n",
       "`;\n",
       "            var blob = new Blob([logs], { type: 'text/plain' });\n",
       "            var url = window.URL.createObjectURL(blob);\n",
       "            var a = document.createElement('a');\n",
       "            a.href = url;\n",
       "            a.download = 'driver_logs_cluster-20260203202803-yawkv5ak-driver_20260204_020851.txt';\n",
       "            document.body.appendChild(a);\n",
       "            a.click();\n",
       "            window.URL.revokeObjectURL(url);\n",
       "            document.body.removeChild(a);\n",
       "        }\n",
       "        </script>\n",
       "        </td></tr></table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <script>\n",
       "    function toggleLinks(containerId) {\n",
       "        var linksDiv = document.getElementById(\"links-\" + containerId);\n",
       "        if (linksDiv.style.display === \"none\") {\n",
       "            linksDiv.style.display = \"block\";\n",
       "        } else {\n",
       "            linksDiv.style.display = \"none\";\n",
       "        }\n",
       "    }\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
      "|node_id      |scontrol_state                        |reason                            |updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
      "+-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
      "|cpu-dm-001   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-002   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-003   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-004   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-005   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-006   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-007   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-008   |[\"MIXED\"]                             |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T23:41:25+00:00|\n",
      "|cpu-dm-009   |[\"IDLE\"]                              |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
      "|cpu-dm-010   |[\"IDLE\"]                              |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-04T01:41:27+00:00|\n",
      "|cpu-dm-011   |[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T07:56:12+00:00|\n",
      "|cpu-large-004|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T08:10:44+00:00|\n",
      "|cpu-large-005|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-02T23:00:14+00:00|\n",
      "|nvl72D004-T01|[\"DOWN\", \"RESERVED\", \"NOT_RESPONDING\"]|[A] DCA Automation; Not responding|2026-02-04T01:55:05.320984+00:00|false|2026-02-02T18:36:58+00:00|gcp-iad-cs-001-v1|2026-02-02T17:58:39+00:00|\n",
      "|nvl72D005-T07|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:10:58+00:00|\n",
      "|nvl72D005-T08|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:11:00+00:00|\n",
      "|nvl72D005-T09|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T18:11:00+00:00|\n",
      "|nvl72D006-T17|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T08:51:28+00:00|\n",
      "|nvl72D007-T02|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T20:20:21+00:00|\n",
      "|nvl72D010-T05|[\"ALLOCATED\"]                         |                                  |2026-02-04T01:55:05.320984+00:00|false|1970-01-01T00:00:00+00:00|gcp-iad-cs-001-v1|2026-02-03T19:15:20+00:00|\n",
      "+-------------+--------------------------------------+----------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
      "|node_id     |scontrol_state                              |reason                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |updated_at                      |stale|reason_changed_at        |cluster_id       |last_busy_at             |\n",
      "+------------+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
      "|nvl72001-T02|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:38:30+00:00|\n",
      "|nvl72001-T09|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:55+00:00|\n",
      "|nvl72001-T10|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:16+00:00|\n",
      "|nvl72001-T12|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:49+00:00|\n",
      "|nvl72001-T16|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:10+00:00|\n",
      "|nvl72003-T14|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:36:58+00:00|\n",
      "|nvl72003-T18|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:58:47+00:00|\n",
      "|nvl72004-T05|[\"FUTURE\"]                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-01T00:33:21+00:00|\n",
      "|nvl72007-T01|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:06+00:00|\n",
      "|nvl72007-T02|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:19+00:00|\n",
      "|nvl72007-T03|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:19+00:00|\n",
      "|nvl72007-T04|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:58+00:00|\n",
      "|nvl72007-T05|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:35:48+00:00|\n",
      "|nvl72007-T06|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:36:27+00:00|\n",
      "|nvl72008-T11|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:02+00:00|\n",
      "|nvl72008-T12|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:03+00:00|\n",
      "|nvl72008-T13|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:03+00:00|\n",
      "|nvl72008-T14|[\"IDLE\", \"RESERVED\", \"DRAIN\", \"MAINTENANCE\"]|[HC] REBOOT (qtD9Xenw) hw_drv_gpu_compute_apps: found process python with pid 2229262 on gpu @01, pci @01 failed to get process info, pci @01 failed to get process info, pci @01 failed to get process info; hw_drv_gpu_clock_idle: gpu 01 gpu util: 100% >= 0%, gpu 01 gpu util: 100% >= 0%, gpu 01 gpu util: 100% >= 0%, gpu 01 gpu util: 100% >= 0%;; [HC] <JobID:1650135> REBOOT hw_drv_gpu_compute_apps: found process python with pid 2229262 on gpu @01,pci @01 failed to get process info,pci @01 failed to get process info,pci @01 failed to get process info; hw_drv_gpu_clock_idle: gpu 01 gpu util: 100% >= 0%,gpu 01 gpu util: 100% >= 0%,gpu 01 gpu util: 100% >= 0%,gpu 01 gpu util: 100% >= 0%; |2026-02-04T01:55:43.953461+00:00|false|2026-02-02T21:04:38+00:00|oci-hsg-cs-001-v3|2026-02-03T21:00:12+00:00|\n",
      "|nvl72008-T15|[\"IDLE\", \"RESERVED\", \"MAINTENANCE\"]         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-03T23:45:03+00:00|\n",
      "|nvl72011-T12|[\"ALLOCATED\"]                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |2026-02-04T01:55:43.953461+00:00|false|1970-01-01T00:00:00+00:00|oci-hsg-cs-001-v3|2026-02-04T01:37:07+00:00|\n",
      "+------------+--------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----+-------------------------+-----------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+---------+-------+\n",
      "|col_name         |data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|node_id          |string   |NULL   |\n",
      "|scontrol_state   |string   |NULL   |\n",
      "|reason           |string   |NULL   |\n",
      "|updated_at       |string   |NULL   |\n",
      "|stale            |boolean  |NULL   |\n",
      "|reason_changed_at|string   |NULL   |\n",
      "|cluster_id       |string   |NULL   |\n",
      "|last_busy_at     |string   |NULL   |\n",
      "+-----------------+---------+-------+\n",
      "\n",
      "+-----------------+---------+-------+\n",
      "|col_name         |data_type|comment|\n",
      "+-----------------+---------+-------+\n",
      "|node_id          |string   |NULL   |\n",
      "|scontrol_state   |string   |NULL   |\n",
      "|reason           |string   |NULL   |\n",
      "|updated_at       |string   |NULL   |\n",
      "|stale            |boolean  |NULL   |\n",
      "|reason_changed_at|string   |NULL   |\n",
      "|cluster_id       |string   |NULL   |\n",
      "|last_busy_at     |string   |NULL   |\n",
      "+-----------------+---------+-------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM GCP_EAST4_maestro_slurm_nodes LIMIT 100\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"SELECT * FROM OCI_HSG_maestro_slurm_nodes LIMIT 100\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"DESCRIBE OCI_HSG_maestro_slurm_nodes\").show(50, truncate=False)\n",
    "spark.sql(\"DESCRIBE GCP_EAST4_maestro_slurm_nodes\").show(50, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NvSparkKernel",
   "language": "no-op",
   "name": "nvsparkaas"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "no-op"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
